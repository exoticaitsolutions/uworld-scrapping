id,title,content,subject,question_count,topic
1,Passage Title: Past and Present,"Often regarded as a requiem for the American dream of material success, Arthur Miller's Death of a Salesman is one of the best-known plays of the modern American stage.  Not so well known, however, is the 1949 drama's original title, ""The Inside of His Head.""  Yet that erstwhile label may more accurately capture the spirit of this uniquely psychological tragedy depicting the downfall of distraught salesman Willy Loman.  For a time, Miller even imagined the curtain rising on an enormous head constructed for the audience to see inside.  Ultimately, however, the Pulitzer Prize–winning playwright settled on less literal means for us to peer inside the mind of his protagonist.
To help dramatize Willy's psychological deterioration, Miller makes liberal use of a technique known as interior monologue.  Like stream of consciousness, which delivers a character's unfiltered and uncontrolled thoughts in all their incoherence, interior monologue exposes the audience to the inner workings of the protagonist's mind, only using slightly more verbal organization.  Thus, through Willy's mutterings, we are privy to his mental state as he becomes increasingly entangled in remembrances of things past.  Eventually, the ill-fated salesman is unable to distinguish between then and now, illusion and reality.  Perhaps more impactful is Miller's corresponding tactic of time shifts: as Loman vacillates between reality and reverie, the audience is effectively shuttled between present and past.  However, unlike the popular cinematic device of flashbacks, which aim to recreate prior events objectively, these intrusions of the past into the present—dubbed ""mobile concurrences"" by Miller—are highly subjective reconstructions imparting Willy's version of those events.
Miller's stratagem of mobile concurrency—or what some critics call ""double exposure"" or ""split consciousness""—is a function of the main character's desperate condition.  Willy is acutely aware of himself as an aging man for whom career opportunities have foreclosed and for whom the time to realize goals has run out.  His hypersensitive state leaves him susceptible to episodes of spontaneous recollection: a seemingly trivial occurrence summons up a similar event from the past, or a casually uttered word or phrase (for example, mention of the Lomans' old car) triggers a barrage of memories.  The recalled event, along with its contiguous incidents and attendant emotions, is pulled from the matrix of the past into the flow of the present and reexperienced.  As Matt Biscay comments about the play, ""Memories constantly impinge on the present situation, and conversely, the present is put at some distance by the flood of recollections.""
Inside the theater, flute music and lighting changes signal to the audience Willy's dissociation from the present moment as he lapses into a romanticized past.  To the other characters on stage, Loman appears to be daydreaming or disoriented.  At times he seems to be talking to himself as he converses with figures from long ago, like his deceased brother Ben—figures invisible to the others but sometimes visible to the audience and even appearing to walk through walls.  As the tide of reminiscence recedes and its ghosts evaporate, Willy reemerges into the present, often resuming conversations with friends or family as if no interruption had occurred.  As Miller himself asserts, his goal was to ""erase any gap between a remembered past—that would be evoked through words…and a present that would be performed on stage.""
Through the techniques of interior monologue and mobile concurrences, Miller manipulates the temporal structure of his drama depicting the American dream's demise.  By making time's boundaries permeable for the broken and nostalgic Willy, Miller allows past and present to coexist for the audience as well.  Thus, the playwright succeeds at giving us his protagonist's subjective view—the view, that is, from the inside of his head.",Critical Analysis & Reasoning Skills,5,Humanities
2,Passage Title: Prior Knowledge: Kenya and Cultural Research,"Research into other cultures has long been informed by the recognition that one must attempt to assimilate the viewpoint of the group under study, rather than proceed from ethnocentric conceptions.  As Herbert Blumer wrote in 1969's influential Symbolic Interactionism, ""[I]f the scholar wishes to understand the actions of people, it is necessary to see their objects as they see them….  Simply put, people act toward things on the basis of the meaning that these things have for them, not on the basis of the meaning that these things have for the outside scholar.""  Those who research other peoples are thus generally well served by the admonition not to import (whether overtly or subconsciously) the presuppositions of their own backgrounds into their understanding of another culture or tradition.
However, a peculiar sort of danger can arise from adherence to this clearly reasonable principle, as illustrated in the recent experience of anthropological photographer Colin Prior.  While studying the Turkana tribe of northwest Kenya, Prior noted the ornate necklaces of colorful beads and other accessories typically worn by females.  Curious to learn about these necklaces and their various elements, he approached a group of girls: ""In my naïveté, I asked the purpose of the keys worn around the girls' necks.  Their bemused reply was as much a surprise to me as the question had been a surprise to them.  'For our front doors.'""
The girls' surprise and bemusement speaks to the fact that the question must have seemed silly to them; indeed, as silly as it would doubtless have seemed to a British woman if similarly queried.  Presumably in the latter case Prior would have thought the function of the keys was obvious, but with the Turkana he appears to have assumed that the obvious answer must not be right.  Paradoxically, Prior's openness to discovering the differences of another society led him away from, rather than toward, a correct understanding.
A mirror image of Prior's experience can be found among girls of another Kenyan tribe, the Samburu.  Noting the rarity of formal education among Samburu women, Carolyn Lesorogol describes the uneducated females' ""incomprehension of what the [educated] girls did in school.  One woman commented that, as a child, she was at a loss to understand why the one school girl sat in her house looking at a paper (i.e. reading).  When the other girls asked her to join them outside to play or go to a dance, she refused, saying she wanted to stay in and look at the paper.""
For those who are literate, the sheer automaticity of the act of reading makes it difficult to imagine the confusion the woman felt as a child.  But to someone for whom even the concept of reading is unfamiliar, the school girl's actions would surely have seemed bizarre; presumably just sitting and staring at a piece of paper would indeed be quite boring.  Based on how most of the Samburu girls conceived of this behavior, it is easy to empathize with their puzzlement over the school girl's choice of activities.
We can see researchers like Prior as taking care to avoid such an outlook in their own studies—that is, as appropriately avoiding the assumption that a different group attaches the same meaning to a particular phenomenon or behavior.  Nevertheless, diversity among peoples with regard to dress, family structure, gender, religious practices, and so on reflects a variety of answers to what are universal questions about human needs.  Thus, the foundation of understanding a culture other than one's own must still include the assumption of a deeper-level similarity underlying the surface-level differences.",Critical Analysis & Reasoning Skills,4,Humanities
3,Passage Title: The Caves of Lascaux,"In September 1940, near the tiny village of Montignac in the southwest region of France, a band of four youths and a dog accidentally stumbled upon the opening to a series of underground caverns whose walls were covered with prehistoric paintings.  Their monumental discovery unleashed a mystery that has continued to puzzle scholars: What function did these magnificent caves perform for the society that painted them?  One plausible theory, yet unproven, is that they constituted a kind of sacred space, perhaps a site for religious ceremonies.
Decades of work recording and analyzing the contents of this labyrinthine underworld, which came to be called Lascaux, have yielded some clues.  Pollen testing, carbon-14 dating, and the examination of flint and bone tools within the caves have been used to calculate the time period during which these subterranean reaches were painted and served their function.  The paintings date from between 17,000 and 12,000 BCE, which places them within the Upper Paleolithic era.  More specifically, they belong to the Magdalenian period of the last ice age that preceded the dawn of the current geological epoch and the invention of agriculture.
More than 2,000 paintings and symbols cover the insides of the caverns.  Approximately 900 of these depict horses, bison, stags, large felines, and hares, clearly reflecting a society of hunter-gatherers.  Various pigments, mainly red, yellow, and brown, were used.  By employing a novel method that took advantage of the edges and curvatures of the cave walls, the artists were able to give lifelike dimensions to these images and endow them with a sense of motion.  Nevertheless, the cave art of Lascaux suggests a purpose that goes beyond artistic expression.
It is likely these chambers served as a sanctuary within which the paintings would have corresponded to the iconography of a temple.  Moreover, countless footprints shown to have belonged to adolescent males suggest that the caves may have hosted initiation rituals marking a boy's passage from childhood to maturity.  At this critical juncture, the youths would have traditionally been separated from their families by the adult males and put through frightening ordeals.  The boys might have been tattooed, confined, or presented with physical challenges.  The purpose of such traumas was to prepare the adolescent for the physical and psychological rigors of the hunt, the brutalities of warfare, and the inevitable trials of adulthood.  Theologian Karen Armstrong describes such experiences as triggering ""a regressive disorganization of the personality"" followed by ""a constructive reorganization"" of the individual's faculties.  In other words, the rites were meant to force the youth to call upon inner resources of which he was not yet aware, thereby inducing a kind of psychological death and rebirth.
Lying deep within the earth, the caves would have provided the perfect container for these traumatic rites—a fact that became evident when Lascaux was opened to the public in 1948.  As mythologist Joseph Campbell recalled of his visit there, when the lights were out, all normal orientation and sense of time were suspended and ""you were never in darker darkness in your life.""  In primitive societies, descending into the darkness of the caves must have seemed like immersing oneself in what religious historian Mircea Eliade has termed ""sacred time,"" a kind of dissociative state conducive to the deeply transformative experience of the initiate.  When illuminated against the blackness, the realistic murals would have become a moving spectacle that simulated the hunt, the wonder and gravity of which was forever imprinted upon the boys' psyches.",Critical Analysis & Reasoning Skills,6,Social Sciences
4,"Passage Title: ""Enlightened"" Architecture","Situated near Weems, Virginia, Christ Church has long belied its appearance as a typical, if elite, Anglican house of worship from the colonial era.  Some have conjectured that the 1735 building is based on a prototype by the renowned English architect Christopher Wren, and, in truth, its resemblance to Farley Church in Wiltshire, England, which he designed, is almost uncanny.  Originally an unassuming wooden construct, Christ Church was commissioned in 1665 by John Carter, a wealthy tobacco planter who had come from his native England to Virginian shores three decades earlier.  In 1730, his son Robert—by then one of the wealthiest and most influential figures in the region—was determined to replace the structure of wood with one of brick.  He died two years later, leaving behind three male offspring to oversee the completion of what experts would later hail as ""the most finely crafted Anglican parish church in all of colonial Virginia.""  Indeed, historical websites laud it as ""an extraordinary example of eighteenth-century architecture, featuring meticulous brickwork, vaulted ceilings, and gorgeous compass-headed windows.""  Notably, the church also contains a triple-decker pulpit—a rare but distinctive feature of its era.  In recent decades, however, Christ Church has become a subject of interest more for what it conceals than for what it reveals.
The intricacies of this remarkable building point to its duality in being not only a religious sanctuary, but a shrine to rationality, a veritable temple of the Enlightenment.  Beneath its surface, Christ Church constitutes an architectural feat of mathematical precision in harmony with nature's laws.  Indeed, horologist H. Stephen Stewart has characterized it as ""a building of numbers""—specifically, calendric, solar, and lunar numbers.  As Stewart explains, two weeks after the vernal equinox, on the mean date for Easter, a sunbeam shines through the oval window above the main entrance that faces due west and rests upon the altar; this phenomenon repeats itself two weeks prior to the autumnal equinox.  Moreover, a shadow cast from a cornice of the building, moving vertically along the church's outer west wall at the pace of a single row of bricks per day, tracks the progression from spring to fall.  For eight hours daily, the structure also acts as a sundial, casting its shadow on the north lawn.  A seemingly multifunctional edifice, Christ Church, as Stewart astutely concluded, is a virtual ""time machine.""
The locale and physical orientation of this sacred structure were critical: not just any terrestrial site could offer complete celestial alignment.  As a result, Christ Church has basked in splendid isolation amidst an untamed countryside in a spot described by journalist Joy Aschenbach as ""a rather lonely one, with woods all around, few or no houses near, and no good roads.""  Its exchange of geographical remoteness for astronomical positioning recalls another singular sanctuary, the fifth-century BCE Temple to Apollo, the god of the sun and rationality, at Bassae in Greece.  Thrust deep into the rural expanse, this imposing temple—whose design has been attributed to the same architect as the Athenian Parthenon—sits atop Kotylion Mountain.  There, this previously forgotten structure holds dominion over a desolate landscape through which, prior to the last century, no truly passable road led.  Oriented north to south rather than (the more typical) east to west, the building seems calibrated to track the sun's annual phases.  Like the church, the temple has moved from dilapidation to restoration, becoming the focus of scrutiny as scientists decipher the calculus of its blueprint.
Clearly, such anomalies were not the result of happenstance.  As spaces of worship, these shrines are at once pragmatic and sacred: their purposes remain overtly divine even as their designs honor nature and extol reason.",Critical Analysis & Reasoning Skills,5,Social Sciences
5,Passage Title: Value Expression as Constituent of Art,"Art has always been an expression of values: from the earliest cave paintings to modern abstract sculpture, works of art are physical embodiments of a people's mores, experiences, and assumptions.  The study of such works thus provides not only insight into the societies that create them, but comparative grounds for exploring cultural differences among their contemporaries.  For instance, pre-Roman Etruscan art evinces a surprising level of freedom for women relative to other societies.  The Cerveteri Sarcophagus includes a sculpture of a husband and wife reclining and eating together; similarly, a mural adorning the Tomb of Leopards displays several married couples dining in this fashion at a banquet.  To the ancient Greeks, however, this Etruscan practice was an unthinkable scandal: as one historian points out, ""It was so foreign to the Greeks that it both shocked and frightened them.""  At Greek dinners, the only women who attended were servants and paid companions; the men's wives ""remained at home, excluded from most aspects of public life.""
To modern ears, this Greek conception of propriety must sound absurd or even humorous.  Nevertheless, it exemplifies the effect of cultural contact in challenging a society's values, whether through direct observation of a practice or mediated through the art that displays it.  Of course, that function of art also operates within a society, and this self-reflection and critique of cultural ethos is one of art's most important functions.  This is not necessarily to say that art must offer such a challenge to be great; indeed, there are many works that have extraordinary aesthetic worth simply for what they represent.  Still, the importance of art in posing difficult, even painful, questions cannot be overstated.
What is truly bizarre, however, is when art seems neither to reflect nor to challenge the values of its time.  It might be wondered how such a scenario would be possible, as any depiction that did not reflect a society's character would at least suggest a tension with it.  Yet the world of early television provides an example.  In most shows of the 1950s and 1960s, like The Dick Van Dyke Show and I Love Lucy, the master bedrooms contained pairs of twin beds where the husbands and wives slept separately.
This television standard is hard to understand, in part because the implicit moral claims involved do not exemplify the kinds of cultural differences described above.  The American audiences of American television were not another society at risk of being shocked and frightened by unfamiliar customs, as the Greeks experienced with the Etruscans.  Nor can the standard be easily attributed to attitudes evolving over time; although much of what modern viewers take for granted would seem indecent to viewers of the past, surely the idea of a married couple occupying the same bed does not fall into that category.  Thus, the prohibition against depicting that practice could only have ""protected"" audiences from something wholly uncontroversial.  As such, this aspect of early television programming represented neither life as it is nor life as it ought to be—nor even how anyone thought life ought to be.  In this respect, then, does it simply fail to constitute art?
Although that conclusion seems in general justified, perhaps it is more accurate to say that the artistic value is diminished or suppressed, rather than absent entirely.  After all, any purposeful representation of life or fantasy is arguably art in some sense.  Hence, it may be useful to conceive of art and non-art less as distinct categories, and more as points along a continuum.  From that perspective, to be art is something that is achieved to a greater or lesser degree.",Critical Analysis & Reasoning Skills,4,Humanities
6,Passage Title: What is Dance?,"People sometimes dance for joy, or report that they are so happy that they could dance for joy.  However, one might wonder why this is so.  What is it about happiness that would lead someone to dance?  What exactly is dancing, when one ponders this activity that might otherwise be taken for granted?  Is it only a human practice, or is dancing something in which other animals also engage?
Indeed, some animals do seem to dance, although their reasons for doing so are goal-based.  For instance, bees dance to provide information to other colony members, using complicated movements to describe the location and quality of a food source.  This behavior is not emotive; it would seem odd, if not unscientific, to ascribe an emotive character to such displays.  Nevertheless, these displays are communicative, as are emotive human behaviors.
Might this similarity bespeak a common evolutionary basis for the dance of humans and other animals?  Perhaps, although there are differences which cast doubt on that hypothesis.  A bee's dance is its main method of communication, whereas most human communication is vocal.  Even with non-vocal communication such as sign language, the types of bodily movements are qualitatively different from those used in dance, as dance movements signify at most a general feeling or impression rather than corresponding to specific words or ideas.  If the bee's dance functions like human sign language (albeit in a much less sophisticated form), it is probably not an appropriate analogue to human dance at all; the term ""dance"" would be simply a weak metaphor.  Thus, if the bee's dance is a type of language, it is not the same sort of thing as human dance.
Still, there are other types of animal behaviors that might more aptly be called dances.  Many animals perform courtship displays of varying complexity, in which carefully choreographed movements are designed to win the favor of a potential mate.  Historically, a connection between human dance and courtship has also been assumed, and often cautioned against.  For instance, in the 1950s Elvis was denounced by parents as a corrupting influence upon the young women who swooned over him, and some of his dance moves were censored.  People today likewise voice concerns with regard to the behavior of crowds in nightclubs and explicit lyrics in popular music.  A similar outcry will undoubtedly be heard in the future, as the struggle of parental restraint against youthful vigor continues.
It should not be thought, however, that all amorous dancing is subject to condemnation.  Weddings include dance as a matter of custom, suggesting a long-standing endorsement within socially approved couplings.  Through social tradition or natural instinct, dance seems linked with romance in the human mind.  Nevertheless, such a connection is clearly not the whole story.  One of the most fundamental uses of dance is the physical expression of music—a language not of words, but emotion.
If music expresses emotion and dance expresses music, then, do we dance not only ""for joy"" but for all emotions?  Seemingly not, but why?  Certainly, music itself conveys a wide spectrum of feelings.  Although we do not dance ""for sadness"" or ""for anger,"" we do droop our bodies or clench our fists, and perhaps more subtle emotions produce other physical markers.  Still, happiness too can be seen in a smile, and to actually dance for joy is comparatively rare.  Is dance then truly an entity unto itself?  Or, is it merely one instance of a wider class of physical behaviors?",Critical Analysis & Reasoning Skills,6,Humanities
7,Passage Title: Translation,"Of the myriad problems facing translators, perhaps the most significant lies in the tension between what a text conveys and the words used to convey it.  Strict fidelity to the specific words of a text often fails to capture the full sense of its meaning.  Beyond this loss of meaning can also arise a loss of artistry and structural features that often cannot be replicated.
For instance, a fully evocative rendering of the 119th Psalm seems unattainable.  Comprising twenty-two strophes of eight lines each, the one hundred and seventy-six verses do appear somewhat poetic in translation.  But in the original Hebrew the Psalm is an acrostic: the verses of the first strophe all begin with the letter aleph, those of the second with beth, and so on, with tav, the last letter of the alphabet, commencing the final eight verses.  The reader of a translation would, understandably, be ignorant of this fact.  While this does not affect the essential meaning of the verses, it does affect their character—something is lost from the original in its non-native presentation.  Although there have been occasional attempts to mimic this structure in other languages, they have all been unsuccessful.  The tortured and unnatural wording that results proves the wisdom of most translators in prioritizing readability.  Nevertheless, the Psalmist wrote with a particular purpose that is left untransmitted.  The sacrifice of the acrostic, however necessary, is regrettable.
Another concern is what might be called translational multiplicity.  Argentinian writer Jorge Luis Borges famously remarked: ""The Quixote, due to my congenital practice of Spanish, is a uniform monument, with no other variations….  [T]he Odyssey, thanks to my opportune ignorance of Greek, is an international bookstore of works in prose and verse.""  To speak of ""the"" Odyssey—or more generally, ""the"" text of any work—runs headlong into the reality that for translated works, ""the text"" is not a single entity but a collection of candidates.  Thus, a perennial difficulty of translation is conveyed through Borges' idiosyncratic expression.  Or is it?  For Borges' remarks are elsewhere listed as: ""Since Spanish is my native language, the Quixote is to me an unchanging monument, with no possible variations….  But the Odyssey, thanks to my opportune ignorance of Greek, is a library of works in prose and verse.""  That we might ask which of these translations more closely represents his actual words is a fact that Borges would likely have found amusing.
Considerations of this sort are magnified when one approaches the writings not only of individual authors, but of past civilizations.  Despite being one of the world's foremost sinologists, the late D. C. Lau was convinced that none of us truly understand classical Chinese.  The sentiment was no mere expression of modesty; fellow scholar Roger Ames described Lau as ""[h]aving assimilated the classical corpus through a lifetime of careful study and reflection.""  Rather, Lau was keenly aware of a difficult truth: the fluidity of language and culture over time can shroud the works of the ancients in obscurities that may be inscrutable to even their most passionately curious descendants.
Modern works may also be altered, however, and sometimes through more subtle types of translation.  The book known as Harry Potter and the Philosopher's Stone in England was changed to refer instead to the ""Sorcerer's Stone"" in the United States, reportedly due to a belief that the original title would be unpalatable to an American audience.  While that depressing assumption may indeed have been correct, the change removes a titular connection to a pre-existing myth.  The generic fantasy idea with which it is replaced thus fails to embody the full character of the original work.",Critical Analysis & Reasoning Skills,4,Social Sciences
8,Passage Title: Nietzsche's Eternal Recurrence of the Same,"In August of 1881, the German philosopher Friedrich Nietzsche was walking beside a Swiss Alpine lake when a thought that he would alternately extol and execrate as ""the thought of all thoughts""—one that would beleaguer the rest of his life and work—flashed through his mind.  The thought first appears in Nietzsche's 1882 collection of aphorisms, The Gay Science, as a hypothetical question: ""What if some day or night a demon were to steal after you into your loneliest loneliness and say to you: 'This life as you now live it and have lived it, you will have to live once more and innumerable times more; and there will be nothing new in it, but every pain and every joy and every thought and sigh and everything unutterably small or great in your life will have to return to you, all in the same succession and sequence….?'""  A curious intertwining of infinity, repetition, and fate, known as the Eternal Recurrence of the Same, this idea would itself recur in the philosopher's writing, particularly in his self-proclaimed masterwork and quasi-hermetic text Thus Spake Zarathustra.
Nietzsche's Eternal Recurrence, or the notion that, as Steven Gambardella expresses it, ""time is a circle and all things are bound and repeated for eternity,"" defies literal belief; however, it should not be reduced to a metaphor nor dismissed as, in one scholar's words, ""a bizarre, fanciful, and poetic idea.""  More properly, Gambardella proposes, it is ""part-parable, part-thought-experiment,"" a stern summons to re-examine our relationship with life and carefully consider the consequences of our choices.  In Gambardella's view, Nietzsche's concept provides ""an alternative ethical benchmark to religious ethical systems.""  Whereas many religions promise followers an afterlife of eternal reward or punishment based on conduct in this life, Nietzsche posits only the prospect of ""an impermanent life repeating eternally.""  Similarly, Lawrence Hatab describes the Nietzschean universe as ""a finite-world-in-infinite-process.""  This endless repetition of the same has, Hatab stresses, the power to eradicate all metaphysics, remove the teleology associated with linear time, and erase ""all afters and beyonds,"" leaving nothing but ""the here and now.""
The consequent eternalization of the transitory moment underlies what Nietzsche considers the superlative struggle.  As the philosopher explains, ""The question in each and every thing is, 'Do you desire this once more and innumerable times more?'  If this thought gained possession of you, would [it] change you…or perhaps crush you?""  In describing the philosopher's idea as ""a deceptively simple yet profound proposition,"" writer Maria Popova acknowledges that the concept of Eternal Recurrence challenges us to confront ""the specter of infinite monotony,"" a task requiring the courage to accept—indeed, to want—what is and demanding nothing less than ""the ultimate embrace of responsibility.""
This doctrine of complete acceptance links Nietzsche's Eternal Recurrence to a concept known as amor fati—a ""love of fate"" that exhorts one to affirm life's every detail.  Nietzsche believed such extreme affirmation epitomized bravery: ""My formula for greatness in a human being is amor fati: that one wants nothing to be different, not forward, not backward, not in all eternity.""  One must ""[n]ot merely bear what is necessary…but love it""—as if having willed to happen exactly what has happened.  This imperative tolerates no omissions: ""Have you ever said Yes to a single joy?...then you have said Yes too to all woe.  All things are entangled, ensnared, enamored; if ever you wanted one thing twice…then you wanted all back.""  Nor are revisions allowed: unlike the protagonist of the film Groundhog Day who can make different choices in reliving the same February 2nd over and over, one cannot redo anything.  Nevertheless, Nietzsche proposes that radical affirmation can simultaneously redeem what was, is, and will be.",Critical Analysis & Reasoning Skills,4,Humanities
9,Passage Title: Film Noir,"Dominated by stock character types and gloomy urban settings, film noir is a highly stylized art form with plots that pivot around crime and intrigue.  Chiefly a product of 1940s and 1950s Hollywood, noir was arguably more an approach, mood, or sensibility than a discrete genre.  Noir drew inspiration for its scenery from classic movies about bootlegging and heists but primarily imported its storylines and dialogue from the crime novels and pulp magazine tales of the Depression Era.  At the same time, noir imitated the unsettling cinematography of the German expressionist movement of the 1920s and borrowed from French poetic realist cinema a fundamentally jaundiced and tragic worldview.  As an unlikely amalgamation of ideas and styles, film noir also readily absorbed other contemporary cultural strains, notably existentialism and psychoanalysis.
Most noir-type films belong to what is known as the ""B"" movie category—low-budget, black-and-white motion pictures originally produced to accompany a higher-budget or ""A"" film as a double feature—and display certain conventions that give them a distinguishing character.  Apart from a brooding mood, or pervasive sense of pessimism and meaninglessness, noir conventions include the investigation of a crime and a difficult-to-decipher plot unfolding in flashback sequences accompanied by voiceover narration.  Camera angles are distorting and disorienting, creating a sense of menace.  Chiaroscuro or contrast lighting predominates, with characters often appearing in silhouette or caught within the contrasting shadows cast by Venetian blinds.  Dark and rainy streets illuminated solely by flickering signs point to a dodgy underworld of corruption that is played out in crowded nightclubs, deserted warehouses, or dingy offices.
Standard noir roles include an isolated, ""hardboiled"" detective and a dangerous female character who proves as duplicitous as she is sharp-tongued and glamorous.  Often, a kind of anti-hero is also featured—a man disillusioned and marginalized, down on his luck, desperate to escape his past, and morally ambivalent—who makes an easy target for the double-crossing woman.  Film noir is also replete with witty and ironic verbal exchanges that heighten the tension between characters who traverse treacherous ground in a hostile universe where fate is inexorable.  Such stylistic tropes, along with morally questionable character types, brought noir into conflict with mid-20th-century American values.
Despite conventions so recognizable they have hardened into clichés that lend themselves to parody, as a genre noir remains indistinct.  Although a few films, such as Double Indemnity and Out of the Past, seem paradigmatic of the style, most vary widely in the degree to which they follow the formula.  Many emphasize or are known for certain conceits: The Big Sleep and The Lady from Shanghai are notorious for the many twists and turns of their incoherent plots; Sunset Boulevard and Gilda for their deceptive female characters; and The Maltese Falcon for introducing the quintessential noir detective, deftly brought to life through the fedora-and-trench-coat-clad character of Sam Spade.
Noir is so notoriously ambiguous and elusive that many films from the era flirt with the classification.  For example, 1941 saw the release of renowned film director Orson Welles' Citizen Kane, a quasi-documentary drama concerning a newspaper magnate, which has long been deemed a masterpiece and is considered noir by some critics.  At the least, it was heavily influential in the development of this film style due to its dark cinematography, enigmatic plot of an investigative nature, voiceover, multiple flashbacks, inventive camera angles, and themes of corruption.  Likewise, the 1942 World War II classic Casablanca features chiaroscuro lighting and flashback; moreover, it takes place in a crowded, urban setting filled with suspicious characters and spotlights a bitter loner haunted by his past.  Given such uncertainty, it is fitting that noir was a label applied only after the heyday of these films ended.",Critical Analysis & Reasoning Skills,6,Humanities
10,Passage Title: The Knights Templar,"The seal of the Knights Templar depicts two knights astride a single horse, a visual testament of the order's poverty at its inception in 1119.  Nevertheless, these Knights of the Temple—who swore oaths not only of poverty but of chastity, loyalty, and bravery—would eventually become one of the wealthiest and most powerful organizations in the medieval world.  So far-reaching was their strength and acclaim that their destruction must have seemed as sudden and surprising as it was utter and irrevocable.  The signs of danger could not have been wholly invisible, however, as the Templars' growing influence became perceived as a threat to European rulers.
The first Templars were nine knights who took an oath to defend the Holy Land and any pilgrims who journeyed there after the First Crusade.  Having secured a small benefice from Jerusalem's King Baldwin II, the knights inaugurated their mission at the site of the great Temple of Solomon.  They quickly attracted widespread admiration as well as many recruits from crusaders and other knights.  Within a year of its founding, the order received a financial endowment from the deeply impressed Count of Anjou, whose example was soon followed by other nobles and monarchs.  As early as 1128, the Templars even gained official papal recognition, and their wealth, holdings, and numbers swelled both in the Holy Land and throughout Europe, especially in France and England.
However, this growing power contained the seeds of the order's downfall.  Although the Templars were generally held in high esteem, the passage of time saw censure and suspicion directed toward them.  The failure of the disastrous siege of Ascalon in 1153 was attributed by some to Templar greed.  Similarly, in 1208 Pope Innocent III condemned the wickedness he believed to exist within their ranks.  Moreover, their increasingly elevated status brought them into conflict with established authorities.  One revealing example occurred in 1252 when, because of the Templars' ""many liberties"" as well as their ""pride and haughtiness,"" England's King Henry III proposed to curb the order's strength by reclaiming some of its possessions.  The Templars' response was unambiguous: ""So long as thou dost exercise justice thou wilt reign; but if thou infringe it, thou wilt cease to be King!""
Ultimately, the impoverished Philip the Fair of France joined forces with Pope Clement V to engineer the Templars' downfall beginning in 1307.  Conspiring to seize the order's wealth, Clement invited the Templar Master, Jacques de Molay, to meet with him on the pretext of organizing a new crusade to retake the Holy Land.  Shortly thereafter, Philip's forces arrested de Molay and his knights on charges the preponderance of which were almost certainly fabricated.  Ranging from the mundane to the unspeakably perverse, the accusations even included an incredible entry citing ""every crime and abomination that can be committed.""  Suffering tortures nearly as horrific as the acts of which they were accused, many Templars confessed.
Not everyone believed these charges.  Despite Philip's urging, Edward II of England remained convinced that the accusations were false, a view seemingly shared by other rulers.  Nevertheless, Clement ordered Edward to extract confessions, a task the king tried to carry out with some measure of mercy.  By 1313, the Templar Order had been dissolved by papal decree, and many of its members were dead.  The following year, Jacques de Molay was burned at the stake after declaring that the Templars' confessions were lies obtained under torture.  Stories would spread that as he died, he condemned Clement and Philip to join him within a year.  Although it is impossible to say whether he truly called divine vengeance down upon them, within a few months' time both pope and king had gone to their graves.",Critical Analysis & Reasoning Skills,4,Social Sciences
11,Passage Title: Making the Liberty Bell,"What would later be known as the Liberty Bell initially had no specific connection to American freedom.  In fact, the Whitechapel Foundry of London created the original bell in 1751, at the request of the State House of colonial Philadelphia.  When the bell finally arrived several months later, the assembled onlookers' anticipation turned to horror as it was rung and immediately cracked.  The Philadelphians accused the Whitechapel Foundry of poor workmanship, while Foundry workers countered that the Philadelphians caused the crack by ringing the bell incorrectly.
Although the Foundry's response might seem like a flimsy excuse, that impression would be misleading.  According to Hanson et al. in a metallurgical study, ""Despite the massive appearance of large bells, they are quite fragile, and cracking may result from inferior casting techniques and vibrational stresses produced by improper ringing.""  The study further explains that ""[t]he optimum composition for bell metal was considered to be about 77% copper and 23% tin.  Alloys containing higher amounts of tin are brittle and thus unsuitable for large bells.""  Therefore, it would have seemed plausible at the time that either party could potentially have been responsible for the bell's damage.
The above ratio of metals comports fairly closely with knowledge from hundreds of years prior, as a twelfth-century set of instructions specifies that bells should be composed of four parts copper and one part tin to possess sufficient structural integrity.  In addition to these durability requirements, medieval bell-makers had also identified the correct ratios between the components of a bell to produce the desired tone, based on the thickness of its soundbow, the area struck by the bell's clapper.  The diameter of the bell's mouth should be fifteen times the thickness of the soundbow, and its upper portion or shoulder seven and a half times that thickness.  Likewise, the height from shoulder to mouth should be twelve times the thickness of the soundbow.
Thus, aside from minor refinements, the process of casting large bells has remained relatively unchanged for centuries.  An inner mould or core is constructed of brick, then covered with clay, animal hair, and manure.  The core is wound with rope, which, when later removed, creates the space for the metal.  The cope or outer mould is shaped around the core, while a third mould for the bell's crown is fitted over the top.  The entire construction is then encased in the ground, leaving two holes to pour the liquid bronze while allowing air to escape.  After several days of cooling, the new bell is ready for final tuning.
In the case of the Liberty Bell, Philadelphia brass workers Pass and Stow were commissioned to recast the cracked contribution from Whitechapel in 1753.  Melting the original bell down, the men added several pounds of copper to strengthen the alloy and prevent another crack.  This process was successful but the bell's tone was subpar, so they recast the bell again before it entered into service.  Its most famous ringing was on July 8, 1776, when its peals announced the signing of the Declaration of Independence.  It was also rung on other significant occasions, including the funeral of Chief Justice John Marshall in 1835, when it cracked once again.
In the years before the Civil War, abolitionists adopted the ""Liberty Bell"" as a symbol representing liberation from slavery.  However, it is arguably remembered more today for its now iconic crack.  In 1960, drillings taken from the Liberty Bell were found to contain about 73% copper and 24% tin, with small amounts of lead and other elements composing the rest of the alloy.  A 1975 analysis using fluorescent X-rays further revealed that the bell's chemical composition varies at different points along its surface, shedding additional light on the reasons for the bell's structural failure.",Critical Analysis & Reasoning Skills,4,Humanities
12,Passage Title: Shang Dynasty Oracle Bone Inscriptions,"The earliest documentation of Chinese religious belief comes from the oracle bone inscriptions of the Shang dynasty, which, intriguingly, the newest archaeological evidence has now dated to approximately 1554–1040 B.C.E.  These inscriptions reveal several aspects of Shang spiritual practice, which was dominated by attempts to divine the future and to secure the aid of various natural and supernatural powers.
The term ""oracle bone inscriptions"" derives from the method the Shang used to seek guidance from the spiritual world.  An animal bone, most commonly a tortoise shell or cow scapula, would be inscribed with a statement, such as ""on the tenth day, there will be rain"" or ""we should go to battle.""  The shell or bone was then exposed to a heated piece of metal, and the king would interpret the resulting cracks as either affirming or denying the inscription.  Usually, the king's pronouncement would then be recorded.
In some cases, the outcome of the king's forecast was also inscribed—however, these cases were almost always when the king had been correct in his divination.  This fact could be seen as evidence that the primary function of the oracle bones was to legitimize the king's authority.  For instance, regarding an inscription that ""there will be no disasters,"" the Shang ruler Wu Ding interpreted the oracular cracks negatively and predicted that ""alarming news"" would soon arrive.  Four days later, the pronouncement was viewed as confirmed when a messenger reported attacks on two settlements.  The king's important role as an intermediary between the divine and physical realms was therefore reinforced.
However, legitimation of royal authority could not have been the only function of the divination process, as there are a limited number of records in which a king failed to predict the future accurately.  In addition, the sheer number and scope of inscriptions suggest that the Shang took otherworldly powers very seriously, and sincerely believed in their ability to affect human life.
The greatest of these powers was Di (or Shangdi), the high god of the Shang.  Able to send or withhold rain and wind, bring fortune or misfortune upon the land, grant military assistance, or even induce other peoples to attack China, Di was seen as essential to the country's well-being.  As a result, one might naturally expect Di to have been the subject of elaborate rituals and sacrifices.  In fact, however, there were no such ceremonies for Di.  Di was viewed as distant from the people and willing to harm as well as aid them throughout various times.  Because of this seeming ambivalence, the Shang instead sought to incur Di's favor through the intercession of their deceased kings.  While these kings were not as powerful as Di, they could bestow some measure of blessing (or bane) upon the living ruler and his subjects.  Moreover, as former humans themselves, they were considered more approachable by the petitioners who sought their aid.
A third group of powers comprised natural spirits, such as the Yellow River Power or the Great Winds.  These forces lacked the direct control over events that Di possessed; however, they did have some influence over natural phenomena.  Hence, both for such powers and for the ancestor kings, some oracle bone inscriptions described a wish and an attempt to gain an entity's favor; for example, ""to the Mountain we pray for rain and offer a burnt sacrifice.""
The frequency and prominence of oracle bone inscriptions, as well as their emphasis on the role of divine forces, evince a high degree of spiritual concern in the time of the Shang dynasty.  These practices and beliefs would guide the course of Chinese thought through successive centuries.",Critical Analysis & Reasoning Skills,4,Humanities
13,Passage Title: The Great Sphinx,"Accompanied by his army and a retinue of scientists and scholars, Napoleon arrived in Egypt in 1798 on his campaign to control the eastern Mediterranean.  On the Giza Plateau, several miles from Cairo, he encountered a massive stone head protruding from the sands, displaying an Egyptian royal headdress and a badly damaged nose.  In the wake of Napoleon's exploits, tales of such archaeological wonders swept across Europe, giving rise to Egyptomania.  In 1817, Italian Egyptologist Giovanni Battista Caviglia led the first modern archaeological dig to rescue the monument from the desert sands.  The recumbent body of a lion was revealed to be supporting the colossal pharaoh head, and the monument became known as the Great Sphinx.  Artifacts from various time periods show that the statue had been dug out no fewer than 6 previous times, the last at the order of the Roman emperor Marcus Aurelius in AD 160.  However, the Sphinx's body would not be completely freed until French Egyptologist Emile Baraize conducted extensive excavations of the monument and surrounding area in 1926.
Thought to date back 4,500 years, the Great Sphinx is the oldest monolithic statue in the world.  It is also the largest, measuring 241 ft long, 63 ft wide, and 66 ft high.  As part lion, the creature most associated with kingship and the sun, the statue served to concretize the pharaoh's identification with Re, the great Egyptian solar deity.
The Sphinx is part of a complex of structures on the plateau including the Great Pyramid, the Causeway, the Sphinx Temple, and the Valley Temple.  Believed to be dedicated to the 4th Dynasty pharaoh Khafre, the complex formed a necropolis with the Sphinx as guardian.  Interestingly, these prodigious stone structures were built not by slaves, as commonly assumed, but by Egyptian civil servants who held a privileged position in society.
Unlike the head of the Sphinx, which is solid, the limestone rock from which the body was carved is friable and prone to disintegration.  Consequently, it has been restored numerous times beginning in antiquity.  Notably, the forepaws of the Sphinx enclose an open temple containing a stele or stone slab.  Dating from circa 1400 BC, the inscription on the stele relates how the god Re appeared in a dream to the young Egyptian prince Thutmose IV, promising that if he dug the Sphinx out of the sand, he would be crowned Pharaoh of Upper and Lower Egypt.  Archaeological evidence confirms that Thutmose not only freed the Sphinx but, having discovered that some of the stones of its body had fallen off, restored them to their proper place and filled in damaged areas.  Lastly, Thutmose had a wall built around the area of the Sphinx, its mudbricks inscribed with his name.
Focus on conservation has stimulated controversy over the statue's age.  An alternative theory dates the original construction of the Sphinx back to 7000 BC or earlier.  Some experts claim that wind and sand would not have caused the pattern of erosion seen on the Sphinx's lower body; more likely, it was the effect of heavy and repeated rainfalls on the limestone.  While many scholars dispute this theory, geological studies have revealed that the Giza Plateau, which sits on the edge of the Sahara Desert, had a few millennia before been a rich and fertile plain subject to frequent precipitation.
Contemporary Egyptologists were dismayed when, in 1988, a piece of the Sphinx's shoulder collapsed onto the ground.  Reconstructive efforts on the statue were accelerated and continue to this day, guided by advances in science and technology.  Despite the increased attention, the Sphinx has not yet revealed all its secrets but remains an archaeological riddle.",Critical Analysis & Reasoning Skills,4,Humanities
14,Passage Title: Everyday Heidegger,"When twentieth-century German thinker Martin Heidegger contemplated the history of philosophy in the West, he noticed something astonishing: it seemed to have forgotten the most fundamental of all philosophical questions, the question of Being.  After all, what does it mean to be, or to say that something exists?  Moreover, he wondered, why are we not more perplexed by this perplexity?
Heidegger tackled such questions in Being and Time—a nearly impenetrable work apt to frustrate and flummox the reader at every turn.  In this tortured and intimidating tome, Heidegger strung words into hyphenated chains to build terms better befitting his dense ideas and dared to target the lofty towers of traditional Western metaphysical thought.  Above all, he believed that the revered triumvirate of Socrates, Plato, and Aristotle had diverted the course of philosophy by turning away from ontology, or the study of Being, toward a metaphysics that merely pondered the nature of individual entities.  Furthermore, Heidegger thought that Descartes, as the first modern philosopher, had exacerbated this misguided focus.  In struggling to establish a foundation for knowledge, Descartes had posited his existence as the precondition of the only thing he could not doubt—the fact of his own thinking.  However, in Heidegger's estimation, Descartes seemed to overlook the fact that existence itself presupposes an already suitable context, or meaningfully structured situation, in which to exist.  While this Cartesian paradigm effectively severed self from world and subject from object, Heidegger adopted a nondualistic, phenomenological approach wherein perceiver and perceived are interdependent.  He also began his quest for the meaning of Being with ordinary, everyday experience, with what it means to dwell within the world as a human being—the only being sufficiently aware of its own existence to question it.
Accordingly, Heidegger sought to gain an understanding of Being by analyzing the inherent structures of our basic ontological condition, the reality of our distinctively human existence or mode of Being, which he referred to as Dasein—a term roughly translated as ""being there.""  Heidegger saw Dasein's primary structure as ""thrownness"": we are summarily tossed into the world and into a particular configuration of circumstances that impose limitations but also provide possibilities.  Overall, we belong to a world that is exquisitely attuned to us and toward which we, in turn, are uniquely disposed.  This existential condition of ""being-in-the-world"" implies complete engagement with our environment: we are inextricably entangled and deeply invested in its intricate web of people and things.  Moreover, in the day-to-day business of living, we play shifting roles within countless contexts and subcontexts that form an interactive, seamlessly integrated totality of which we are the co-creators.
Consequently, we are interconnected not only socially but ""equipmentally.""  The world is filled with objects we can manipulate for a variety of purposes.  For example, a hammer is typically not an object we stop and analyze but a tool we pick up and use, experiencing it more as an extension of our hand than an ontologically distinct entity.  Therefore, the hammer's common mode of Being is usefulness—in Heideggerian terminology, it is ""ready-to-hand"" rather than merely ""present-at-hand.""  Likewise, a boat on the water is only secondarily an object for observation and interpretation.  It functions primarily as a piece of equipment within a nexus of related such items that might include oars, fishing tackle, and life preservers.  Furthermore, the vessel operates within many possible domains—boatbuilding, fish markets, travel—and might be used in sundry activities by diverse individuals.
Thus, in trying to ""call us back to the remembrance of Being,"" so long consigned to oblivion, and to overturn Western philosophy, Heidegger pitched ordinary experience into the ontological register, grounding metaphysics in everyday reality.",Critical Analysis & Reasoning Skills,4,Humanities
15,Passage Title: Aristotle's Rhetoric,"On first reading, there appears to be significant tension between what Aristotle initially claims in the Rhetoric and what he later proceeds to argue.  At the beginning of Book 1, the philosopher admonishes his students toward conscientious public speaking, denouncing the sophistical practices of arguing for false or unjust positions and making emotional appeals to the judges in court.  Rather, he instructs, one must argue only for what is true and good, and do so by appealing to proper reasoning and facts.  In light of Aristotle's other works, none of this is surprising.  What is puzzling, however, is that as soon as the second chapter he seems to suggest the opposite.
For example, one of the modes of persuasion discussed there entails putting the audience into ""a certain frame of mind"" which ""stirs their emotions.""  One might suppose that Aristotle is not advocating this technique but merely acknowledging it as a common practice.  However, that explanation is undercut by the subsequent recommendation about written and unwritten law: ""If the written law tells against our case, clearly we must appeal to the universal law, and insist on its greater equity and justice….  If however the written law supports our case, we must urge that…not to use the laws is as bad as to have no laws at all.""
Another suggestion might be that Aristotle's view evolved, and that extant copies of the Rhetoric's first chapter are from an earlier version of the work.  However, this position would conflict with most of the Aristotelian corpus.  Notably, the Metaphysics bases all reasoning on the law of non-contradiction, and the Nicomachean Ethics associates injustice with taking more than one's deserved share.  Both are a far cry from the Rhetoric's call to affirm or dispute the legality of contracts depending on ""whichever way suits us.""  This apparent shift toward sophism thus seems strangely anomalous.
A more fruitful resolution to this conundrum lies in drawing an important distinction.  While the Rhetoric does contain two conflicting implications about public speaking, the change in Aristotle's position may affect the method rather than the goal of rhetoric.  One point in favor of this interpretation is Aristotle's view of judges as a necessary but regrettable feature of the courts.  Because laws cannot cover every conceivable situation or contingency, judges are needed to decide whether a man has acted lawfully in a particular circumstance.  Unfortunately, this makes the justice system subject to potential bias or corruption, or simply the errors of fallible judgment.  The latter concern is considerably heightened due to dishonest orators, who may use emotional appeals and deceptive reasoning to confuse the judges into inadvertently rendering an unjust decision.
A virtuous speaker thus faces a choice: (1) to present his case logically and without appeal to emotions or chicanery, trusting that the truth will prevail; or (2) to use every means at his disposal, noble or base, to achieve victory.  If Aristotle had observed enough instances of legal injustice, he may well have concluded that unwavering purity of principle simply risks too much.  Perhaps he even had Socrates in mind, who famously upheld (1) at his own trial yet was convicted and executed through corruption in the Athenian courts.
On this line of thought, Aristotle came to believe that ensuring the right outcomes justified whatever techniques were needed to secure them.  His instruction would therefore presuppose that a virtuous speaker always argues for the correct side.  Otherwise disreputable oratorical methods would then be permissible in service of a righteous cause, especially if one must counter similar tactics from an opponent.  This interpretation alleviates the tension within the Rhetoric by making the aim of achieving justice a precondition of those modes of argumentation.",Critical Analysis & Reasoning Skills,6,Humanities
16,Passage Title: Chess,"Since its origins in India around the 5th century C.E., the game of chess has gradually evolved into the modern form now played across the globe.  Although most people have some idea of what chess entails, there are many misconceptions about its practice.  For instance, a non-player might wonder about the maximum number of moves that can be planned in advance, but an experienced player would find that question naïve.  The unsatisfying but accurate answer is that it depends on the position, because the opponent's potential moves will at some points require calculating numerous possibilities but at other points will be an entirely forced sequence.  Similarly, a common scenario in fiction is for a player to defeat an opponent by sudden surprise—the rival's look of confidence morphing into complete disbelief as the winner unexpectedly proclaims ""Checkmate.""  In real chess such abrupt triumph occurs rarely or never, as the inevitability of checkmate will have been readily apparent to both sides.
Checkmate is the primary goal of chess, but it is not the only way in which games may end, nor even the only way in which games may be won.  Indeed, at higher levels of play checkmate is uncommon, because the losing side typically resigns rather than prolong an unavoidable defeat by playing until checkmate has occurred.  One may also win ""by flag"" if the opponent runs out of time.  Nonetheless, not all games of chess result in a win for either side.  A draw may ensue by agreement of the players, by stalemate, or under certain other conditions.
Public discourse about chess can occasionally veer into the ridiculous.  In 1912, it was reported that a move made by chess champion Frank Marshall elicited such admiration from onlookers that they showered gold pieces onto the board.  While the move may indeed be considered a brilliancy (thought by some to be the most beautiful chess move ever played), the story must otherwise be spurious.  Even supposing that the viewers were so impressed as to engage in such specific and effusive praise, precisely how much gold were they supposed to have had?
At one time, the prospect of machines playing chess was widely deemed absurd.  Writer Edgar Allan Poe declared in 1836 that its impossibility could be established a priori because, unlike the steps of a mathematical demonstration, ""[n]o one move in chess necessarily follows upon any one other.""  In the early 19th century, a human-shaped, turban-wearing machine called ""The Turk"" toured the United States as a supposed chess-playing automaton.  Although its owner claimed and seemed to prove that it was purely mechanical, Poe correctly concluded that the device was actually operated by a real person hidden inside its cabinet.  Despite this subterfuge, however, machine chess was not fundamentally impossible as Poe believed.  The early chess computers of the 1960s possessed only limited abilities, but by 1997 IBM's ""Deep Blue"" had defeated world champion Garry Kasparov.  Today, the strongest chess computers are unbeatable in fair contests by even the most skilled human grandmasters.
Nevertheless, however unassailable in the realm of calculation, computers will always lack the most valuable trait of any chess player.  Blaise Pascal once wrote: ""When the universe has crushed him man will still be nobler than that which kills him, because he knows that he is dying, and of its victory the universe knows nothing.""  In a similar sense, although computers may ""win"" at chess, they can never experience their victory.  The artistry of a move like Marshall's will forever be lost on them, while a human being can view and partake in its beauty, making the enjoyment of chess a distinctly human pleasure.",Critical Analysis & Reasoning Skills,6,Humanities
17,Passage Title: Re-affecting the Stage,"Throughout the twentieth century, the performer-spectator dynamic has been challenged both in artistic practice and from a theoretical perspective.  Contextual Futurists, nonsensical Dadaists, and the surreal avant-garde theatrical experiments upended the conventional notion of passive spectatorship, paving the way for performers to disrupt the invisible wall between the artist and audience altogether.  Most scholarly attention, however, has traditionally been directed toward one-way theatrical practices by which performers engage, transform, and heighten the bodily state of the audience through a framework of communal ""felt-experience,"" and not vice versa.
The actors on stage are ostensibly the central object of attention, yet their communication with those in attendance is not solely a one-way discourse.  After the curtain falls at the end of a performance, it is common to hear professional performers commentate on audience reaction: sometimes the spectators were ""attuned""; at other times they seemed ""aloof.""  Occasionally, performers feel as if they ""captured"" the audience; at other times, they perceive that they ""lost touch"" with the viewers entirely.  One could say that this jargon is merely an oversimplified and closed communication that reduces the true complexity and aesthetic dimensions of the theater experience.
Nevertheless, performers are ultimately the vessels responsible for detecting and absorbing the moods, attitudes, and emotions of individuals, and these aspects are vital to the apparent success or failure of the performative process.  One day, actors might find the audience energetic, welcoming, and appreciative; the next day, they may find a stiff, critical, and disconnected crowd, even when performing in the same production with unchanged levels of enthusiasm.  In some instances, a single audience may be unmoved, regardless of how well a show is performed.  In concert with this insight, the language used by artists from across the globe to portray such audience encounters is notably similar.  Their predilection for tactile idiomatic expressions when describing the performer-audience connection is perfectly captured in these remarks from a well-known stage actor:
<""The level of attention the audience gives to what is happening on stage provides a certain quality of stillness that makes it possible for a performer to know whether the audience is attuned or not.  However, for the audience to be 'with' the performer, it must embrace a state of tension and immerse itself in the profundity of the performance…it's a very 'tangible' moment—it's all I can find as a word.""
Because the audience is the proclaimed foundation of a theatrical event, a staged play aims to affect the audience, usually by ""capturing"" the viewers in some poignant manner.  According to this paradigm, there can be no performance without an audience.  Despite this, the presence of spectators does not guarantee that a meaningful emotional, or affective, exchange will transpire.  Such collective encounters in a shared space and time present only the possibility—for connection or disconnection; and, accordingly, an audience member must be physically present and willing to be affectively influenced.
The audience serves an integral function in the performance: it activates, intensifies, and amplifies the circulation of emotional affect in a communal social space.  Individually and collectively, each spectator is able to participate in the intrinsically variable theatrical plot.  Affect is thereby experienced simultaneously through action, thought, and perception by both the artist and the audience.  This mutual ""transmission of affect"" induces a corporeal sentiment that ultimately resonates as a palpably emotional atmosphere.  Entertainers can only know if the audience is ""present"" to the degree that they also embrace the mutual tension of the experience.  In this way, the performer dutifully influences the audience, and the audience, in essence, ""re-affects"" the stage.",Critical Analysis & Reasoning Skills,6,Humanities
18,Passage Title: The Walls of Jericho,"The ancient city of Jericho has long been famous for its walls.  Every Sunday school child learns about the Israelite siege of that stronghold, and even Elvis memorably sang about how ""the walls came tumbling down.""  However, the historical record surrounding Jericho has caused considerable scholarly controversy.  Archaeological evidence does confirm the existence of that walled city, located at modern day Tell-es Sultan in Palestine. Yet, this same evidence raises a number of mysteries.
Initial radiocarbon dating indicated that Jericho's original walls were built in around 6900 to 6800 B.C.  As Emmanuel Anati noted in a 1962 publication, that would not just make Jericho ""the only place in the world where a walled town has been found to date back to nine thousand years ago,"" but also make it ""four thousand years older than any similar settlement known at present.""  Moreover, the city would have been politically unique for its time and region, as constructing the fortifications—including a defensive ditch and tower in addition to the walls—must have required extensive coordination of labor by a strong, centralized leadership.  Other contemporaneous groups were generally much smaller and more loosely organized, often semi-nomadic confederations largely dependent on hunting.
These and other considerations have divided the archaeological community, with scholars such as Robert Braidwood stating that the supposed radiocarbon dates are too early and must be wrong.  Others, like British School of Archaeology director Kathleen Kenyon, accept the accuracy of the dates but seek to explain why Jericho appears to represent such an anomaly.  One issue relates to Jericho's food supply, which must have been substantial to maintain a large, permanent city.  In 1954, Kenyon concluded that ""the basis of the settlement must be successful agriculture,"" further noting ""the spring by which the settlement is situated, assuring the productivity of the fields.""  But Braidwood disputes this conclusion, arguing that Jericho's land was no more suitable for agriculture than were neighboring areas.  Another puzzle is why Jericho, if it was particularly conducive to cultivating crops, seems to have been abandoned at certain points in its history while nearby settlements flourished.  All sides in the debate agree that hunting would have been insufficient to sustain a population of Jericho's size.  Fishing has been suggested as an alternative possibility, but this too is widely deemed unlikely to have provided adequate support for Jericho's growth.  Moreover, that suggestion depends partly on the disputed supposition that the waters of both the Dead Sea and the Jordan River were more voluminous in ancient times than today.
Anati argues that the Dead Sea indeed played a critical role in Jericho's sustenance, but in a different way, namely, by providing valuable economic resources.  Especially in light of the problems identified with other proposed explanations, Anati concludes: ""Only trade comes seriously into consideration as a major means of accumulating the surplus necessary to maintain such a wealthy town in the seventh millennium B.C.""  As he points out, tools and ornamental items originating from Anatolia, Sinai, and the Mediterranean coast have been found at ancient Jericho.  More importantly, in surrounding regions ""the Dead Sea has always been the principal source of salt,"" which was ""becoming an indispensable substance [for preparing and preserving food] at exactly the time when Jericho was becoming a walled town.""  The sea would also have yielded large amounts of bitumen, useful for fashioning tools and waterproofing containers, as well as sulphur, which even today is ""still used by Bedouin to prepare medicines and to light fires.""  Hence, the profit obtained from exporting these and other goods could plausibly account for how and why Jericho occupied its unique position in the ancient Near East.",Critical Analysis & Reasoning Skills,5,Social Sciences
19,Passage Title: Cartesian Dreams,"By the seventeenth century, the noxious fumes of epistemological doubt were rapidly seeping into the sensibility of the European intelligentsia.  Society had been left reeling from the world-altering discoveries and inventions of the past two centuries.  Additionally, the seismic shifts of the Renaissance and Reformation had set churchmen at odds with other thinkers and ignited devastating religious wars.  Scholasticism—that longstanding but unwieldy edifice of Catholic dogma buttressed by Aristotelian reasoning—was now tottering.  Into this collapsing Weltanschauung was born René Descartes—a thinker seemingly destined to become the symbol of a new totalizing skepticism as well as the progenitor of a rationalism so absolute it promised to unite the sciences and philosophy into a single system of thought.  Less overtly, however, Descartes would embody the irreconcilable differences of an age in which faith and reason, superstition and science, and alchemy and chemistry vied with one another.
By the time Descartes had completed his preliminary education and entered law school, he was thoroughly disillusioned.  As Paul Strathern notes, the would-be philosopher felt only scorn for the steady diet he had been fed of ""rehashed Aristotle encrusted with centuries of interpretations.""  Believing that mathematics alone afforded certainty, he nimbly circumvented what Strathern calls ""the morass of metaphysics.""  Abandoning his studies and seeing the Thirty Years' War raging through Europe, the now aimless youth enlisted as a volunteer in the army.  While garrisoned in the Netherlands, he encountered Dutch mathematician and thinker Isaac Beeckman, who renewed his appetite for original thought.  Descartes began to imagine that by applying the laws of mathematics to philosophy he could achieve the certainty he craved.
Months later in Bavaria, when inclement weather forced him to sequester in a stove-heated room, Descartes had leisure to cogitate.  His diary entry for 10 November 1619 read: ""I was filled with enthusiasm, discovered the foundations of an admirable science, and at the same time my vocation was revealed to me.""  Indeed, that fateful night brought feverish dreams that would propel him toward revitalizing philosophy.
In the first dream, Descartes found himself pursued by apparitions while fierce whirlwinds prevented him from retreating inside a church.  He awoke, only to be terrorized by a vision in which a lightning bolt flooded his room with countless sparks.  Finally, in a third dream, a dictionary and a poetry anthology materialized on a table next to him.  He opened the anthology to a verse by the late Latin poet Ausonius entitled, ""Which path in life will I choose?""  The shadowy figure of a man appeared and recommended another poem beginning with the Pythagorean phrase ""Yes and No.""  Descartes was sure that this verse too was included in the collection but could not find it.  Turning to the dictionary, he noted some missing pages.  He tried to converse with the stranger, but the interloper suddenly vanished.
While still slumbering, Descartes began to decipher this last dream: the dictionary represented the sciences, whereas the poetry anthology symbolized the union between wisdom and philosophy, and ""Yes and No"" alluded to the accuracy or error of human understanding.  Upon waking, he interpreted the winds from the first dream as forces directing him away from Scholasticism and the lightning bolt of the second as ""the signal of the Spirit of Truth.""  The rational thinker was convinced that these overpowering dreams had descended ""from above.""  As reported by his earliest biographer, Adrien Baillet, Descartes ""thought he could perceive through their shadows the signs of a path which God had marked out for him"" because of his commitment to the truth.  Thus, this rising ""Father of Modern Philosophy"" whose instinct was to doubt everything drew the inspiration for his theories from the elusive realm of dreams.",Critical Analysis & Reasoning Skills,6,Humanities
20,Passage Title: When Defense is Indefensible,"Suppose a prosecutor is considering whether to bring a case to trial.  He is not sure that the suspect is guilty—in fact, based on the evidence, it's more likely that the suspect is not guilty.  Nevertheless, he feels confident he can secure a guilty verdict.  His powers of persuasion are considerable, and there's a good chance he could trick a jury into believing the evidence is strong instead of weak.  In addition, the case is high profile and could be very lucrative; winning would likely lead to a substantial raise or promotion.  He decides to charge the suspect, and ultimately succeeds in persuading the jury to convict.
Looking at this situation, most of us would easily judge the prosecutor as extremely unethical.  His conduct is outrageous and wrong—he clearly acted with corrupt intent, perpetrating injustice in order to profit financially.  Why is it not shocking, then, that we tolerate the mirror image of this behavior from defense attorneys?  For they engage in the same outrageous conduct, only on the other side.  Paid handsomely to represent even the vilest of clients, they apply their oratorical prowess to manipulating jury perception, keeping the guilty free and unpunished in exchange for money and status.  To the extent that this behavior takes place, are some defense attorneys as unethical as our hypothetical prosecutor?
It is worth distinguishing two senses of the word ""ethical"" here.  For there are standards of professional ethics to which any attorney must conform, including standards particular to the defense.  Most relevant to our purposes, a lawyer is obligated to provide their client with a ""zealous defense.""  In other words, once an attorney takes on a client, they are ethically bound to promote that client's rights, interests, or innocence—in fact, not to do so would be unethical.  Thus, one might try to suggest that this obligation undermines the claim that some defense attorneys act unethically.
However, meeting that professional standard is not the same as being ethical in the general sense of the word.  The standard depends on the condition: once an attorney takes on a client.  With the exception of court-appointed attorneys or public defenders, who are assigned to provide representation to those who would otherwise lack it, an attorney is never required to represent a defendant.  Therefore, meeting one's obligations as a defense attorney does not necessarily make one ethical, because the choice to accept a specific case (and thus to incur those obligations in the first place) may itself be an unethical act.
Moreover, the role of court-appointed attorneys is to help protect the rights of citizens who cannot secure their own representation, usually for financial reasons.  Although preserving those rights is necessary to uphold justice, this situation highlights how wealth and class contribute to injustice.  While some defendants possess the means to hire top-level private lawyers, others must depend on public servants—frequently less experienced lawyers from overloaded, understaffed agencies.  As a result, the rich are more likely to escape conviction even when they are guilty, and the poor are more likely to be convicted even when they are innocent.
It is doubtful that private defense attorneys could be somehow forbidden from representing guilty clients.  Hence, the needed reforms to the system must come from individual attorneys committing to work for the right reasons.  For those who strive to ensure citizens' rights, or who truly believe their clients are innocent, providing a defense is a noble undertaking.  But for those whose overriding motivation is greed, that legally ""zealous defense"" is ethically indefensible.",Critical Analysis & Reasoning Skills,4,Humanities
21,Passage Title: Margaret Fuller: Misfit and Messiah,"There can be little doubt that Margaret Fuller embodied a new paradigm for women.  At a time when women were barred from higher education, she was arguably the best-read individual in New England and would become the first female public intellectual in the US.  Having earned the admiration of the eminent Ralph Waldo Emerson, Fuller penetrated the inner circle of the highly influential but short-lived sociophilosophical movement known as Transcendentalism, where her talents found an outlet.  An integral part of both nineteenth-century social reforms and the literary renaissance that bred Hawthorne and Dickinson, Fuller became deeply woven into the fabric of American history.
However, for Margaret—a child prodigy who was reading before the age of 4 and studying Latin under her father's stern but liberal-minded tutelage by 6—realizing her considerable potential would come at a price.  Throughout her brief life, this unfettered spirit would struggle with how to share what she called her ""secret riches within.""  From her earliest school days, Fuller's precocity provoked both awe and ridicule.  Later, when she demonstrated her irrepressible love of learning, she was accused of ""flaunting her erudition in gratuitous digressions.""  Although nonconformity, freedom, and self-development were the prevailing watchwords of Transcendentalism, Fuller was nevertheless stigmatized for exhibiting what some perceived to be ""the bravado of a misfit"" and many deemed ""an unwomanly arrogance.""
Fuller would leave her mark as an impassioned pedagogue, brilliant translator and critic of literature, and daring journalist.  She taught at progressive New England schools before managing to support herself by leading discussion groups for women in Boston.  These seminars, or Socratic-style ""Conversations,"" were meant to supplement the paltry educational opportunities for women by allowing them to explore their intellectual interests and stimulating them to think in new ways.  In 1844, Emerson made Fuller the inaugural editor of the transcendentalist magazine The Dial.  Within 2 years, the caliber of her journalistic contributions spurred an invitation from Horace Greeley to work for the New York Tribune as America's first full-time literary critic, and she went on to become the first-ever female foreign correspondent.
Yet Fuller is probably most celebrated by posterity for the role one biographer has described as ""feminist messiah.""  Her emancipatory tract Woman in the Nineteenth Century was an instant bestseller in 1845 and has since evoked favorable comparisons with perhaps the two greatest manifestos of women's liberation—Mary Wollstonecraft's A Vindication of the Rights of Woman, published a half-century earlier, and Simone de Beauvoir's The Second Sex, published a century later.  Fuller's book also provided much of the impetus for the Seneca Falls women's convention in 1848.
In Woman, Fuller not only sounded the clarion call of social equality for women but also advocated for the rights of all human beings, regardless of gender or race.  A synthesis of classical mythology and German Romanticism led her to believe that the ideal person was a felicitous integration of male and female elements that perpetually flowed into one another.  Thus, although Fuller bemoaned male dominance, she felt it was incumbent upon women to embrace an Emersonian self-reliance and develop their ""Minerva"" or rational, warrior side rather than merely play the traditional part of supporting muse.  Likewise, she urged men to cultivate their ""Apollo"" or intuitive side.  Indeed, Fuller insisted that the well-being of society hinged on replacing the radically polarized model of the sexes with a more balanced one.
A farrago of history and myth featuring performative set pieces such as staged dialogues, fictional vignettes, and political speeches, Fuller's Woman was a tour de force that merited a sequel.  But tragedy would write the ending to the Margaret Fuller story: she perished in a shipwreck off the New York coast on returning from Italy in 1850.",Critical Analysis & Reasoning Skills,4,Humanities
22,Passage Title: Doubting Thomas,"The story of so-called ""Doubting Thomas"" is generally familiar to Christians: after the resurrection Christ appears to his followers, but Thomas is absent from the group at the time.  When the other apostles tell him that Christ has risen from the dead, Thomas replies: ""Unless I see the mark of the nails in his hands, and put my finger in the mark of the nails and my hand in his side, I will not believe"" (John 20:25, NRSV).  The following week, Thomas is present when the apostles meet, and Christ again appears.  Thomas then proclaims his belief, to which Christ replies: ""Have you believed because you have seen me?  Blessed are those who have not seen and yet have come to believe"" (John 20:29, NRSV).  Hence, the description ""Doubting Thomas"" now refers to a person who refuses to accept a claim unless proof is supplied.  Like other biblically inspired terms such as ""Good Samaritan"" and ""Prodigal Son,"" it has entered the general lexicon and is used in both religious and non-religious contexts.  However, the connotations attached to these uses are largely unjustified.
For instance, one Christian environmentalist recently rebuked ""Doubting Thomases"" who dispute the relevance of religion to ecological justice; likewise, a secular article on education criticized the skepticism that a ""Doubting Thomas"" had displayed about a teaching method.  In each case, the term was decidedly negative, reflecting the popular portrayal of Thomas as both wrong and confrontational toward his fellows.  Unswayed by their testimony, Thomas declares his unbelief, challenging the faith of the other apostles and, by extension, the basis of Christianity.  'Unless my standards of evidence are met,' he seems boldly to say, 'I will never believe what you tell me.'  Thus Thomas becomes a symbol of stubborn refusal, a negative exemplar to contrast with the belief to which Christians are called.  But as useful as that depiction may appear to countless sermons about faith, it misunderstands the message of the scripture.
The most important point to recognize in this regard is that Thomas does not ask for anything beyond what the other apostles have already received.  They have seen Christ while he has not, having experienced the physical confirmation that Thomas still lacks.  Hence, Thomas is not unique in wanting proof—or, if he is, then his uniqueness is a matter of chance.  We might well have had ""Doubting Peter"" or ""Doubting Bartholomew,"" if one of them had been absent instead.  With this observation in mind, Thomas' words are cast in a new light: ""Unless I see the mark of the nails in his hands, and put my finger in the mark of the nails and my hand in his side, I will not believe.""  Typically, words in scriptures are not italicized to indicate inflection in this way, but those emphases clearly reflect the intended meaning.  Thomas' remark is far from the defiant challenge to faith it has so often been viewed as.  Instead, it is the mournful, perhaps desperate admission of one who wants to believe but is afraid to hope.
This interpretation also informs how Christ's response to Thomas should be understood.  By referring to ""those who have not seen,"" Christ does not distinguish between Thomas and the other apostles, all of whom believe because they have seen.  He distinguishes between them collectively and anyone who only hears that Christ has risen.  Thus Thomas does not exemplify a blameworthy unbelief, but a joyful confirmation that the testimony he heard about Christ is true.  The message of the scripture, then, is that those who have faith without sight will one day experience that same confirmation.  This lesson of fulfilled hope is what the story of ""Doubting Thomas"" truly represents.",Critical Analysis & Reasoning Skills,4,Humanities
23,Passage Title: Why Study Literature?,"Rhetorical questions with an end purpose can be propositioned with the intention of challenging curricular stratagem, particularly in the name of educational progress.  ""Should we teach literature in our college classrooms?"" or ""Do we have an ethical obligation to teach humanities to the next generation of scholars?"" are the sorts of enquiries that can set precedence in academic circles, providing fodder for discussion on implementing the best instructional tactics in the imminent restructuring of scholastic environments.  One particular issue in academia that is currently under deliberation is the justification for literary study in the classroom.
So why study literature?  First, research has shown that studying the likes of Shakespeare, Wordsworth, and Dickens can generate feelings of human connectedness, an essential component for fulfilled living, social stimulation, and a long and healthy existence.  Immersive or ""deep"" reading of a literary piece serves as a meditation on, an excavation of, and a careful examination of the ubiquitous sociocultural ideas connecting the worlds in which both the author and reader reside.  One study on connectedness also found that emotionally compaginated individuals live longer than those who do not exhibit similar closeness with other humans.  Taken together, it can be surmised that ""deep readers"" may have a superior ability to understand and empathize socially by viewing the world through the differing perspectives of others, enhancing the potential for human connectivity.
Second, it may be beneficial to recall our literary past in its truest form, the written word, over the superfluity of technological representations of ancient texts.  For instance, Shakespeare's body of work has been reduced to two-minute self-produced video clips found on the Internet, a valuable teaching tool for younger students with ever-shortening attention spans but one that lamentably excludes the essential beauty of the original prose.  The writings hold the intended ""environmental literary print"" up to the light, allowing readers to gain a valuable snapshot of history that cannot be substituted.  As our society continues to advance technologically, we can nevertheless find existential value in remembering our past.
Third, if we could measure the unbridled joy felt upon discovering a forgotten text lying on the shelf, we might shift our focus from an extrinsic value of literature to an intrinsic one; that is, the written text is prized due to its ability to incite felicitousness, laughter, woefulness, and a myriad of emotions that make us inherently human.  As George Eliot writes in the canonical The Mill on the Floss, ""We can never give up longing and wishing while we are thoroughly alive.  There are certain things we feel to be beautiful and good, and we must hunger after them.""  This exemplifies the human condition that strives toward fulfillment at our core psyche, a sentiment perhaps best expressed in the written language itself.
Lastly, there is much to be gained from pedagogical models of studying literature:  women's studies teaches the value of gender equality, cultural studies intrinsically teaches less ethnocentrism and more tolerance, and the reader-response theory teaches that each individual response has value based on unique and subjective human experiences, to name a few.
In certain higher education communities, there has been a grassroots movement gaining some traction in favor of removing humanities requirements from the core teaching curriculum, yet the lifelong consequences of doing so may not be quantifiable.  The humanities professors who protest silently, hoping that their colleagues will see the merit in studying literature as requisite, must remain reticent no more.  Without literary study to instill in us the value of the human experience, we may willingly sacrifice more than just coursework:  we may be on the verge of surrendering our human spirit.",Critical Analysis & Reasoning Skills,5,Social Sciences
24,Passage Title: Freudian Choice,"Sigmund Freud opens his 1913 essay ""The Theme of the Three Caskets"" with the following comment: ""Two scenes from Shakespeare, one from a comedy and the other from a tragedy, have lately given me occasion for posing and solving a little problem.""  As Freud describes, in The Merchant of Venice, the young Venetian suitor Bassanio must choose correctly between three metal caskets if he is to win the hand of the attractive and intelligent heiress Portia; in King Lear, the aged monarch demands a testimony of filial devotion from each of his three daughters before deciding how to apportion his kingdom amongst them.  The curious congruence of these scenes caught the pioneering psychologist's imagination.  However, if Freud's introductory remarks give the impression that he traffics in trivialities, we are sorely disabused: in solving his ""little problem,"" the Viennese psychoanalyst pursues profundities so doggedly his initial disclaimer seems disingenuous.
Freud conjectures that the well-known scenes represent variations on a more common literary motif in which a maiden chooses between three suitors.  Making some unexpected substitutions, as if by sleight of hand, he then unveils the hidden parallels: a male suitor now chooses among three caskets symbolizing three marriageable women, and these characters find rough counterparts in the elderly king with three daughters.  In both plays, the last and least assuming of the three choices constitutes the correct one: it is the lead casket, not those of gold or silver, that delivers the prize of Portia, and, unlike the older sisters' effusive flattery, Cordelia's more measured declaration conveys authentic love for her father.  Freud then casts his net of comparison more broadly, discerning permutations of the pattern in other tales.  For example, Cinderella, the youngest and humblest but most beautiful of three sisters, becomes the prince's bride.  Finally, Freud identifies the three Fates as the primordial mythic feminine constellation, the third of whom represents death itself.  These associations, he insists, suffice to prove this malleable motif's universality.
But how to uncover the deeper meaning of this theme with a thousand variations?  From a Freudian perspective, a literary scene is like a dream, a production of the human psyche arising from the unconscious.  As with dreams, the true meaning is to be found beneath the surface (in what Freud calls the latent content in contrast to the manifest content).  Accordingly, the most effective tools for parsing the pair of Shakespearean scenes are the psychoanalytic methods of dream interpretation.
But can dream theory explain the evolution from a mythology of fate to tales of free choice?  How are the likes of Portia, Cordelia, or Cinderella potential surrogates for a death deity?  After all, given a choice of three women, who would wittingly pick the agent of death?  Freud posits that in producing literary motifs, as in the creation of dreams, the psyche imposes its censorship to disguise elements it deems unacceptable, often inverting them: hence, fate becomes freedom, and the third woman is no longer a goddess of death but of life-affirming love.  The scenes thus compensate for the reality of death; they express a wish to escape mortality by opting for love's vitality.
In general, Freud believed that great literary works not only lend themselves to psychoanalytical interpretation but, in illustrating the laws of psychical functioning so well, also offer a resource for understanding psychology.  In particular, as Norman Holland notes, the Viennese psychoanalyst looked to the British bard to confirm and refine his theories: ""Freud's method was to take a pattern of mental life…and hold it up, as it were, against the play to discover a congruous pattern.""  Thus, Shakespearean themes contributed to the development of psychoanalytical methods, and Freud's ""little problem"" carried big implications.",Critical Analysis & Reasoning Skills,4,Humanities
25,Passage Title: Voting Paradox,"Let us consider a widely accepted principle, what is called the ""Principle of Voter Rationality.""  It states that rationally motivated members of a democratic society ought to vote, as it is in their own best interest to do so.  The idea behind this principle is remarkably simple, of course: rationally motivated people ought to do whatever is in their own best interest.  Democratic societies provide citizens with the opportunity to exercise their will through the process of elections.  Rationally motivated citizens ought to vote, then, because electing candidates who will act on their behalf is in their own best interest.  If true, the principle elegantly justifies the scaffolding that upholds democracy itself: citizen engagement with the democratic process.
As it turns out, there are excellent reasons to reject this principle.  Notice that the probability that any single vote will have an impact on the outcome of an election decreases as the number of voters increases.  In nationwide elections in democratic societies, the probability that any one vote will change the outcome of an election is infinitesimally small; so small, in fact, that a modest estimate for presidential elections in the US puts this probability at roughly one in one hundred million.  For all practical purposes, it is true to say that any single person's decision to vote will not affect the outcome of an election.
Citizens who are rationally motivated in the relevant sense, when deciding whether to vote in an election, must weigh the potential benefits of voting against its known costs.  And there are many such costs: the loss of one's personal time, frustration caused by long lines at the polling station, the energy needed to sufficiently educate oneself about candidates' policies, and so on.  Because the probability that one's vote will have any significant impact on the outcome of the election is so minuscule, however, citizens can be practically certain that the costs of voting will outweigh its potential benefits.  Therefore, contra the Principle of Voter Rationality, citizens must conclude that it is not in their best interest to participate in the democratic process…
Moreover, if we are willing to assume, in the way the principle does, that democratic citizens are rationally motivated, then, given a sufficiently large population of citizens who deliberate about whether to engage in voting behavior, the democratic process should break down.  For each citizen should find that he or she ought to abstain from participating in this process due to the aforementioned reasons…
In truth, the democratic process is fully intact.  The problem with the Principle of Voter Rationality is that it assumes, falsely, that voting behavior is ""rationally motivated"" when and only when it is done for the purpose of influencing the outcome of an election.  But other sources of justification are available.  As I see it, it is a commitment to the principles of democracy itself that should compel rational citizens, independently of whether it furthers their personal interests, to participate in the democratic process.  For with such a commitment comes a civic duty to prevent the breakdown of one's own democracy and, with this, comes the justification for voting behavior.
But what can be said about the nature of ""civic duties,"" and why should we suppose that there are any such duties in the first place?  For my part, I believe that a democratic government's policies and its citizens' civic duty to vote derive from the more general obligation upon man to help his fellow citizen, to increase the welfare of others with no expectation of a reward, a duty for altruism…",Critical Analysis & Reasoning Skills,6,Social Sciences
26,Passage Title: The Evolution of Modern Dance,"In the 1890s, the world of American dance underwent a profound transformation when a group of pioneering dancers dared to challenge the rigid 350-year-old protocols of classical ballet.  The likes of Isadora Duncan, Loie Fuller, and Ruth St. Denis rebelled against ballet tradition while simultaneously rejecting the alternative world of vaudeville.  Duncan, in particular, was a proponent of what was deemed ""uninhibited freedom of movement.""  Appearing on stage in a loose tunic with bare feet and her hair down, she performed natural, flowing movements she claimed ""emanated from her solar plexus.""  For her part, Fuller combined billowing costumes, dramatic lighting, and improvised movements in attempting to imitate natural phenomena, as exemplified by her ""serpentine"" dance.  Meanwhile, St. Denis and her partner Ted Shawn incorporated elements of Native American and East Asian ritualistic dance into their work.  These artists managed to break through the prejudice of tradition, clearing the way for the advent of modern dance in the 20th century.
The second generation of artists, which included Doris Humphrey and Lester Horton, continued to loosen dance from the restraints of classical ballet.  Many were also determined to raise the audience's consciousness about contemporary social concerns during a time that witnessed the aftermath of World War I, increased industrialization, the Great Depression, and the rise of fascism in Europe.  For a while, the modern dance movement became more than an assault on classical ballet; by dramatizing events on stage to inspire a collective emotional response, dance became a vehicle designed to spur sociopolitical change.
Martha Graham, a student of St. Denis and Shawn, went on to develop a radical new system of movement.  The ""Graham technique"" originated from her cardinal belief that movement expresses the emotions that people attempt to conceal and words are inadequate to express.  The new technique was organic; based on the natural rhythm of breathing, it exaggerated the muscular contraction and release, as well as the consequent flexion of the spine, required to breathe.  The visual effects were startling, and the iconoclastic Graham went so far as to assert that the technique ""represented the human being's inner conflicts.""  Furthermore, the movement of contraction became a regular feature of modern dance, which, as some sources stress, was ""used, altered, and redefined by many subsequent choreographers.""
As modern dance evolved, it became the antithesis of ballet, which followed strict patterns of movement and aimed to be light and graceful.  By contrast, the new dance style was often angular, jagged, and stark; the choreography featured nonlinear movements and irregular contours.  Instead of working against gravity as in ballet's sequences of vertical leaps, the contemporary style made gravity the dancers' ally in initiating movements from the lower abdomen with their bare feet rooted to the floor.  Whereas ballet emphasized the body's limbs, the new dance mode accentuated the torso.  While ballet was made to appear effortless, Graham's main goal as a choreographer was to display the physical exertion of articulating emotion through the body's movement.
Graham also strove to make the works she created and choreographed holistic.  She collaborated with sculptor Isamu Noguchi on stage sets, consulted with the designer Halston on costumes, and partnered with composers such as Louis Horst and Samuel Barber.  Aaron Copland wrote the score to the highly acclaimed modern ""ballet"" Appalachian Spring, which Graham choreographed and in which she performed the lead female role in 1944.  Building on the foundation of her precursors, Graham—who continued to dance until age 76 and to choreograph until shortly before her death at 96—made modern dance, in one scholar's estimation, ""an established art form that was no longer merely regarded as an avant-garde aberration.""",Critical Analysis & Reasoning Skills,4,Social Sciences
27,Passage Title: Candide and Zadig,"Voltaire's most famous work is undoubtedly Candide, which satirizes both 18th-century Europe and the philosophy of Gottfried Wilhelm Leibniz.  In particular, the character of Dr. Pangloss (who teaches ""metaphysico-theologo-cosmolonigology"") parodies Leibniz's view that this is the best of all possible worlds.  While Pangloss offers many supposed proofs that all events ""must necessarily be for the best end,"" the novel's characters endure a constant succession of horrible and comically absurd misfortunes.  Voltaire's clear message is that life—far from being ordered to produce the greatest good—is full of unnecessary suffering.
Given this understanding, those who see Voltaire through the lens of Candide would likely be shocked to read another of his works, Zadig.  The stories are similar in that both title characters experience a long series of undeserved evils.  However, whereas Candide moves from believing in Pangloss' worldview to eventually rejecting it, Zadig's lesson seems to be the exact opposite.  Convinced of the unfair cruelty of life, Zadig is visited by the angel Jesrad, who justifies many of the apparent tragedies Zadig has witnessed.  The man whose house burned down found a hidden treasure beneath it; the youth who drowned would have grown up to be a murderer; the impoverished fisherman would regain even more than he had lost.  Thus, Jesrad explains, ""Men…judge of all without knowing anything,"" but everything that occurs ""ought to be, in its proper time and place, according to the immutable decrees of him who comprehends all.""  Following this divine enlightenment, Zadig's trials likewise end happily.  The angel directs him to return home, where he is crowned king, bringing prosperity to the entire country.
Candide was written long after Zadig, near the end of a career in which Voltaire faced frequent difficulties of his own.  The satirizing and free-thinking that made his novels so popular with readers were not always appreciated by other aristocrats or monarchs, and Voltaire found it necessary to move from city to city throughout Europe.  Although the writer's hardships never approached the severity of those visited upon his characters, it is easy to imagine them influencing both his outlook and his stories.  A fairly straightforward conclusion would be that Zadig reflects a younger and more optimistic Voltaire, while Candide expresses the more cynical sentiments of a man grown frustrated at the world's ignorance and persecution.
However, there is another way to view the two works.  On several occasions, Zadig laments that he has been punished for doing what is right: ""Knowledge, virtue, and courage, have hitherto served only to make me miserable.""  While he never wavers from righteousness, he does question why evils befall those who do not deserve them.  Jesrad explains: ""The wicked…are always unhappy.  They serve to prove and try the small number of the just that are scattered throughout the earth.""  Thus, the angel implies that human cruelty is to blame for most of the unhappiness in the world.
That assessment is fitting not only in Zadig's case but also in that of Candide and his companions, who are repeatedly the victims of other people's greed, selfishness, and violence.  As James the Anabaptist, one of the ""small number of the just"" in Candide, observes: ""Mankind…must have somewhat corrupted their nature; for they were not born wolves, and yet they have become wolves.""  Shortly thereafter, James is left to drown by a man he himself had just saved from drowning.  Both tales, then, are best understood as a commentary not on the callousness of providence, but on the heartlessness of humanity.  Life is replete with pointless suffering, even if some misfortune could potentially serve a greater purpose.  But in Voltaire's estimation, it is people's wickedness that truly fills the world with misery.",Critical Analysis & Reasoning Skills,6,Humanities
28,Passage Title: For Whom the Bell Toils,"In nineteenth-century America, most people dismissed the notion that someone might assassinate the president.  The presumption was based not only on ethics but practicality: a president's term is inherently limited, and an unpopular one could be voted out of office.  Therefore, it was reasoned, there would be no need to consider removal through violence.  This belief persisted even after the shocking murder of Abraham Lincoln in 1865, which was viewed as an aberration.  Thus it was that on July 2, 1881, Charles Guiteau could simply walk up to President James A. Garfield and shoot him in broad daylight.  As Richard Menke portrays events, ""Guiteau was in fact a madman who had come to identify with a disgruntled wing of the Republican Party after his deranged fantasies of winning a post from the new administration had come to nothing.""  Believing that God had told him to kill the president, Guiteau thought this act would garner fame for his religious ideas and thereby help to usher in the Apocalypse.
In an interesting parallel, Garfield had felt a sense of divine purpose for his own life after surviving a near-drowning as a young man.  Unlike Guiteau's fanatical ravings, however, Garfield's vision worked to the betterment of himself and the world.  Candice Millard describes his ascent from extreme poverty to incredible excellence in college, where ""by his second year…they made him a professor of literature, mathematics, and ancient languages.""  Garfield would go on to join the Union Army, where he attained the rank of major general and argued that black soldiers should receive the same pay as their white compatriots.  While serving in the war he was nominated for the House of Representatives but accepted the seat only after President Lincoln declared that the country had more need of him as a congressman than as a general.  The reluctant politician would later himself become president under similar circumstances, after multiple factions of a deadlocked Republican convention unexpectedly nominated him instead of their original candidates in 1880.  An honest man who opposed corruption within the party, Garfield strove both to heal the fractures of the Civil War and to uphold the aims for which it was fought, until ""the equal sunlight of liberty shall shine upon every man, black or white, in the Union.""
Although Guiteau's bullet would ultimately dim this light for Garfield, the president actually survived the initial attack and for a time appeared headed for recovery.  Tragically, however, the hubris shown by his main physician, Dr. Willard Bliss, would lead instead to weeks of prolonged suffering.  None of the doctors who examined Garfield were able to locate the bullet, and its lingering presence—along with the unwashed hands of the doctors who probed for it—led to an infection.  As the president's condition worsened, inventor Alexander Graham Bell attempted to adapt his patented telephone technology to locate foreign metal in the human body.  Inspired by speculation that the bullet's electromagnetic properties might be detectable, Bell used his newly developed ""Induction Balance"" device to listen for the sounds of electrical interference he hoped would isolate the site of the bullet.
Unfortunately, Bell's searches were unsuccessful.  Like Garfield's doctors, he had been looking for the bullet in the wrong area.  Menke asserts that Bell's efforts ""would probably have fallen short"" regardless.  However, other historians suggest that Dr. Bliss, unwilling to consider challenges to his original assessment, prevented Bell from more thoroughly searching the president's body.  Certainly, Bliss ignored the advice and protestations of other physicians, even as Garfield continued to decline.  With death imminent, Garfield asked to be taken to his seaside cottage, where he died on the 19th of September.",Critical Analysis & Reasoning Skills,4,Social Sciences
29,Passage Title: Pascal's Wager,"In the 17th century, Blaise Pascal provided a purely practical argument for theism with his famous ""wager.""  The most basic form of the argument can be understood as follows.  Suppose one believes in God.  This choice includes the loss of certain earthly goods, for instance, time and resources devoted to spiritual matters instead of oneself.  If God does not exist, these goods are lost unnecessarily, but if God does exist, one gains eternal happiness in the afterlife.  On the other hand, by not choosing to believe in God, one gains those earthly goods.  If God does not exist, one loses nothing, but if God does exist, one loses eternal happiness.  Pascal's Wager, then, is essentially a risk/reward analysis.  Believing in God involves a finite loss but may result in an infinite gain, whereas not believing in God risks an infinite loss for the sake of a finite gain.  Therefore, Pascal argues, choosing to believe in God is the only rational decision to make in terms of one's own self-interest.
This argument elicits several objections, the most immediate of which relate to the nature of belief.  First, there is clearly a difference between truly believing in God versus choosing to believe for the sake of a hoped-for reward.  Hence, many critics have viewed the Wager as essentially cynical—not to mention the fact that if God does exist, he would presumably not be fooled by ""belief"" deriving from such mercenary motivations.  Even setting such concerns aside, however, the more important problem is that, put simply, belief doesn't work that way.  One cannot just will oneself to believe a proposition apart from some kind of evidence.  Rather, we accept or reject claims according to whether they match our experience of the world.  Sometimes this process is overt, drawing a conclusion based on an explicit chain of reasoning.  More often it is basically automatic or subconscious, arising from what is immediately apparent to perception and memory.  Thus, we might imagine someone who wants to believe in God for any number of reasons.  She may be concerned about eternal consequences, or view her religious acquaintances as morally admirable, or perhaps long for the kind of community shared by some persons of faith.  Regardless of the reasons for her wish to believe, belief is not a matter of choice; she cannot make herself believe in God or anything else.  Hence, a prescription to believe would seem to be pointless, even if one accepts that believing would represent the most rational, self-interested decision.
However, as compelling as those objections might appear, it is easy to overlook the fact that Pascal was well aware of them, and actually raised such worries himself.  Notably, he does not treat choosing to believe in God as a discrete event, but as a process that unfolds throughout one's life.  Thus, Pascal offers advice for those who struggle with their inability to believe: ""Learn of those who have been bound like you…who know the way which you would follow, and who are cured of an ill of which you would be cured.  Follow the way by which they began; by acting as if they believed, taking the holy water, having mass said, etc.  Even this will naturally make you believe....""  In this way, Pascal implies that by practicing religion and engaging in its rituals one may eventually grow to develop real faith, a phenomenon that he suggests is valuable in its own right because it gives one's life meaning and purpose.  As such, Pascal's Wager need not be seen as either cynical or impossible, but as an attempt to open a genuine path to theism.",Critical Analysis & Reasoning Skills,4,Humanities
30,Passage Title: Egg Donation,"The first case of successful egg donation, resulting in the birth of a human child, occurred in 1981.  Since then, the number of egg donors has drastically increased to generate a multibillion-dollar industry that is known as the oocyte market.  As the clientele of fertility clinics—primarily women seeking to conceive through the aid of medical intervention and reproductive technology—continues to grow, so does the immense global demand for egg donors.  This has left today's market with considerably fewer donors than needed to supply clinics with high-quality eggs.
In the current market, fertility clinics offer as much as fifteen thousand dollars to women who are willing to donate their eggs.  Perhaps unsurprisingly, then, numerous sources now report that a disproportionate number of female college students are donating their eggs.  It seems that recent hikes in tuition have driven many women to finance their education using compensation from the oocyte industry.
This raises certain moral or ethical concerns, however.  Some bioethicists are particularly worried about the current lack of government oversight in the oocyte market.  Without regulations governing the commercialization of the oocyte industry, for instance, women in need of short-term financial gain become especially vulnerable to exploitative or coercive business practices.  Notice that, at present, there is very little research on the long-term risks and effects of egg donation on women's health.  In today's unregulated marketplace, fertility clinics are not required to warn would-be donors about the potential health risks or the inadequate body of knowledge we currently have about these risks.  In the absence of such information, it is unclear that ""informed consent"" is part of the donation process.  And for impoverished women—because financial hardship leaves them with so few options—it is not even apparent that consent is part of the process at all.
Because of these issues, bioethicists have called for the establishment of a global register of egg donors and longitudinal follow-ups that aim to identify potential negative side effects.  But even if this were implemented, other ethical concerns would loom heavy.  For instance, there is the question of why, or on what grounds, fertility clinics pay donors thousands of dollars for eggs in the first place.  Donors of other sorts are not compensated for the goods they donate, so why should egg donors be so fortunate?  It is relevant to note that, by definition, an egg is a nonrecyclable sex cell; women are born with a fixed number of these and this number drastically decreases with age.  Critics claim that this aspect of the industry is revealing: In compensating donors, they say, the egg procurement industry transforms women's bodies into an economic resource, mere organisms from which a supply of biological matter must be ""harvested"" for the ultimate gain of others.
In response to this criticism, advocates for the oocyte market claim that, unlike donations of other sorts, the egg donation process requires a medically invasive and time-consuming procedure.  Their position is that the thousands of dollars given to donors merely serves as compensation for the valuable time (not the eggs) lost during the donation process.
If this claim helps to alleviate the concern that the oocyte industry commodifies women's bodies, there will be room for a collaborative effort toward long-term progress.  The proponents of this industry, as well as its opponents, can work together to ensure that government regulations provide safeguards against issues of consent and exploitation.  This would certainly be desirable since, as it seems, the oocyte industry is here to stay.",Critical Analysis & Reasoning Skills,5,Humanities
31,Passage Title: Dead Sea Scrolls,"Since the 1950s, scholars have been engaged in reconstructing a multifarious collection of ancient sacred texts, now known as the Dead Sea Scrolls, from a welter of as many as 100,000 fragments of disparate size and shape.  With some fragments no larger than a postage stamp and covered with barely decipherable writing, the formidable paleographic project presents a jigsaw puzzle of epic proportions and frustrating intricacy.  Yet, despite the fragmentary condition of the scrolls, their discovery has turned out to be the most important religious archaeological windfall of the 20th century.
The scrolls were found in large pottery jars hidden inside the caves that dot the northwestern shore of the Dead Sea, at a site called Khirbet Qumran, 13 miles east of the city of Jerusalem, by Bedouin goatherds in late 1946.  In the desert near Qumran, members of a Jewish sect known as the Essenes had established a small community during the latter half of the Second Temple period that began in the 6th century BCE and lasted into the first century CE.  The Essenes are noted in the works of the Roman statesman and historian Pliny the Elder, the Hellenistic Jewish philosopher Philo, and the Jewish historian Josephus.  In the first century, Pliny described the Qumran community as a monastic brotherhood that followed an ascetic lifestyle of work, prayer, and the study of sacred law and esoteric doctrine.  The Essenes differed from the mainstream Jewish community in objecting to animal sacrifices and subscribing to the heretical dictum that fate is ""the mistress of all.""  The group had migrated to the desert, eschewing urban areas, as Philo reports, ""because of the ungodliness customary among the city-dwellers.""
The scrolls date from approximately 250 BCE to 68 CE.  Scholars speculate that they represent either the library of the Essene community or the collected works of various local Jewish groups.  It is likely that the Essenes stashed the scrolls in jars and placed these inside the caves when they fled the imminent destruction of Jerusalem and the Second Temple (Herod's Temple) by the Romans in 70 CE.  The more than 900 identified manuscripts include parts of every book in the Old Testament canon except the Book of Esther, some with as many as 39 copies.  Only one scroll, the Great Isaiah Scroll, offers a virtually intact book: its 24 feet of parchment features all 66 chapters of the Book of Isaiah.  Other scrolls contain noncanonical texts, including apocrypha and pseudepigrapha—late pseudonymous writings ascribed to various biblical prophets and patriarchs.  Before the scrolls' fortuitous discovery, some of these works were known only through translations, or even translations of translations.  A few scrolls contain extra-biblical texts, chiefly sectarian manuals cataloguing the rules of the Essene community.
Over 75% of the parchment and papyrus scrolls are written in Hebrew; most of the others are in Aramaic (a related language that had become the standard linguistic currency in the Near East), while a smaller number are inscribed in ancient Greek.  The writing on the scrolls moves from right to left with only the occasional paragraph break for punctuation, which makes deciphering the manuscripts almost as daunting a task as piecing together their fragments.
The manuscripts demonstrate remarkable consistency with the Masoretic or medieval Hebrew text used for many English translations of the Bible, notably the King James Version.  Thus, the scrolls, which predate the Masoretic text by about 1,000 years, largely substantiate its reliability and authoritative status.  They also offer a window onto a critical period in Judeo-Christian culture; in particular, they confirm the religious diversity of the time and the similarity of an eschatological, messianic Jewish sect of the first century to early Christian communities.",Critical Analysis & Reasoning Skills,6,Humanities
32,Passage Title: The Author Vanishes,"On December 3, 1926, the 36-year-old author of half a dozen popular crime novels and creator of eccentric Belgian sleuth Hercule Poirot exited her suburban London home and drove off into the cold dark night.  What would transpire was a real-life mystery not beyond the imaginative pale of this well-known writer named Agatha Christie—an author who, Laura Thompson notes, presents a carefully constructed world of ""strychnine and crumpets"" where murder and mayhem are served up with ""post-Wildean wit.""  Into this made-up world, Agatha would later inscribe herself as a recurring character, the quirky crime fiction writer Ariadne Oliver.  Moreover, in her most innovative detective novel to date, The Murder of Roger Ackroyd, Christie had conjured a narrator so unreliable he elides the details of the incident most crucial to identifying the murderer, only to later reveal himself as the perpetrator.  Similarly, the events about to unfold would possess all the makings of a classic Christie whodunnit.  They form a conspicuous lacuna in the writer's autobiography, rendering her the unreliable narrator of her own life story.
In the early hours of December 4, Christie's abandoned Morris Cowley—its front wheels suspended over the edge of a chalk pit, headlights still blazing, and a fur coat, rummaged valise, and expired driver's license in the backseat—was discovered in Newlands Corner near a mist-shrouded lake called Silent Pool, some fifteen miles from her home.  By afternoon, multiple search parties were underway for the missing author; by the following day, these had ballooned to include 15,000 volunteers, a sizeable canine patrol, and, for the first time in British history, airplanes.  Ponds were dredged, downs were combed, but the woman who would be dubbed the ""Queen of Crime"" had vanished, leaving behind only taunting clues.  Mounting frustration prompted the constable in charge, William Kenward, to remark, ""I have handled many important cases in my career, but this is the most baffling mystery ever set me for solution.""
Police suspected foul play and could not rule out suicide.  Rumors swirled that distress at her mother's passing combined with her husband's infidelity had caused Agatha's life to unravel, leading her to seek revenge.  Other people were unconvinced, pointing out that notoriety from the author's disappearance would likely boost sales of her new book.  Meanwhile, the urgency of the situation led occultist and Sherlock Holmes creator Sir Arthur Conan Doyle to consult a medium, who assured that the missing mystery writer was very much alive, albeit in a state ""half-dazed, half-purposeful.""
Days later, Christie was spotted at the Harrogate Hydro, an exclusive spa hotel located hours north of London.  Registering under the surname of her husband's mistress, she availed herself of the amenities and fraternized with guests.  Purportedly, the celebrated crime writer also perused the newspapers without the slightest recognition of herself on the front page—a scenario not altogether improbable if she was suffering from a dissociative fugue state or stress-induced amnesia.  Even more intriguingly, in Christie's short story ""The Case of the Missing Lady,"" the protagonist turns up at a health spa after being thought the victim of a plot.
Although temporary amnesia remained the official explanation for the author's mysterious eleven-day disappearance, her autobiography, published posthumously in 1977, omits the notorious episode entirely.  Christie herself discussed the matter only once in a 1928 interview, maintaining the received rationale.  Thus, an alleged gap in memory became a gap in biography.  Later insights suggest that the detective fiction writer—like her character Jane Finn, who wiles a marriage proposal from a suitor after feigning amnesia—masterminded the whole escapade to recapture her husband's attention.  Hence, the disappearing act of Agatha Christie may have been a less-than-mysterious case of life imitating art.",Critical Analysis & Reasoning Skills,4,Humanities
33,Passage Title: Humanistic Frameworks,"As with rival scientific theories, acceptance of a humanistic framework can be gained by endeavouring to encompass and surpass the insights of competing schools of thought.  Sinologists such as DeBary and Bloom see the Neo-Confucian revival in Song dynasty China as employing such a strategy in relation to Daoism: ""In effect, [Neo-Confucianist Zhou Dunyi] was co-opting Daoist terminology to show that the Confucian worldview was actually more inclusive than the Daoist.""  The result, strangely enough, was a cosmology with resemblances to both Platonic rationalism and Augustinian Christianity.
In Neo-Confucian doctrine, the ""Supreme Ultimate"" comprises principle, conceived of as the origin and essence of existence, and material force, which permeates corporeal things.  According to the scholar Zhu Xi: ""Principle refers to the Way, which is above the realm of corporeality and is the source from which all things are produced.""  Everything in the universe depends upon the Supreme Ultimate, which Zhu Xi further identifies as ""the principle of the highest good.""  Naturally, such a description invites comparison to Plato's Theory of Forms; in particular, the idea that everything in the universe is but a paler or brighter imitation of the Form of the Good, from which all existence is derived.
To be certain, the claim that ""each and every thing has in it the Supreme Ultimate"" is not an exact analogue to the Platonic notion that all things possess a degree of reality commensurate with how closely they resemble the Good.  The Supreme Ultimate's inherence in material objects is distributive, not hierarchical.  Moreover, the Forms exist independently of the material entities which partake in their being, whereas in Neo-Confucian metaphysics, principle is inseparable from material force: ""Without material force, principle would have nothing to adhere to.""  Plato would reject such a claim; thus the seeds of principle remain decidedly more ""grounded"" than the rarefied abstraction of the Forms.  Nevertheless, the similarities between the two systems of thought are provocative.
In like fashion, one may analyze Neo-Confucian and Augustinian ideologies regarding the nature of evil.  In Book VII of his Confessions, Augustine writes: ""When I asked myself what wickedness was, I saw that it was not a substance, but perversion of the will when it turns away from you, O God, who are the supreme substance.""  Congruently, Neo-Confucianism situates human beings within a natural order constituting an ""all-pervading perfection.…  When humans act out of accord with it, there is evil.""  Zhu Xi further states: ""It is not true that there is originally an evil existing out there, waiting for the appearance of good to oppose it.  We fall into evil only when our actions are not in accord with the original nature.""  The similarity is even more intriguing given that evil as an independent substance in opposition to good is a primary tenet of the Manichaeism that Augustine came to so vehemently oppose.  Of course, the views do exhibit significant disparities: the Supreme Ultimate of Neo-Confucianism is not identified as a deity, and, requiring material force in which to inhere, it is still less the omnipotent, self-sufficient God of Augustine's faith.  Again, however, the parallels between the two conceptions are notable.
Clearly, neither Plato nor Augustine had contact with Zhu Xi or other Neo-Confucianists.  Hundreds of years and thousands of miles separate these thinkers, making all the more striking the similarities between Platonic and Neo-Confucian metaphysics, and between Augustinian and Neo-Confucian views of good and evil.  Perhaps the world will one day produce a Grand Unified Theory of Humanism which encompasses all major strands of humanistic thinking—or which, at the least, will ever more closely reflect the reality they mutually seek to describe.",Critical Analysis & Reasoning Skills,4,Social Sciences
34,Passage Title: Jackie in 500 Words,"Born in New York in 1929, Jacqueline Bouvier first came into the public eye as the wife of the 35th president of the United States, John F. Kennedy.  The president was assassinated in 1963, and by the end of what turned out to be a turbulent decade, Mrs. Kennedy had transformed herself into the enigmatic Jackie O., wife of Greek shipping magnate Aristotle Onassis.  Multifaceted and always elusive, the former first lady never ceased to fascinate; however, people had to be satisfied with only glimpses of this fashion icon, culture advocate, historic preservationist, polyglot, equestrienne, and book editor.  Indeed, upon her death in 1994, Jacqueline Kennedy Onassis was described as ""the most intriguing woman in the world.""  Often topping lists of the most admired individuals of the second half of the 20th century, this celebrated woman is likely someone many wish they had known.  Barring such a possibility, the best way to fully appreciate Jackie's exceptional nature might be to consider the people she wished she had known.
It is unsurprising that this woman who captured the public's imagination for decades distinguished herself from her peers early on.  Notably, in 1951, Ms. Bouvier entered a scholarship contest sponsored by Vogue and open to young women in their final undergraduate year, the annual Prix de Paris.  Among other assignments, applicants were asked to compose a 500-word essay, ""People I Wish I Had Known,"" spotlighting three individuals influential in art, literature, or culture.  The future first lady chose an iconoclastic trio from the Victorian era: the French symbolist poet Charles Baudelaire, the Irish wit Oscar Wilde, and the innovative Ballets Russes dance company founder Sergei Diaghilev.
In a brief composition, Jackie provided deep insights into this bohemian threesome of poet, aesthete, and impresario with whom she strongly identified.  She concluded that Baudelaire deployed ""venom and despair"" as ""weapons"" in his poetry.  She idolized Wilde for being able ""with the flash of an epigram to bring about what serious reformers had for years been trying to accomplish.""  Diaghilev she defined as an artist of a different sort, someone who ""possessed what is rarer than artistic genius in any one field—the sensitivity to take the best of each man and incorporate it into a masterpiece.""  As Jackie poignantly observed, such a work is ""all the more precious because it lives only in the minds of those who have seen it,"" dissipating soon after.  Furthermore, although these men espoused different disciplines, she discerned that ""a common theory runs through their work, a certain concept of the interrelation of the arts.""  Finally, foreshadowing her self-assumed role in the White House as the nation's unofficial minister of the arts, Jackie paid homage with her vision: ""If I could be a sort of Overall Art Director of the Twentieth Century, watching everything from a chair hanging in space, it is their theories of art that I would apply to my period.""
The contest committee judged Jackie's essay to have exhibited a profound appreciation for the arts combined with a truly outstanding level of intellectual maturity and originality of thought.  Similarly, biographer Donald Spoto deemed Jackie ""remarkably unorthodox,"" not unlike the men about whom she wrote in her unusual composition which he pronounced ""a masterpiece of perceptive improvisation.""  Thus, from a pool of 1,279 applicants representing 224 colleges, Jacqueline Bouvier was declared the winner.
Although Ms. Bouvier went on to decline the prestigious award, which would have involved living and working in Paris, she never gave up her dream of being the century's art director.  As first lady, she tirelessly promoted the arts and culture.  Today, the John F. Kennedy Center for the Performing Arts in Washington, DC, is a legacy of Jackie's vision.",Critical Analysis & Reasoning Skills,4,Humanities
35,Passage Title: Square One,"The word ""edutainment"" refers to educational entertainment, media designed to educate while entertaining the audience.  One of the more successful of such enterprises was Square One, a television show that aired in the 1980s and 1990s.  Focusing on mathematics, Square One aimed to instruct elementary school children in a way that captured their attention and made them eager to learn.  Such objectives are common to most attempts at teaching.  But well-made educational entertainment goes a step further than simply trying to make learning enjoyable; ideally, the program is of a quality such that audiences find it worthwhile to watch for its entertainment value alone.
Square One approached this goal in a number of ways, its iconic music videos being one of the most memorable.  Songs like ""Angle Dance"" and ""Less Than Zero"" described concepts such as acuteness or negative numbers, while the lyrics of ""Archimedes"" lauded the mathematical insights of that famous thinker.  Most of the songs contained humorous elements: the exhausted singers of ""That's Infinity"" tried vainly to reach the largest number, while ""Eight Percent of My Love"" saw a boy serenade his girlfriend about the small fraction of love he could give her (having already given away 92 percent to friends, the USA, his bicycle, and other things).  All of the songs boasted high production values and talented performers, subconsciously encouraging viewers to continue humming their lessons to themselves even after the show's closing credits.
The creativity of these songs featured popular culture aspects that were often more noticeable to parents than to the child viewers themselves.  For instance, adults could easily guess that ""Ghost of a Chance"" (a song about probability) was inspired by Michael Jackson's ""Thriller,"" and ""Juan Cougar"" was a reference to an actual singer, John Cougar Mellencamp.  Other parts of the show included similar references.  Details such as these functioned as a bonus, however, as the target audience found the material compelling despite not grasping all the references that elicited smiles from their elders.  Cabot and Marshmallow's ""What's in a Name?"" was comedic even without knowledge of the famous Abbott and Costello ""Who's on First?"" skit from which it was derived.  The math-based detective work of ""Mathnet"" was amusing and instructional even to those too young to have seen Dragnet.
In addition to the music, skits, cartoons, and parodies constituting many of the program's segments, game shows were another way in which Square One imparted instruction.  The prizes were unexciting (everyday apparel bearing the Square One logo), but that was to be expected for a production that aired on public broadcasting.  Of course, the prizes were also not the point.  Part of the draw of a commercial game show may be a fabulous package of expensive prizes and money, but the game shows of Square One focused on education.
In light of that distinction, one can imagine the most typical criticism of any educational entertainment program.  Education and entertainment are, at their cores, not aimed at the same ends.  Whether it is the worse crime to be insufficiently didactic or to be excessively didactic may depend on whom one asks; either way, one might worry that a work attempting to impart both amusement and instruction would ultimately fail in at least one of those objectives.  Yet Square One was that rare instance in which both goals were met.  Many children who disliked mathematics in school still enjoyed viewing a show with songs and skits about the subject, and for those who already enjoyed mathematics, all the better.  Teachers had good reason to be happy about the show's existence, and its lively presentation of educational concepts can surely be credited with the improved academic performance of many students.",Critical Analysis & Reasoning Skills,7,Humanities
36,Passage Title: Playing Politics,"A common charge leveled at elected officeholders is that they are ""playing politics.""  The implication of the claim is that its target has skewed priorities: he or she ought to care about addressing the needs of the citizenry through legislation and governing but is wrongly focused on politics instead.  Of course, one could be forgiven for pointing out that both accuser and accused in such cases are themselves typically politicians; this oddity notwithstanding, the accusation is ubiquitous among those who portray their rivals as failing to serve the interests of the people.
In the classical sense, however, there is no distinction between engaging in politics and addressing society's needs.  Aristotle categorized politics as the macrocosm for which ethics is the microcosm.  That is, ethics concerns the good of the individual, while politics concerns the good of society (literally, the polis or city-state).  The two disciplines differ mainly in scope; each has the good of humanity—singly or collectively—as its object.  Under this conception, focusing on politics is precisely what a politician ought to do.
So why has ""politics"" become a dirty word in today's—well—politics?  One way of conceptualizing the division between the word's roots and its modern perception consists in a means/ends distinction.  The paradoxical-sounding assertion that a politician is being too political begins to make sense if political power is viewed as an end in itself.  Politics is inherently agonistic: a struggle between competing ideas about the good of society and between competing aspirants to society's leadership.  In its ideal form, the latter contest is subordinate to the former.  In other words, the object of political power is to bring to fruition a particular conception of the good, and it is for the sake of that good—a public good—that power is sought and exercised.  But if the power itself is the goal—if one seeks authority for the sake of having authority—then the good of society becomes either forsaken entirely or, at best, an afterthought.
Such an end we might call the ""false microcosm,"" a corrupted analogue to ethics in the framework Aristotle describes.  In that situation, a politician achieves neither society's good nor her own, but simply the continuation of her time in office.  When extended to political parties, playing politics would entail striving to aggrandize one's own side or diminish the opposition without actually contributing to the resolution of an issue.  In this cynical form of politics, persuasion is attempted by comparison rather than substance, the vaunting of an empty façade in which the supposed qualification to lead is divorced from leadership.  A party that achieves nothing, or even stands for nothing, can still maintain power if the alternative party seems sufficiently loathsome—even if, ironically, the supposed loathsomeness comes from a perception that its members are ""playing politics.""
So how do we fix this problem, restoring the original sense of ""politics"" in the minds of both voters and politicians?  The answer is twofold.  First, politicians must commit themselves to acting in good faith and to upholding a kind of mutual aspiration.  For instance, when attendees at a 2008 John McCain rally suggested that Barack Obama was untrustworthy and not an American citizen, McCain famously defended his rival as ""a decent family man, citizen, that I just happen to have disagreements with on fundamental issues.""  Second, citizens must commit themselves to valuing nuanced positions and open-minded reflection from candidates, rather than the snappy soundbites and one-liners that typify so much of political debate and media coverage.  If each group strives to embody these aims, we can return to genuinely engaging in politics rather than playing politics.",Critical Analysis & Reasoning Skills,6,Social Sciences
37,Passage Title: Increasing Public Appreciation for Abstract Art,"For those of us whose careers are devoted to art, a common frustration is the low opinion that the public at large has of abstract works and styles.  The tendency for most people is to strongly prefer representational art, while mostly disdaining works that do not portray literal likenesses.  Naturally, we in the art world would like to change this attitude, to better share what we recognize as valuable artistic experiences.  Perhaps we even look down upon the public a little bit—smugly superior in the face of one more clichéd remark that ""a four-year-old could do that.""  I think it behooves us to re-evaluate our perspective.  If we are committed to the idea that art is enriching to ordinary lives, we must take more seriously the question of why the popular preference exists.  To begin with, we should make an admission: there is a sense in which—if not objectively, then at least concretely—representational art is superior to abstract art.
Although that claim may seem shocking, the reasons behind it need to be acknowledged.  One advantage of representational art that abstract art often lacks is definite conditions for success.  To a significant degree, with representational works it is clear when an artist has done well: if an object or person has been accurately portrayed, anyone can see that something skillful has been accomplished.  Moreover, human beings have a natural desire to depict the world around us.  With representational art, then, ordinary viewers will generally see some purpose to the work whether or not they discern any deeper meaning or symbolism that the artist may have intended.
With many abstract works, however, there are no broadly accessible grounds for judging their success or failure.  Nor is it always apparent what the purpose of such a work is.  To a curator, an untitled Joan Mitchell canvas may be a ""bold departure from traditional forms"" warranting serious contemplation.  But to museum visitors, it may be an indistinct hodgepodge warranting only a brief glance en route to paintings with recognizable subjects.  The problem is not necessarily a perception that abstract works lack meaning—the real issue is more subtle.
To see why, let us revisit the hypothetical four-year-old.  When a child produces art, we likely judge the production as beneficial to her: she has expressed herself, exercised creativity, and so on.  But there is little reason to think that anyone else (save perhaps her parents or teacher) should find her work important.  Thus, our frustrating cliché may reveal an important truth: the difficulty with abstract art is not that ordinary viewers can find no meaning in it.  Rather, there is no obvious reason why anyone other than the artist should care what that meaning is.
This, then, is the issue that we as art scholars must address if we want to better convey the value of abstract works.  To explore this topic fully will require a more extensive treatment, but I would like to offer a starting point for reflection.  A typical skepticism toward art analysis stems from the impression that it is characterized by trickery or puffery of some sort.  The average viewer sees little of note in a given work but is assured of its great significance by a member of the intellectual elite; if only the viewer understood art, he would perceive the work's many virtues….  We must dispel this misconception.  The task is to universalize: to make clear how an artist's vision is more than idiosyncratic, extending to the shared spaces of human experience.",Critical Analysis & Reasoning Skills,5,Humanities
38,Passage Title: The Scream,"Norwegian artist Edvard Munch's instantly identifiable painting Der Schrei der Natur or, more simply, The Scream, destabilizes the very concept of a work of art as discrete and unique.  Boasting two versions in pastel and two in oil as well as a lithograph—all generated between 1893 and 1910—Munch's masterpiece arguably constitutes not a single but a multiplicitous work.  Moreover, Munch originally conceived of the painting not as a freestanding piece but as part of a more ambitious project called the Frieze of Life.  The Frieze included 22 panels portraying life circumstances and moods, such as illness, love, and death; the panel depicting despair was the precursor for The Scream.
In a poem based on an 1892 diary entry, Munch relates the genesis of what would become his most celebrated work.  He was walking across a bridge with two friends in the vicinity of Ekeberg Hill, overlooking Oslo, when ""…the sun was setting—suddenly the sky turned blood-red—I paused, feeling exhausted, and leaned on the fence—there was blood and tongues of fire above the blue-black fjord and the city––my friends walked on and I stood there trembling with anxiety—and I sensed an infinite Scream passing through nature.""
The artistic consequence of Munch's terrifying epiphany is a black-shrouded figure, ill-defined and ghostlike, whose sickly pale face stares out at the viewer against the menacing background of a bloodshot sky.  Mouth wide open, eyes bulging, hands clasped to its ears in a pose more horrified than horrifying, the figure appears to tremble on a bridge over turbulent waters.
The Scream aptly captured both the self-indulgence and despair characterizing the turn of the twentieth century at the same time that it heralded the existential angst of the mid-century.  Influenced by luminaries such as the impressionist painter Monet and the more deeply expressive Van Gogh, Munch's free-form style, vivid colors, and striking juxtapositions mark this work as avant-garde.
Nevertheless, as art curator Martha Tedeschi asserts, The Scream is one of only a handful of paintings in the history of art able to ""communicate a specific meaning almost immediately to almost every viewer""; its primitive character transmits a raw emotionality and subjectivity.  Unsurprisingly, then, the painting has over-spilt the bounds of traditional art to become an icon of popular culture.  In the 1970s, Andy Warhol mass-produced the image on silk, the serial replications reflecting the painting's own multiplicity.  Wildly appropriated and often misused, the image—through which Munch purported to be doing nothing less than studying his own soul—has appeared on everything from keychains to dormitory posters and is in danger of becoming kitsch.
Interestingly enough, two different versions of The Scream have been the targets of unusual thefts; however, in both instances the paintings were recovered.  First, a version was stolen from the National Gallery in Oslo during the 1994 Olympics in Lillehammer.  In an almost comical heist, two men employed a ladder to easily access a second-story window and abscond with the painting, taking the time to leave a postcard that read, ""Thanks for the poor security.""  Then, in 2004, robbers entered Oslo's Munch Museum in broad daylight, intimidated museum guards with firearms, and used a wire cutter to extract another version of The Scream from the wall along with Munch's Madonna painting.
Munch was a remarkably prolific painter; when he died in 1944, he left a legacy of over a thousand paintings, several thousand drawings, and more than 15,000 prints.  Ironically, however, it is the ""single"" provocative image of The Scream that, as both masterpiece and pop icon, has eclipsed all this artist's other works.",Critical Analysis & Reasoning Skills,5,Humanities
39,"Passage Title: ""The Victorian Internet""","Lasting roughly from 1820 to 1914, the Victorian Era is often defined by its many distinctive sociological conditions, including industrialization, urbanization, railroad travel, imperialism, territorial expansionism, and the frictions sparked by Darwinism and democratic reform.  However, descriptors that may not come as readily to mind are ""the Information Age"" and ""the Age of Communication""; nor would we likely associate this era with something called the ""highway of thought,"" which emanated from the electric telegraph—an invention that, in retrospect, could be renamed ""the Victorian Internet.""  Yet the rise of the electric telegraph in the mid-1800s constituted the greatest revolution in communication since the invention of the printing press in the fifteenth century and until the launch of the World Wide Web at the end of the twentieth century.
Historically, messages could be conveyed only as fast as a person could travel from one location to another.  However, by the end of the eighteenth century, the Chappe brothers had constructed a rudimentary optical telegraph on a hilltop tower in France.  Using the large, jointed arms attached to the roof, operators could form various configurations to communicate a message to a similar tower farther away, which would then relay the message to a third tower, and so on, in a long-distance chain.  Nevertheless, even with telescopes, visibility severely hampered the efficacy of this semaphore system.  After countless attempts by scientists to use electricity to transmit messages, innovators in both Britain and America, including the painter and polymath Samuel F.B. Morse, worked at harnessing electromagnetic forces to send communications via cable.  But while in Britain the technology was initially reserved for railway signaling, in the U.S. it culminated in the transmission of a message along a 40-mile telegraphic wire from Washington, D.C., to Baltimore in 1844.  Using a binary code of short and long electrical impulses, or ""dots and dashes,"" Morse dispatched the words: ""What hath God wrought?""
Within 20 years, telegraph cables crisscrossed the continental U.S. and much of Europe.  The telegraph office became ubiquitous, and telegrams—often bouncing from one office to another like emails tossed from server to server—reached ever more remote destinations.  Following some spectacular failures, a durable cable was stretched across the floor of the Atlantic Ocean, enabling Europe and the U.S. to exchange messages within minutes.  In the 1870s, the British Empire connected London with outposts in India and Australia.  While individual telegram speed remained relatively constant, an endless deluge of information began to pour through the wires.  There was, as Tom Standage observes, ""an irreversible acceleration in the pace of business life,"" reflecting how ""telegraphy and commerce thrived in a virtuous circle.""  It was as though ""rapid long-distance communication had effectively obliterated time and space,"" begetting the phenomenon known as ""globalization.""
A new type of skilled worker, the telegrapher, was born.  He or she belonged to a vast, online community, whose semi-anonymous members shared a unique intermediary role as well as a language of dots and dashes—vaguely prefiguring the exchange of bits and bytes along modern computer networks.  Like today's online communities, these telegraphers fostered their own subculture: jokes and anecdotes flew over the wires; some operators invented systems to play games, and occasional romances blossomed.  Regrettably, hackers, scammers, and shady entrepreneurs also frequented the byways of this early internet.
Predictably, newer forms of the original telegraphic technology—first the telephone and, much later, the fax machine—eventually encroached.  The last telegram departed from India in 2013, but the twenty-first century has arguably seen a revival of this communication form in the text message.  As Standage claims, texting has not only ""reincarnate[d] a defunct nineteenth-century technology"" but reinforced ""the democratization of telecommunications"" inaugurated by the miraculous Victorian Internet.",Critical Analysis & Reasoning Skills,4,Social Sciences
40,"Passage Title: ""Golden Bough or Gilded Twig?""","Ironically, one of the most influential works of anthropology ever written was destined to have its most enduring impact outside that field.  An instantaneous bestseller when it appeared in 1890, Sir James Frazer's The Golden Bough: A Study in Magic and Religion was nevertheless considered controversial for its treatment of many religions as fertility cults centered on a cycle of death and rebirth.  While the book's overwhelming popularity assured its place in posterity, the changing face of science and the further emergence of anthropology as a discipline meant it would end up as reviled as it was revered.  At worst it was dismissed as a relic, an egregious example of ""armchair"" anthropology and an encyclopedic monstrosity; at best it was exalted as a dazzling display of erudition packaged in poetic prose.  Moreover, as a rare example of adventure tale, travelogue, and detective story rolled into one, The Golden Bough was enormously entertaining to read.
First published in two volumes, Frazer's vast compendium of myth and ritual had by 1915 filled twelve.  The work emphasized the striking similarities found in myths across cultures and time periods—in other words, a comparativist or universalist view that Frazer did not invent but employed lavishly, piling example upon example.  Frazer's text proved an intellectual and stylistic phenomenon with an ever-expanding influence on the cultural milieu.  Those pulled into this Frazerian vortex included pioneering psychologists Freud and Jung as well as many of the most creative writers of the early 20th century, such as James Joyce, Joseph Conrad, and T. S. Eliot.  It is perhaps no coincidence that the first one-volume abridgement of The Golden Bough was published in 1922—the same year in which Joyce's experimental novel Ulysses and Eliot's unorthodox poem The Waste Land appeared in print.
The year 1922 also saw the publication of such groundbreaking anthropological works as Bronislaw Malinowski's Argonauts of the Western Pacific and Alfred Radcliffe-Brown's The Andaman Islanders.  By adopting a more socially oriented and practical approach called ""functionalist,"" these works departed radically in method and style from Frazer's grand exercise in mythography and ethnography.  For example, Malinowski—who described himself as having been ""enslaved"" by Frazer's text as a youngster—began incorporating fieldwork into the discipline, focusing on the ways mythic practices contributed to the overall stability of a society.  Claude Lévi-Strauss went on to integrate Frazer's comparative method with the structural linguistics of Ferdinand de Saussure and Roman Jakobson to create structural anthropology, which organized the basic elements of myths into binary oppositions.
Later, Malinowski's student Edmund Leach synthesized his teacher's methods with those of Lévi-Strauss.  Twice removed from the Frazerian orbit, Leach gazed backward upon The Golden Bough as a target rather than a benchmark.  While acknowledging Frazer's legendary status as ""a colossus in the history of English scholarship,"" Leach nevertheless attacked him as a plagiarist, distortionist, and popularizer.  In a particularly scathing critique entitled ""Golden Bough or Gilded Twig?"" he condemned his predecessor's ""artistic license,"" alleging that he routinely embellished sources and twisted information to support preformulated theories.  According to Leach, Frazer relied shamelessly on secondhand material culled from missionaries and imperial officials rather than on his own interactions with native peoples.  Moreover, many of his comparisons of myths seemed based solely on superficial resemblances between just a few elements.  Lastly, Leach joined in decrying what critics termed Frazer's ""misapplication of Darwin's theory of biological evolution to human history and psychology"" that depicted cultures as progressing through predictable stages from belief in magic to belief in religion, and, finally, in science.
Such criticism notwithstanding, the popularity of The Golden Bough has remained largely undiminished and its impact on the culture of the early 20th century indisputable.",Critical Analysis & Reasoning Skills,4,Humanities
41,Passage Title: Ravens: Learning through Insight,"Research on animal intelligence has proliferated in recent decades, producing additional evidence of complex types of thought in non-human creatures.  Although primates have been the most frequent subjects of such study, observations of avians have at times proved even more striking.  In particular, some members of the Corvid family (such as ravens, crows, and jays) display a high capability to successfully confront novel challenges through innovative behavior.
One occasion of such behavior was documented by University of Vermont zoologist Bernd Heinrich in an experiment with Common Ravens.  Heinrich's central focus was whether the birds could discern the solution to a newly encountered problem of food acquisition without any training or prior familiarity with the problem's elements.  To address this question, he presented groups of ravens with pieces of meat suspended from a perch by a length of string.  The food was too far from either the perch or the ground to be reached from those vantage points.  Hence, accessing the food efficiently would require using the string itself, a situation not encountered in the wild and to which none of the birds had been previously exposed.
The first two ravens to approach the food found little success in retrieving it.  As Heinrich relates, both Bird 3 and Bird 27 attempted to jump and grab the meat from the air.  Although this process resulted in only tiny scraps of food being obtained, both birds initially ""tried no other method after the pattern was established on the first day.""  However, another raven fared far better.  After pecking and yanking on the string, Bird 4 ""appeared to abandon all attempts to get the meat, but [six hours] later again tried the same behavior.  This time, however, after one of these yanks of the string, the bird put one foot on this pulled-up string, reached down and pulled up another length of string, to step on it again after backing up another step along the perch, to repeat the process until reaching the meat.""  Thus, the raven ""performed a behavioral sequence of at least six steps…with no apparent trial-and-error learning for most if not all of these steps, chaining these steps together into a single unbroken sequence.""  Days later, Bird 5 also discovered how to access the meat in this way, completing ""the entire repetitive sequence correctly all at once.""  Eventually Bird 3 and Bird 30 learned as well, but only after weeks of further opportunities.
The most notable point of these observations is the ravens' apparent ability to solve a problem for which neither instinct nor experience could be expected to have prepared them.  Heinrich casts his experiment in terms of testing for insight, or ""the possibility of mental awareness—of seeing into a situation.""  Naturally, such awareness can only be inferred rather than observed directly, and researchers should be wary of the tendency to anthropomorphize animals when discussing their mental capabilities.  Nevertheless, such wariness should not prevent us from drawing at least probable or tentative conclusions.  Bird 4 especially seems almost to have mentally worked through the scenario in the intervening time between its originally unsuccessful and subsequent successful attempts to retrieve the food.  In the human world, we might speak of someone thinking about a problem for a while before trying another solution.  Even if we are reluctant to view Bird 4's behavior as a precise analogue, it seems clear that the raven displayed evidence of a noteworthy mental operation.  In further experiments, some other ravens also demonstrated the ability to pull up food on strings.  Although Heinrich notes that the birds exhibiting this behavior are ""possibly exceptional,"" they represent a fascinating example in our understanding of animal intelligence.",Critical Analysis & Reasoning Skills,4,Social Sciences
42,Passage Title: Doctor Scenario,"Evil can most accurately be defined as the direct antithesis of good.  However, when a human situation does not lend itself to clear designations of ultimate good and ultimate evil, or of right and wrong, then the best that one can hope for is to be able to make an informed decision about which course of action is ""more right"" and which is ""more wrong.""
Consider the hypothetical case of Dr. Jones, who, in discovering the antidote to a poison (Poison H) has so far concocted only a single vial.  Now suppose that six citizens have drunk from a well whose water has been contaminated with Poison H, but Citizen A has consumed an inordinate volume of the contaminated water.  Accordingly, the doctor has only enough of the antidote to save either Citizen A or the five other citizens.
It may seem clear that a doctor could not justifiably allocate the entirety of an antidote to preserving one citizen's life at the expense of losing five others.  However, suppose that Citizen A is the only patient currently present in the doctor's clinic, as the remaining patients are being transferred to Dr. Jones' facility by their respective physicians.  Citizen A is, in fact, Dr. Jones' longtime patient.  The doctor must therefore decide between two exigencies:  to save his own patient's life or to await the arrival of the five remaining citizens.
Would Dr. Jones be acting unethically in sacrificing Citizen A given the prospect of administering the antidote to the remaining citizens in time?  This dilemma asks the judicious individual to take into account additional factors that might encumber his or her ability to save the citizens who have not yet arrived.  One should consider, for instance, how long each patient can survive without the antidote.  Unfortunately, no one has a crystal ball to predict future events.  A decision must be made in conjunction with the information available at the time.
With this in mind, one may argue that the determination of good versus evil in making a decision lies in the doctor's motivations as he considers the consequences of his impending actions.  Imagine Dr. Jones' nurse exclaiming, ""But the doctor cannot make an objective comparison between the lives of other citizens and his own patient!""  Yet, perhaps the doctor would be selfishly inclined to save Citizen A, as Citizen A is the only one whose death would be considered a grievous personal loss.
But are such selfish human instincts then to be labeled as evil?  The same propensities labeled as selfish underlie characteristics classified as decent human virtues, such as compassion, empathy, and loyalty.  There are particular circumstances in which the doctor would indeed be remiss to the point of what could be argued as evil—as in cases where he opts to dismiss or distrust his individual conscience.  Consider the case in which he doubts his ability to arrive at a judicious conclusion and abdicates his responsibility for making this grave decision to the nurse.  Or consider the case in which he concludes that it is indeed more valiant to save the five citizens, yet fails to act on his decision out of personal anguish at losing Citizen A or to put an end to the ordeal.
In such circumstances, the decision between right and wrong, or the moral dilemma, escalates into what is more accurately a personal determination of good and evil.  In this way, the relative construct of right and wrong, as determined by one's best attempts at logic and sense, must be subordinated to a moral imperative that may be said to be absolute:  namely, that one must abide by an internal commitment to what one has reasoned to be the right decision and follow a corresponding course of action in the allotted time.",Critical Analysis & Reasoning Skills,6,Humanities
43,Passage Title: Meaning: Readers or Authors?,"Of late it has become popular among linguists and literary theorists to assert that a work's meaning depends upon the individual reader.  It is readers, we are told, not authors, who create meaning, by interacting with a text rather than simply receiving it.  Thus, a reader transcends the aims of the author, producing their own reading of the text.  Indeed, on this line of thinking, even to speak of ""the"" text is to commit a conceptual error; every text is in fact many texts, a plurality of interpretations that resist comparative evaluation.  This view is nonsense.  That many otherwise sensible scholars should be attracted to it can perhaps be readily explained, but we should first delineate why the theory goes so far astray....
The absurdity of the view can be demonstrated by a practical analogy.  Suppose Smith is conveying his ideas to Jones in conversation (the particular topic is of no consequence).  Afterward, we discover that the men differ in their accounts of what Smith had expressed.  At this point, Jones may decide that he misunderstood Smith, or perhaps that Smith was unclear.  A more complex supposition might be that Smith misused some key term, so his words did not fully match his intentions.  Any of these possibilities would reasonably describe why Jones and Smith possessed different opinions about what Smith had said.
What Jones may not justifiably conclude is that his own interpretation is what Smith really meant.  He may not, in effect, say: ""Yes, I admit that Smith honestly claims to have been saying something different, but I have formed my own equally correct understanding.""  Someone who made such an assertion would be suspected of making a joke; if he proved to be serious, we could only conclude that he was deeply confused or else being deliberately quarrelsome.  For in questions about what Smith meant, it is surely Smith whose answer must be accepted….  [T]his is not a matter of agreeing with a speaker; Jones might judge Smith's ideas to be wrong, unfounded, etc.  But whether Smith's ideas are right or wrong is a different matter from what those ideas are.  On that count, Smith must be the authority.
However, this observation is in no way changed if Smith's ideas are written rather than spoken—sent by letter, for instance.  Regardless of any interpretation Jones may produce, the letter's true meaning is whatever Smith intended to convey.  Likewise it is, then, with a book, poem, or whatsoever object of literature a scholar (or ordinary reader) encounters.  The writing down of ideas does not magically imbue them with malleability or render their content amorphous.  From the loftiest tomes of Shakespeare or Milton to the lowliest of yellowed paperbacks, authors produce works with a particular message in mind.  It is readers' task to discern that message, not to superimpose their own volitional perspectives.
To think otherwise is to undermine the foundation of literary scholarship.  For what is the purpose of such scholarship, if not to seek understanding of an author's creation?  One examines the text, taking note of style, historical context, allusions to other works, and other factors, in addition, of course, to the surface sense of the words themselves.  If such an enterprise is to be reasonable, it must presume the existence of standards for success: accuracy and inaccuracy, depth or shallowness of analysis, grounds for preferring one interpretation to another.  Different readers may come to different conclusions about a text, it is true.  But to excise authorial intent from the evaluation of those conclusions does a disservice both to individual works and to literary study as a discipline.",Critical Analysis & Reasoning Skills,4,Humanities
44,Passage Title: Whaling Conflict,"In 2010, the Japanese vessel Shonan Maru No. 2 crashed into Australia's Ady Gil, severing eleven feet of the latter craft and causing it to sink.  Neither of the captains involved was legally judged to have caused the collision intentionally.  However, each blamed the other, and the Ady Gil's captain, Peter Bethune, was arrested after he boarded the Shonan Maru No. 2 and attempted to force the Japanese to pay for the loss of the Australian ship.
Antipathy on both sides of this encounter actually extends much farther than the crash, which represents merely one incident in a decades-long maritime conflict.  Tensions between the two nations rise regularly for a period each year, as Japan's annual whale hunts are confronted by Australian fervor for conservation.  ""Save The Whales"" is a long-touted, even clichéd, rallying cry of environmentalism, but Australia possesses its own particular sense of guardianship for the Antarctic whales that swim up its coastlines.  As anthropologist Adrian Peace points out, descriptions of these animals are ""suffused with anthropomorphism,"" in which ""our whales"" are ascribed with characteristics such as shyness or affection, and somehow imbued with the knowledge that the people of Australia want to keep them safe.  In Peace's words, ""[I]dentification with the totemized whale has become part of how Australians imagine themselves as decent and enlightened citizens in the contemporary world.""  Consequently, Australian minds tend implicitly or explicitly to embrace the view that ""pro-whaling countries can lay no similar claim to being enlightened and humane.""  To engage in whaling is seen as treating a species as a source of financial gain rather than as creatures deserving protection, and ""since only the Japanese kill 'Australian Whales', they are the target of sustained demonization.""
Australia is not the only country that has objected to Japan's whaling practices.  As reported in The American Journal of International Law, the International Whaling Commission voted in 1982 to end all commercial whaling by 1986, leaving only a few types of whaling permissible.  After initially refusing, Japan agreed to stop its commercial whaling in 1987 but ""immediately announced that it would continue to take hundreds of minke whales each year for 'scientific purposes.'""  Considering Japan to be acting in bad faith, U.S. president Ronald Reagan responded by revoking that country's rights to fish in United States waters.  Similarly, in 2000 the U.S. secretary of commerce, Norman Y. Mineta, wrote to President Bill Clinton that ""Japan's research program has dubious scientific validity….  Many suspect that Japan's motivation…has less to do with validation of scientific hypotheses and more to do with paving the way for outright resumption of commercial whaling.""
Naturally, the Japanese have their own view of the situation, which frames the issue differently.  Peace describes the significant cultural importance attached to Japanese whaling, noting that the tradition of consuming whale meat dates back thousands of years.  Moreover, supplies of whale meat enabled Japan to endure the effects of famine in the early twentieth century, as well as food shortages in the waning years of the Second World War.  For many reasons, whaling is ""valorized and preserved"" among ""the exceptional elements of Japanese culture.""  Specifically, ""Eating whale meat represents an integral part of how the Japanese locate themselves in a world geared to the destruction of regional differences.  The anti-whaling rhetoric of countries like Australia and the United States is thus interpreted as a challenge to the right of the Japanese to live as they choose.""  Consequently, whereas Australian cultural identity has become entwined with opposition to whaling, in Japan hunting and eating whales constitute ""significant traditions which must be honoured.""  Given this stark societal difference, a compromise position is unlikely to be realized soon.",Critical Analysis & Reasoning Skills,4,Social Sciences
45,Passage Title: Literary Synthesis and Bernard,"To consult diverse interpretations is always a useful corrective against complacency in literary scholarship.  Reviewing alternative readings of a work helps remind us that questions regarding the work's meaning should seldom be viewed as settled but instead as opportunities for further inquiry.
Nevertheless, another pitfall must be avoided as well.  The fear of missing genuine interpretive possibilities may lead one to imagine more differences in readings than truly exist, obscuring a hidden homogeneity among seemingly divergent understandings.  It has been said that to truly understand a people one must synthesize its stories; we must also acknowledge that to truly understand a story one must synthesize its interpretations.
David McKee's deceptively rich Not Now, Bernard is an instructive example.  The plot initially concerns the futile attempts of a child to gain his parents' attention, to which, no matter the circumstance, the busy adults invariably reply: ""Not now, Bernard.""  The culmination of these encounters comes when Bernard informs them of a monster outside that wants to eat him, only to receive the same dismissive response.  Bernard walks into the yard and approaches the monster, whereupon ""The monster ate Bernard up, every bit.""
The reader's shock at this turn of events is only increased upon seeing the reaction of Bernard's parents, who are entirely unfazed.  Indeed, they are entirely unaware.  The monster enters the house, eats Bernard's dinner, breaks Bernard's toy, and even bites Bernard's father, all the while finding itself just as ignored as Bernard had been.  Finally facing the indignity of being tucked into bed, the monster complains: ""But I'm a monster,"" only to be told—naturally—""Not now, Bernard.""
Marina Warner views this delightfully savage story as informed by the same primal motivations which produced the carnivorous fiends of traditional fairy tales.  Referencing the psychoanalytic theories of Melanie Klein, Warner writes: ""the ogre who used to stalk children has now been internalized as the image of inner compulsions, especially greed and the ferocious survival instinct.""  In the parent's mind, ""it is the child who can be understood to be the potentially insatiable devourer, the possible monster of greed and gratification and excess.""  Likewise, Bernard's parents ""cannot tell the difference between a tiny tot and an angry beast because the two are all the same to them: that is how they see Bernard.""  Thus Warner casts McKee's tale as an incisive commentary on ""parents who are so automatic in their oblivion—and rejection—that they do not notice when their child has been eaten.""
Intriguingly, writer Sheila Hancock assumes that Bernard's parents are not oblivious.  On her interpretation no one has been eaten and replaced; rather, the monster is Bernard.  It is worth noting that of the two lines spoken by the monster, the first is simply ""ROAR,"" a bestial vocalization that makes little impression upon Bernard's mother (""Not now, Bernard"").  The human-sounding ""But I'm a monster"" comes later, after a series of events in which the presence of a monster in the house has been treated as unremarkable.  The apparent powerlessness of the monster to be perceived as anything but mundane suggests it could be merely the external manifestation of the child's frustrated psyche.
Even so, Hancock's view would not completely absolve Bernard's parents, who consistently ignore their son even if they do not literally allow him to be devoured.  Nor does the story lose its significance on Hancock's symbolic interpretation of events; rather, the latent psychological motivations identified by Warner would be just as operant.  Hence, whether the monster is literal or metaphorical, these seemingly divergent interpretations in fact exhibit a clear and striking coherence.",Critical Analysis & Reasoning Skills,6,Humanities
46,Passage Title: Superstition,"The most straightforward assumption would be that superstition results from causal fallacies.  One event is seen to have followed another, but this sequence may simply be chance.  As Hume warned, empirical observations can easily lead to mistaken connections in the human mind.  This erroneous type of reasoning has occurred throughout history.  For example, the Greek writer Plutarch reported that maternal ingestion of specific foods and herbs would ensure male offspring, Roman citizens thought that sleeping next to pieces of iron could prevent nightmares, and the ancient Chinese believed they could predict the future by analyzing fractures in animal bones.
But if causal fallacies are to blame for such beliefs, then the persistence of superstition in society seems mysterious.  Superstitions should quickly vanish after continued observations.  In colloquial terms, people surely would see that superstitious practices did not ""work.""  However, superstitions are not so easily dispelled.  As the early modern English scientist Francis Bacon lamented, ""The human understanding is of its own nature prone to suppose the existence of more order and regularity in the world than it finds.""
In light of those points, it must be remembered that a scientific mindset is essentially a modern privilege.  Standards of science-based experimentation are second nature today but were largely absent a millennium or more prior to the sixteenth- and seventeenth-century Renaissance.  Still, that is not to say that such methods were entirely unheard of.  Saint Augustine rejected astrological predictions in the fourth century after learning of a slave and a noble who had been born under the same stars.  Clearly, his conclusion was based on an inference from observed fact.  Centuries earlier, the Roman thinker Lucretius attributed disease to material causes, not supernatural forces.
Yet even contemporary knowledge has not eliminated magical beliefs.  For instance, adults laugh at children's reluctance to step on cracks in the pavement but build skyscrapers such as New York's Macmillan Building, where Floor 12 is followed by Floor 14.  University graduates can describe the physical laws governing the world but refuse to watch a sporting event without wearing a ""lucky"" shirt.
One potential motive for superstitious belief does not truly stem from belief at all, but from purely pragmatic concerns like those described by William James or Blaise Pascal.  A person might decide that there is nothing to lose from acting based on a superstition; therefore, one might as well heed it ""just in case"" there really is an associated causal effect.  On those grounds, following the superstition would involve a sort of rationality.  If one's situation is sufficiently desperate, such thinking would be all the more appealing.  However, the rationality is a skewed one, and to act on it prizes wishful thinking over reason.
Some may simply conclude that humanity is inherently prone to superstition.  In the novel Jingo, author Terry Pratchett emphasized this point by describing an exception: his character Ahmed was ""substitious.""  In contrast to being superstitious, Ahmed ""believed instead in the things that were true in which no one else believed.""  Fictional exceptions aside, superstitions still hold sway in many people's minds.  However, over time the spread of knowledge can be expected to refute magical beliefs; thus humankind can only benefit from naturalistic explanations.  For example, to walk under a ladder is indeed ""unlucky"" because doing so is dangerous.  An umbrella opened indoors may strike objects in the enclosed space without any mystical forces to cause this ""misfortune.""
Given such clarifications, superstitions should lose appeal, despite what Pratchett implies about human tendencies.  Genuine causal connections can be distinguished from fallacious ones, and empirical observations can expose superstitions as errors rather than engendering them.",Critical Analysis & Reasoning Skills,5,Social Sciences
47,Passage Title: Simeon the Stylite,"In AD 313, Constantine declared Christianity the official religion of the Roman Empire, and the persecution of the Christians ceased.  Subsequently, the number of religious recluses proliferated.  Hermits retreated in droves to the Scetis Desert of Egypt, paradoxically seeking solitude.  By the fifth century, many had gathered into cenobitic societies that later evolved into monasteries.  Some renunciants, however, explored styles of seclusion that featured ever-more rigorous, innovative, and questionable displays of self-discipline and physical austerity.
One of these solitaries gained world renown as the first of what were known as ""stylites,"" or pillar saints, from the ancient Greek word stylos, meaning pillar.  The son of a shepherd from the Amanus Mountains of Syria, Simeon joined a monastic community at the age of 16 but felt called to a more reclusive and stringent mode of existence.  Withdrawing to a nearby hut, he took up such self-mortifying practices as abstaining from food and drink throughout the whole of Lent and standing upright for as long as possible.  After three years of this voluntary confinement, he settled upon a rocky prominence in the desert where he continued his practice, at times overwhelmed by the pilgrims who thronged to hear his spiritual counsel.
When Simeon spied a pillar amidst the ancient ruins of the surrounding area, he contrived a new, radical method of sequestration by constructing a small platform at the top of the column where he could stand.  There he vowed to live out the remainder of his days, exposed to every vagary of weather.  Village boys climbed a ladder to deliver supplies.
Simeon was not, however, without precursors.  Among purported proto-stylites was Epidus, a recluse from Palestine who dwelt in a mountaintop cave for 25 years.  Saint Gregory of Nazianzus told of a fourth-century solitary who stood for years without lying down.  Theodoret, Simeon's contemporary and Bishop of Cyrus, reported that a hermit had lived for 10 years inside a tub suspended in midair by poles.
As Simeon's fame spread throughout the Byzantine or Eastern Roman Empire, he mounted increasingly taller pillars to escape the madding crowds; his vertical reclusion ultimately reached 80 feet.  For the next 37 years, Simeon stood on the platform atop his pillar, perfecting his self-mortifying exercises and preaching twice a day.  Although certain critics deemed this lifestyle a pointless, self-willed martyrdom, he won the veneration of Emperor Theodosius.  Moreover, the pathos of the stylite's demise was testimony to his conviction.  As Edward Gibbon relates in his history of the Roman Empire, ""The progress of an ulcer in [his] thigh might shorten, but could not disturb, this celestial life, and the patient Hermit expired, without descending from his column.""
Simeon inspired more than a few imitators.  As one scholar commented, ""For the next century…stylites were a common sight throughout the Byzantine Levant.""  For many pillar dwellers, a rudimentary shelter built onto the top of the column helped to mitigate the effects of exposure.  Female pillar saints were not unheard of, and there were reports of stylites who found themselves stationed close enough to debate theological issues from their respective posts.  Among notable followers, Simeon the Younger took up a perch near his predecessor's in Aleppo in Syria while Daniel the Stylite occupied a column near Constantinople for 33 years.  In the sixth century, Simeon's feat was surpassed by Alypius, who is said to have dwelt atop his pillar for a total of 67 years, the last 14 of which he spent lying down.  Nevertheless, it was Simeon's reputation that, for better or worse, endured.  In 1833, the English poet Tennyson composed an ironic verse in which the stylite boasts, ""Show me the man hath suffered more than I.""",Critical Analysis & Reasoning Skills,4,Social Sciences
48,Passage Title: Socialism,"For decades, socialism has been a pervasive bogeyman laying siege at the periphery of the American psyche.  Policies and candidates alike are dismissed out of hand by pronouncing them ""socialist,"" the term sufficing to preclude further consideration.  However, such reactions to any topic ought to give us pause: have we truly judged an idea as unworthy, impractical, or unjust?  Or do we simply demonstrate an unreflective fear of a label?
Initial prejudice notwithstanding, legitimate scrutiny is warranted.  Perhaps the gravest threat of socialist programmes to capitalist America is their perceived unfairness—that the rewards of labor or industry will be siphoned from those who rightfully own or generate them and bestowed upon those who did not contribute to their production.
Shall we accept without question, however, the assumptions intrinsic to such notions of ""rightful"" ownership, or more generally, those of fairness itself?  In fact, capitalists and socialists will likely agree on the broad outlines of what fairness entails.  The objection to redistribution of wealth rests on the idea that it takes from the more deserving and gives to the less—if not to the lazy or the layabout.  Given equality of opportunity, why should he who toils be denied the right of harvest?  Why should he who idles be afforded an equal reward?
Nonetheless, this claim of initial social equality is precisely what socialism rejects as illusory.  Let us suppose that everyone in society begins life upon equal footing.  None are afforded a greater portion of opportunity, but all are competitors in open tests of skill over time.  By merit, coupled with the vicissitudes of fortune, some ascend to the heights of success, others plummet to the depths of failure, still more inhabit some middle ground.  Equality is shattered, but justly, and from an early uniformity of status rise differences in wealth, garnered according to the abilities and enterprise displayed by each person.
As time passes, however, the wealth or destitution of one generation is bequeathed to the next, and then inequality becomes ingrained.  What was before an earned difference of outcome is now an unearned difference of potential.  One individual has an advantage that another lacks—not gained through striving and perseverance but inherited through accident of birth.
Such effects only compound over time.  Inevitably, dynasties of wealth accrue by which a few through mere circumstance enjoy economic mastery over the many, who face through the tyranny of chance an undeserved impediment.  The socialist's claim, then, is that complaints against redistribution of wealth fail in light of the very concern for fairness upon which they are founded.
This same concern has led various leaders throughout history to implement socialism.  For instance, eleventh-century Chinese statesman Wang An-shih instituted state control of the economy ""with a view to succoring the working classes and preventing them from being ground into dust by the rich.""  This noble goal met with only temporary success, however, as the system later fell to governmental corruption, private greed, and untimely droughts and floods.
Are the dismissive reactions towards socialism then justified?  Not as such.  Capitalism has many virtues, but it is equally liable to exploitation.  In post-industrial America, governmental reforms and regulations on commerce and industry were required to redress the systemic injustices perpetuated by dynastic wealth, as well as to protect the welfare of a multitude bound to wages insufficient to secure life's necessities.  Perhaps the lesson to be drawn is that a ""pure"" form of either capitalism or socialism will ultimately prove destructive.  Rather than embrace or reject one system from the outset, we must avoid a focus on labels and instead inquire which ideas, and in what combination, best promote the justice whose value we profess.",Critical Analysis & Reasoning Skills,6,Social Sciences
49,Passage Title: On Conducting,"To the untutored layman, an orchestra's or choir's conductor may seem a superfluous appendage (although the view is unlikely to be expressed in such lofty terms).  Indeed, this sentiment is aptly conveyed by one bewildered concert-goer, who asked his companion: ""They know the song, and the music's right there in front of them.  So what do they need the guy with the wand for?""  This flagrant ignorance aside, it is true that the purpose of a conductor (and his baton) is but little understood by those who only listen to music rather than produce it.  Yet conductors play a pivotal role in these productions, elevating a performance beyond what it would be as merely a group of musicians engaged in a common cause.
For, as one student of conducting perceptively remarked, a conductor does not ""simply allow the ensemble to discover its own interpretation of the piece…the ensemble should be absorbed in the music and character of the piece through what they see in the conductor.""  In essence, the conductor is a unifying force, effecting a full transformation of complementary musical parts into a singular yet multifaceted whole.  A composer's vision is thereby transmitted through the musicians to be delivered to the audience—by the hand of the conductor, the truth of the music is made manifest.
Nor is it purely a figure of speech that one can see a musical piece's character in the conductor.  Timing, volume, and emotional expressiveness can all be communicated non-verbally; not only through overt gestures but kinesthetically, via the operation of mirror neurons.  As Riikka Pietiläinen-Caffrey notes, ""Studies of the mirror neuron system are helpful for explaining scientifically what conductors already know by experience.""  Singers subconsciously mimic physical cues from a conductor, affecting their ""tone of voice, pronunciation, gesture, body posture, and breathing rates.""  Through even small adjustments to his or her own body, a conductor may therefore correct—or exacerbate—issues with singers' vocal quality, stance, proper inhalation, and so on, both during rehearsals and, more crucially, in concerts.  The conductor is the model upon which all aspects of the performance are shaped; thus, his or her physical comportment is of utmost significance.
Such factors explain why the comparison of a conductor to a sports coach would represent an inadequate understanding.  For while a coach instructs the players, determining their course and addressing their deficiencies, he does not take the field or court along with them.  However much he guides them, when the whistle blows, it is they who partake in the actual striving.
A more apt analogy is provided by Paul Engelstad: ""At the conclusion of a fine concert, the conductor shares the applause and is quick to give credit to his choir.  But when the ship flounders and is lost, the responsible captain accepts all blame and absolves his crew from any fault.""  The conductor is as much a performer as the singer or instrumentalist under his command; together they voyage successfully or sink.  But though their fate is shared, both its burden and authority fall solely on the conductor: ""Just as there is no democracy aboard a ship on the high seas, so is there the strictest form of dictatorship on the concert stage.  As you conduct, your every gesture must be obeyed immediately and without question!""
The conductor as captain thus gives the music its ultimate sense and expression.  The musical score has its instructions, but it is the conductor's interpretive vision that determines how the notes should be played or sung.  In short, the conductor infuses a work with passion, that element which, in the words of Terry York, ""modulates the performance to something greater than the sum of its parts.""",Critical Analysis & Reasoning Skills,4,Humanities
50,Passage Title: The Kiss to All the World,"Housed in an underground chamber of the Vienna Secession Building—a special exhibition space built for early modernist art—is the Beethoven Frieze, the creation of innovative Austrian painter Gustav Klimt.  One hundred twelve feet long and seven feet high, the frieze offers a visual representation of the finale to Beethoven's Ninth Symphony.  This spectacular wall painting was commissioned as part of the fourteenth exhibition of the Viennese Secession, an association of radically nontraditional artists founded in 1897 and headed by Klimt.  Unveiled as an homage to the great composer in 1902, the exhibit was meant to be temporary.  However, in the case of Klimt's frieze, what began as an ad hoc display, painted directly on brittle plaster, was restored and became an enduring masterpiece.
With his Ninth Symphony, first performed in 1824, Beethoven revolutionized the symphonic genre; in particular, the final movement was the first such work to feature a chorus.  Ironically, the prima materia of that sublime finale was a drinking song, called ""Ode to Joy,"" written by the German Romantic poet Friedrich Schiller.  As biographer Jan Swafford sums up, Beethoven ""erected a movement of epic scope on a humble little tune that anyone can sing.""  In Swafford's astute estimation, the Ninth embodies Beethoven's own quest for the perfect melody; it is about ""the composer composing.""  Having variously served as protest song, hymn to freedom, and unifying chant, the nearly two-hundred-year-old choral piece has been regularly hailed as ""a universal human anthem"" and ""the battle cry of humanity.""
Klimt further immortalized the tune with his monumental frieze spanning three sides of the room it occupies.  The ""Ode to Joy"" mural depicts humanity's desperate struggle to achieve happiness through the power of art.  In the process, the painter reveals his own tortured search for the perfect style.  As a result, the finished frieze reflects a syncretism of ancient Greek, Byzantine, early medieval, Japanese, and contemporary Viennese art, yet also gleams with the unique gold inlay that would characterize Klimt's work.
In illustrating Beethoven's rarefied version of Schiller's ode, Klimt's decorative cycle constitutes a visual allegory, a linear narrative that can be read from left to right.  First, sylph-like female figures—genii signifying the human yearning for happiness—float past.  Next, naked, pitiable figures, including the emaciated form of a girl and a kneeling couple, epitomize humanity's suffering.  They implore a knight in shining golden armor, behind whom looms the feminine dyad of Compassion and Ambition, to protect humankind.
The middle wall portrays the hostile forces—external as well as internal—that obstruct human happiness.  The giant gorilla-like monster Typhoeus, whose lower body morphs into that of a winding serpent, symbolizes not only the typhoid disease that ravaged Europe but also the destructive winds of war.  To the left stand his three, snake-haired, Gorgon-like daughters, above whom peer the grimacing faces of Sickness, Madness, and Death—scourges prevalent in the Viennese society of Klimt's day.  To the monster's right, the threefold evils of Lasciviousness, Wantonness, and Intemperance lurk, while dwarfed before his serpentine coil appears the horrifying hag of Gnawing Grief.
On the far wall, the ephemeral feminine genii of human desires once again float past.  In the following panel, a vertical arrangement of five females appearing to awaken represents the different art forms.  Farther to the right, the magnificent image of a solitary muse holding a golden lyre attests that the human quest for happiness will find its ultimate fulfillment in art.  The narrative concludes with a choir of angels singing in paradise as, locked in an embrace, a man and woman kiss.  Thus, humanity's longing for joy culminates in what Schiller describes as ""the kiss to all the world.""  The message reverberates: art will save mankind.",Critical Analysis & Reasoning Skills,5,Humanities
51,Passage Title: Twin Earth,"A long-standing position in linguistics is that terms with the same intensional meaning must refer to the same objects in the world.  Hilary Putnam's ""Twin Earth"" thought experiment challenges this view by arguing that two speakers could use a term in the same way yet refer to different entities.  In the experiment's original formulation, Putnam asks the reader to imagine Earth centuries in the past, along with a nearly identical Twin Earth.  However, while the substance called 'water' on Twin Earth is indistinguishable from Earth water in terms of appearance, taste, boiling point, etc., it is not H2O but a different chemical compound that Putnam calls 'XYZ.'
Given this scenario, the intension of the term 'water' would be the same on Earth and Twin Earth.  People on both worlds would possess the same concept of that term and embody the same mental states while using it.  However, the extension of 'water' on each world would actually be different, because it would apply to H2O on Earth but XYZ on Twin Earth.  Therefore, Putnam argues that a term's meaning is constituted partially by the features of a speaker's world, even those of which the speaker is unaware.
The most immediate objection to Putnam's argument is that the situation described is incoherent, as the human body is largely composed of water.  Although this fact means that Putnam's description of Twin Earth is literally impossible, most scholars have been oddly willing to overlook this deficiency.  For instance, Dennett deems it irrelevant, while Ebbs opines that ""the difficulties it raises need not affect the point of the thought experiment.""
If for whatever reason we ignore that objection, we may at least attend to a different example, set in contemporary times.  In this version of Twin Earth, the metal molybdenum is used instead of aluminum for pots and pans, and ordinary people cannot tell the difference between them.  ""Finally,"" Putnam adds, ""we shall assume that the words 'aluminum' and 'molybdenum' are switched on Twin Earth: 'aluminum' is the name of molybdenum, and 'molybdenum' is the name of aluminum.""
This formulation avoids the problems inherent to ""twin water,"" as switching molybdenum and aluminum would not entail an essential physical difference between people on Earth and those on Twin Earth.  Thus this example might seem to justify Putnam's position on linguistic reference.  Given an Earthling Oscar1 and his Twin Earth counterpart Oscar2: ""there may be no difference at all in their psychological states when they use the word 'aluminum'; nevertheless, we have to say that 'aluminum' has the extension aluminum in the idiolect of Oscar1, and the extension molybdenum in the idiolect of Oscar2.""
However, some linguists, such as Crane, have posed an intriguing question in response to this claim.  Why not suppose that for Oscar1 and Oscar2, the extension of the word 'aluminum' is both aluminum and molybdenum?  If either man somehow encountered a pan from the other Earth, he would call its metal 'aluminum,' hence each man's use of that word arguably refers to both metals.  In that case, while 'aluminum' would indeed have the same intension on Earth and Twin Earth, it would have the same extension as well, dissolving Putnam's argument.
Putnam might object that only features of each man's actual world fixes the extension of 'aluminum.'  On that view, the term's meaning is not affected by the hypothetical question of how, for instance, Oscar1 would mentally classify a pan from Twin Earth.  But that question does not really differ from the equally hypothetical one of how Oscar1 would classify any new pan that he happened to encounter on Earth.  Accordingly, Crane's response to the Twin Earth scenario cannot be dismissed so easily.",Critical Analysis & Reasoning Skills,4,Social Sciences
52,Passage Title: Gorgias' (Utterly Ironic) Encomium of Helen,"Composed around 400 BCE as a demonstration piece by the Greek sophist and rhetorician Gorgias, the Encomium of Helen has well merited its reputation as an insidiously persuasive and surprisingly subversive work.  Despite advertising itself as an ""encomium,"" or speech in praise of an individual, the text does not proceed to laud its subject, the mythological Helen of Troy.  It instead defends the woman who, by abandoning her husband, King Menelaus of Sparta, and eloping with the Trojan prince Paris, has perennially borne the blame for instigating the Trojan War.  Thus, in advocating for her innocence, Gorgias' efforts must aim not just at absolving a wayward wife of wrongdoing, but at toppling the long-held tradition that turned a Greek matriarch into a traitor of country and consort.  Consequently, defending the infamous Helen amounts to defending the indefensible.  By undertaking this feat, Gorgias risks incurring the accusation frequently hurled by conservative thinkers at the sophists, namely, of contriving to make the weaker argument the stronger.  However, such paradoxical maneuvers are at home in a work defined by ironies and menaced by contradictions.
In support of the much-maligned matron, the ancient rhetorician marshals a series of arguments that, while following the laws of logic, seem more fanciful than probable.  Firstly, Gorgias submits, Helen is not guilty since her departure to Troy was likely the will of the gods or the result of fate or chance—forces against which she was powerless.  Alternatively, she may have been physically coerced, in which case she stood helpless against her abductor.  It is also conceivable that the Spartan queen, enamored at the sight of the Trojan prince, was the victim of Love, a powerful deity against which she could hardly have been expected to prevail.
A further possibility—one that Gorgias entertains at length—cites the efficacy of speech, for despite the ""subtlety"" of its material substance, speech is a ""mighty despot.""  Certainly, as Gorgias proposes, when persuasion is added to speech, it ""molds the mind in the way it wishes.""  Furthermore, he posits, its effect on the mind is ""as irresistible as that of drugs on the body.""  Therefore, the blame for Helen's seduction and consequent defection belongs not to the one persuaded, but to the persuader.  With this startling refutation, the sophist transfers the culpability for Helen's desertion not just onto the Trojan Paris as her seducer, but onto his words and onto the whole of language as an overpowering and potentially deceptive force.  Yet, ironically, this argument—along with the others—is transmitted through words.  Hence, Gorgias uses the duplicitous medium of language to demonstrate to his audience that language is itself duplicitous.  Moreover, such hyperbolic statements in praise of words could easily constitute an encomium of their own.
It is noteworthy, though typical of Gorgias, that he uses his innovative and elaborate style to reflect the substance of the Encomium with all its paradoxes.  His distinctively paradoxical style mixes poetry with prose, overflows with rhetorical figures, and juxtaposes contrasting terms.  The results are often spellbinding—indeed, the most ornate examples of Gorgianic oratory have been likened to magical incantations but also subject to unfair attacks from imperceptive critics denouncing them as ""a plethora of words and a paucity of ideas.""
Gorgias' systematic defense culminates in a complicated quod erat demonstrandum.  The orator proclaims that by revealing the truth he has not only countered ignorance but exonerated Helen from blame, thus freeing her from ignominy.  However, the Encomium now threatens to tarnish its own reputation.  In a final self-referential and self-contradictory twist, perhaps designed to cast doubt on authorial intent, the sophist discloses: ""I have composed this speech to praise Helen—and as an amusement for myself.""",Critical Analysis & Reasoning Skills,4,Humanities
53,Passage Title: Looking over Theory,"The overemphasis on theory in art scholarship is lampooned in the 1997 film Bean.  In an early exchange between its principal characters, American museum curator David Langley mistakenly believes his English visitor to be an eminent historian rather than an ignorant buffoon.  Upon learning that ""Dr."" Bean's role at the Royal National Gallery is to sit in the corner and look at the paintings, Langley remarks: ""Ah!  That is brilliant!  If only more scholars would do that, you know, just sit and look!  You know, not lecture, and write, and argue.  Just sit and look at the paintings themselves!  Now that is brilliant.  Yep.""
Although the purpose of the movie scene is simply comedic effect, Joan Holladay has made a similar point quite seriously about her work collaborating on Gothic Sculpture in America.  Reflecting on this experience, she writes: ""One of the more sobering lessons was that we may not be training our students very well at old-fashioned looking.  With the increasing interest in theory, connoisseurship has fallen by the wayside; one younger colleague had considerable trouble finding appropriate stylistic comparisons, while another noted, 'I think I'm not very good at this.'""
The danger of this state of affairs is the prospect that works of art will be fitted to theories instead of theories being fitted to works of art.  While theory cannot be abandoned, it must follow criticism rather than lead it.  Otherwise, both beginning students and established art historians may find their analyses guided toward conclusions that are more or less pre-ordained, like detectives who begin an investigation with a suspect already in mind.  An artwork's style and features must be discovered from the bottom up, letting the work speak to us on its own terms and reducing the chance that important evidence is misconstrued or overlooked.
Such an approach helps to mitigate the risks of misclassification or critical complacency, as well as the related issue of scholarship's contamination by political interests.  For instance, a regrettably long-standing assumption in Asian art studies was that Korean artworks were dependent upon outside cultural influences from China or Japan.  In some cases, this belief was the result of honest mistakes, albeit ones that more attentive research might have avoided.  As Youngna Kim stresses, however, we must not ignore the perceived value of that assumption to those wishing to justify Korea's former status as an occupied nation.  In the years since Korea gained independence in 1945, the original, non-derivative insights of Korean art have begun to be recognized.  Still, Kim points out that as recently as 1994, the fifth edition of A History of Far Eastern Art included ""only 17 Korean artworks, compared with about 240 from Japan.""  Although progress continues to be made, challenges to establishing Korean art as a more comparably sized discipline remain.
Even apart from such concerns, it is beneficial overall for works to be studied free of theoretical constraints.  This is not because established theory is necessarily wrong, but because doing so allows critical insights to develop organically.  Moreover, if there are flaws in accepted theory, then the value of this approach is made all the clearer.  Perhaps fittingly, a central plot element of Bean involves an exhibition of James McNeill Whistler's Portrait of the Artist's Mother—an intriguing fact given Frank Trapp's assertion that ""With Whistler's Mother [the critic] may be led to interpretations that contradict the habitually accepted sureties as to the meaning of the work.""  Although Trapp grants that ""some may prefer to retain the traditional view of the Whistler portrait as…homey and sentimental,"" he argues strongly against that view—suggesting in essence that scholars' understanding might change if only they would just sit and look at the painting itself.",Critical Analysis & Reasoning Skills,4,Humanities
54,Passage Title: The Semicolon: The Bane of Orthography,"Of all the marks to grace a page of prose, the semicolon may be the most controversial.  A latecomer to the punctuation pantheon, its mongrel form conflates comma and colon while its use sows the seeds of semantic confusion often enough to make it the bane of orthography.  Typically found suspended between two independent clauses, the mark serves both to connect and separate, nudging the reader forward even as it draws him backward.  Although the semicolon frequently substitutes for a conjoining word—for example, ""and"" or ""but""—unlike such terms, it fails to elucidate the nature of the relationship between the information conveyed on either side of it.  Thus, as writer Paul Robinson points out, the semicolon creates a dubious linkage he suspects may ""gloss over imprecise thought.""  Robinson also impugns the hybridized mark as likely superfluous, implying that nearly half of all semicolons might be replaced with periods while for more than a quarter, the comma would suffice.  In essayist Lewis Thomas' estimation, the semicolon's sole purpose is to signal that ""there is still some question about the preceding full sentence,"" which sometimes gives rise to ""a pleasant feeling of expectancy.""
Not a few modern writers, among them Ernest Hemingway and George Orwell, famously spurned the semicolon.  Gertrude Stein—christened the ""energetic enemy of all punctuation"" by grammar maven Lynne Truss—disparaged it as a glorified comma, a mark she scorned as ""servile.""  According to Robinson, the semicolon bears the stigma of being ""the first fancy punctuation mark"" many people learn only to overuse in hopes of lending their writing a sense of gravitas or pinch of panache.  Consequently, it is easy to see how the hybrid acquired a reputation for being both insufferably elitist and pathetically bourgeois.  Even its appearance has been zealously attacked—American novelist Donald Barthelme describes it as ""ugly as a tick on a dog's belly""—tales of early typesetters waxing rhapsodic over the elegant silhouette of its half-sibling the comma notwithstanding.
For all its purported deficiencies and disadvantages, the semicolon came to boast sundry useful functions.  Perhaps the most indisputable of these is the ability to effectively organize items in a sentence otherwise cluttered with too many commas.  As a versatile mark of myriad possibilities, the semicolon was enthusiastically embraced by the likes of George Bernard Shaw and Virginia Woolf; the latter used them idiosyncratically to stitch disparate grammatical structures into paragraph-long sentences.  In Truss' opinion, employing a semicolon represents one of the highest compliments a writer can pay her readership, as if to say that the perspicacious reader is eminently capable of deciphering authorial innuendo.  More subtle than a period, more substantive than a comma, the semicolon breeds nuance, hinting at hermeneutical depths.
It was Italian humanist and printer extraordinaire Aldus Manutius who first contrived to wed the comma with the colon to invent the semicolon in 1494.  This feat capped what Cecelia Watson has characterized as a time ""of writerly experimentation and invention""—a time ""when there were no punctuation rules,"" only a protracted ""testing-out and tinkering-with of punctuation by the fifteenth-century literati"" as they struggled to communicate the subtleties of their ideas and express their voices in print.  As Watson further notes, no sooner had the ink dried on its premier instantiation than the mark began to proliferate.  With Manutius—who also introduced the italic font and gave the comma its distinctive contours—punctuation came of age, moving from a matter of the author's discretion to a matter of rules as society progressed from oral recitation to silent reading.  Thus, rather than a servant of prosody, merely marking pauses like musical notation, punctuation became the handmaid of syntax, purveying sense along with style.",Critical Analysis & Reasoning Skills,4,Humanities
55,Passage Title: The Hunger Winter Babies,"No one would argue that much good has ever come out of the crucible of war, and certainly not from the kinds of atrocities that characterized the Second World War.  However, the horror of the Dutch famine that occurred toward the end of the war, what in the Netherlands is commonly called the Hongerwinter, has provided epidemiologists a unique opportunity to research the long-term health effects of nutritional deficiency in utero on a specific cohort of individuals.
Because it was so widespread—affecting as many as 4.5 million people in the German-occupied Netherlands—but limited in duration and swiftly followed by a return to prosperity, the Dutch famine constituted a highly aberrant situation of deprivation in a modern, developed nation.  The devastation began in September 1944 when Allied Airborne Forces failed to seize control of the bridge that crosses the Rhine River at Arnhem.  To impede German ground troops and assist the Allies, the Dutch government then ordered a railway strike, prompting the Germans to retaliate with an embargo that cut off food and fuel supplies to the central and western provinces of the Netherlands, including the country's largest urban centers.  Even after the embargo ended, railways remained dismantled and the Germans continued to requisition goods, stalling supplies.  This already dire situation was exacerbated by war-ravaged agricultural regions and an unusually harsh winter that froze the canals.  As the fighting dragged on, food rations dwindled.  Faced with escalating scarcity, urban dwellers desperately scoured the countryside seeking anything edible for which they could trade their valuables.  Many were reduced to eating tulip bulbs and grass.  The death toll from starvation alone reached 20,000.
The famine was among the best documented in modern history, and the Dutch Hunger Winter Study has been the first study to examine the long-term consequences of intrauterine starvation.  The study compared 422 individuals exposed to adverse intrauterine conditions with 463 siblings who were the products of pregnancies that either came to term before the famine began or occurred afterward.  The initial results, published in 1976, revealed that, among other health consequences, individuals exposed to deprivation during early gestation had much higher than normal rates of obesity as adults.  Research by scientists from both the Netherlands and the US, together with new information from the field of epigenetics (the study of changes in gene expression barring alteration of the genetic code), now suggests that nutritional restriction during early gestation effectively inactivates specific genes.
Additional studies have shown that genetic expression can be altered through the addition of a methyl group, an organic compound consisting of one carbon and three hydrogen atoms, to a DNA molecule.  This methylation can modify gene expression without altering the DNA sequence.  In other words, it changes an individual's phenotype, or the observable genetic expression, but does not change the genome.  With this discovery, scientists now theorize that in individuals whose mothers suffered near-starvation during the early stages of pregnancy, DNA methylation silenced specific genes that normally help boost the rate at which the body consumes cellular energy.  The inactivation of these genes lowers metabolism, causing an increased tendency toward adult obesity.
Other factors such as diet, environment, and even stress have been shown to alter methylation patterns on genes.  Furthermore, it has now been documented that the offspring of the Hunger Winter babies frequently exhibit the same health problems as their parents, including not only obesity, but also diabetes and cardiovascular disease.  Thus, with the advancement of genomics, this unfortunate period of starvation in the Netherlands offers strong evidence that DNA methylation is heritable—invaluable knowledge that can be used to understand a multitude of disease processes.",Critical Analysis & Reasoning Skills,5,Social Sciences
56,Passage Title: American Local Motives,"Locomotives were invented in England, with the first major railroad connecting Liverpool and Manchester in 1830.  However, it was in America that railroads would be put to the greatest use in the nineteenth century.  On May 10, 1869, the Union Pacific and Central Pacific lines met at Promontory Point, Utah, joining from opposite directions to complete a years-long project—the Transcontinental Railroad.  This momentous event connected the eastern half of the United States with its western frontier and facilitated the construction of additional lines in between.  As a result, journeys that had previously taken several months by horse and carriage now required less than a week's travel.  By 1887 there were nearly 164,000 miles of railroad tracks in America, and by 1916 that number had swelled to over 254,000.
While the United States still has the largest railroad network in the world, it operates largely in the background of American life, and citizens no longer view trains with the sense of importance those machines once commanded.  Nevertheless, the economic and industrial advantages those citizens enjoy today would not have been possible without America's history of trains; as Tom Zoellner reminds us, ""Under the skin of modernity lies a skeleton of railroad tracks.""  Although airplanes and automobiles have now assumed greater prominence, the time has arrived for the resurgence of railroads.  A revitalized and advanced railway system would confer numerous essential benefits on both the United States and the globe.
The chief obstacles to garnering support for such a project are the current dominance of the automobile and the languishing technology of existing railroads.  In a sense these two obstacles are one, as American dependence on personal automobiles is partially due to the paucity of rapid public transportation.  The railroads of Europe and Japan, by comparison, have vastly outpaced their American counterparts.  Japan has operated high-speed rail lines continuously since 1964, and in 2007, a French train set a record of 357 miles per hour.  While that speed was achieved under tightly controlled conditions, it still speaks to the great disparity in railroad development between the United States and other countries since the mid-twentieth century.  British trains travel at speeds much higher than those in America, where both the trains themselves and the infrastructure to support them have simply been allowed to fall behind.  In much of Europe it is common for trains to travel at close to 200 miles per hour.
To invest in a modern network of railroads would improve the United States in much the same way that the first railroads did in the nineteenth and early twentieth centuries.  A high-speed passenger rail system would dramatically transform American life as travel between cities and states became quicker and more convenient, encouraging commerce, business, and tourism.  Such a system would also make important strides in environmental preservation.  According to a 2007 British study, ""CO2 emissions from aircraft operations are...at least five times greater"" than those from high-speed trains.  For similar reasons, Osaka, Japan, was ranked as ""the best…green transportation city in Asia"" by the 2011 Green City Index.  As Lee-in Chen Chiu notes in The Kyoto Economic Review, Osakans travel by railway more than twice as much as they travel by car.
It is true that developing a countrywide high-speed rail system would come with significant costs.  However, that was also true of the original Transcontinental Railroad, as indeed it is with virtually any great project undertaken for the public good.  We should thus move ahead with confidence that the rewards will outweigh the expenditure as citizens increasingly choose to travel by train.  Both for society's gain and the crucial well-being of the planet, our path forward should proceed upon rails.",Critical Analysis & Reasoning Skills,6,Social Sciences
57,Passage Title: Stock Market Crash,"The 1929 stock market crash in the United States, also known as the Great Crash, affected virtually every American who had a stake in the financial market.  In its wake, a deep economic crisis creating political and social unrest caused the United States government to create a strict financial regulatory system that worked effectively from the 1930s through the 1960s.
In the late 1930s, as experts clamored to unravel the source of this financial disaster, British economist John Maynard Keynes organized a dynamic revolution in neoclassical macroeconomics, purporting that free markets could ultimately lead to more adequate employment, rather than the normative ""boom and bust"" cycle of fiscal recessions and depressions in the United States.  However, Keynes also supported government interference, contending that a reduction in consumer spending due to economic hardship must be adequately supplemented by government expenditure.  The American economics professor Hyman Minsky joined the Keynesian cause two decades later, adding that an accumulation of debt by the national government drives an unstable economy toward crisis, especially in the nongovernment sector.
Preceding the Great Crash, the financial market was already unstable, which in hindsight has proven to be one of the many warning signs for emerging crises.  Regrettably, these ""red flags"" were not characterized until after the collapse.  To prevent another crash as experienced in 1929 and, to a lesser extent in 2008, changes must be made as financial markets expand and become more intricate.  Many economists agree that the mainstream theory of financial markets, which is the foundation of support for the archaic New Financial Architecture (NFA), must be replaced by the realistic economic theories proposed by Keynes and Minsky.  In opposition to singular market ownership, Keynesian economics points to a macroeconomic system wherein a ""mixed economy"" places the bulk of financial responsibility on households, to be supplemented by governmental spending when necessary.  However, NFA regulations tout the practice of ""business as usual,"" allowing the market to cycle in and out of recessions.
Progress has been made toward overall reform, however slowly.  A January 2009 report from the Comptroller General's Office to the United States Congress was delivered in hopes of shaping potential regulatory reform efforts.  These efforts ""seek to: (1) describe how regulation has evolved in banking, securities, thrifts, credit unions, futures, insurance, secondary mortgage markets and other important areas; (2) define several key changes in the financial sector in recent decades that have highlighted significant limitations and gaps in the existing regulatory system; and (3) present an evaluation framework that can be used by Congress and others to restructure regulatory reform.""
Also, there must be a broad political mandate in support of serious financial regulatory reforms.  For too long, monetary elites such as lobbyists, private bankers, and commercial investors have controlled a potentially fragile financial system.  Regulation is needed for remedying market failure and protecting the economy from an imminent collapse.
These conjectures are meant to replace fragmented and complex arrangements of federal and state regulations—put into place over the past 150 years—that have not kept pace with major developments in financial markets and commercial products in recent decades.  Without safeguards in place, the American financial market will face a continuous ebb and flow of prosperity and may be forced to face another crisis.",Critical Analysis & Reasoning Skills,5,Social Sciences
58,"Passage Title: ""Unvanquishable Greenness""","An untamed but generative masculine spirit, ubiquitous though often elusive, haunts the interconnection of humans with nature.  Among its most prominent images are the carved faces, formed from leaves or sprouting them, that peer out from medieval church ceilings and walls.  At once comic and forbidding, this foliate head is neither a gargoyle nor a merely decorative motif, but an archetype whose history stretches as deeply into the mists of time and myth as the roots of a tree.  Nevertheless, it was only as recently as 1939 that British scholar Julia Hamilton Somerset, better known as Lady Raglan, noting a carved face of entwined leaves in a church in southeast Wales, initiated a study of similar images and christened this mysterious male presence the ""Green Man.""
Like many mythological figures, the Green Man is syncretic, interweaving several images and themes or variations on a pattern.  He is, as writer John Matthews claims, ""far larger than any simple attempt to define him.""  Chiefly, the Green Man symbolizes the union of humans and nature.  Indissolubly linked with the vegetative cycle and the agricultural year, he exudes vitality and fertility and signals both material and spiritual abundance.  Whether as the foliage-covered King of the May Day, also called Jack-in-the-Green, or as the King of the Harvest, John Barleycorn, the Green Man has been an indispensable element of traditional European village celebrations.  He is Keeper of the Forest as well as woodwose, the wild man of the woods, and is sometimes recognized as the consort of Mother Nature.
Green Man images can be found in mosaics and carvings from the early Roman Empire.  Originally a pagan icon, the Green Man was incorporated into early Christian iconography, reaching a zenith of architectural popularity in Europe from the eleventh through the fifteenth century.  Notably, Chartres Cathedral in France, built in 1194, features 70 foliate heads, while in Rosslyn Chapel in Scotland, constructed in 1446, no fewer than 103 such heads can be counted.  The Green Man was also known farther east; for instance, near Hatra in present-day Iraq, an imposing leafy countenance stares out from the façade of an ancient temple.
The Green Man's lineage is multifaceted.  In the West he is a variant of Dionysus, the god of the vine who dies and is reborn.  Indeed, the fifth-century BCE statue of a leaf-clad Dionysus or Bacchus in Naples, Italy, is perhaps his oldest surviving image, although Dionysus may himself descend from the green-skinned Egyptian god Osiris who likewise dies and rises.  The Green Man is thought to be related to the rustic Greco-Roman deities Pan and Silvanus, and, more speculatively, to Cernunnos, the Celtic horned god of the hunt and lord of the animals.  In the East this mythic being manifests in the figure of Al-Khidir, the Verdant or Green One who is a spiritual guide of heroes in the Koran.
Green Man figures also pervade Western literature, which was influenced at its origin by the Sumerian Epic of Gilgamesh and its character of Enkidu, a wild, shaggy nature man fashioned by the gods as a counterpart to the young King Gilgamesh.  Later the medieval tales of King Arthur feature the baffling Green Knight, who, like the eastern Al-Khidir, is a warrior guide.  In the fifteenth century, the Green Man reemerges as Robin Hood; in the twentieth century, J. M. Barrie's eternally youthful Peter Pan is tellingly ""clad in skeleton leaves and the juices that flow from trees.""
In Matthews' view, the Green Man is the embodiment of ""unvanquishable greenness.""  This perspective might explain the recurrence of his image and its recent adoption by environmental awareness movements as an assurance of ecological renewal.",Critical Analysis & Reasoning Skills,4,Social Sciences
59,Passage Title: Fred Rogers and the Pastore Hearing,"The fledgling public television industry faced an uncertain future in the late 1960s.  Near the end of his presidency, Democrat Lyndon Johnson allocated $20 million to create the Corporation for Public Broadcasting.  In 1969, however, his newly elected Republican successor Richard Nixon hoped to reduce government spending and was considering a cut of 50% or more to that appropriation.  Before a final decision was made, public broadcasting representatives were invited to appear before Congress to make their case for the full funding.  One of their leaders, future Public Broadcasting Service president Hartford N. Gunn, Jr., asked TV personality Fred Rogers to join him in testifying as a key advocate for public television.
As the creator and host of the show Mister Rogers' Neighborhood, Rogers embodied moral values, a deep sense of purpose, and concern for the welfare of children, all of which would later make him one of the most beloved figures in television history.  At the time of the congressional hearing, however, Rogers' show had aired nationally for only a year.  Although highly regarded by his current audience, Rogers did not yet possess the level of recognition he would eventually obtain.  His televised appearance before the Subcommittee on Communications would be many Americans' first time seeing him.  In addition, despite having studied early childhood development under such prominent psychologists as Benjamin Spock and Margaret McFarland, Rogers was not widely known as an expert in that field.  In short, Rogers was an unusual figure to be chosen to testify before Congress.  Rogers' biographer Maxwell King describes Gunn's decision to depend so heavily on Rogers' testimony as ""a pretty big gamble.""
Chairing the subcommittee hearing was Democratic senator John Pastore of Rhode Island, portrayed by King as ""a blunt, no-nonsense social conservative who shared the Republican interest in keeping federal spending in line.""  Pastore had also been critical of the television industry for what he saw as its promotion of immorality.  Although those criticisms had been leveled primarily at commercial programs, Pastore's views certainly couldn't have helped the case that Rogers needed to make.  The unproven nature of public television as a recent enterprise was an additional hurdle to securing the funding.
Nevertheless, despite the odds stacked against him, Fred Rogers greatly impressed Pastore and the rest of the subcommittee.  Rather than giving the type of formal testimony that might have been expected at such a hearing, Rogers simply spoke earnestly about what, in his view, made public television so important.  He shared Pastore's worries about television's content, and was especially concerned about television geared toward children.  Mister Rogers' Neighborhood, which addressed children directly and acknowledged their unique perspective, was based on Rogers' vision of public programming as a positive social force.  As he explained, taking children's problems seriously and showing them that their lives can be understood and managed makes a profound difference in the kind of people they become and the society they help to create.
The highlight of the testimony came when Rogers recited the lyrics to one of the songs from his show, which had been inspired by a child's question about how to deal with anger.  The song acknowledges the frustrations and fears of childhood while affirming each child's ability to control their feelings and behavior.  Rogers' simple but encouraging words clearly moved the subcommittee members, including Senator Pastore, who responded: ""Looks like you just earned the twenty million dollars.""  Hartford Gunn's gamble had paid off.  In an interesting twist of affairs, Pastore himself testified the following year at the White House Conference on Children—an event chaired by none other than Fred Rogers.",Critical Analysis & Reasoning Skills,4,Social Sciences
60,Passage Title: The Divine Sign of Socrates,"From his bare feet to his bald pate, the potentially shapeshifting figure of Socrates found in the literary tradition that arose after his controversial trial and death presents an intriguing array of oddities and unorthodoxies.  Most conspicuously, his unshod and shabby sartorial state flaunted poverty at a time when the city of Athens had become obsessed with wealth and its trappings.  Yet the philosopher's peculiar appearance was but a hint of the strange new calling he embraced.  Inspired perhaps by the famous Delphic dictum ""Know thyself,"" he embarked on a mission devoted to finding truth through dialogue.  In what struck some as a dangerous new method of inquiry, he subjected nearly everyone he encountered to intense cross-examination, mercilessly exposing the ignorance of his interlocutors.  Moreover, in a culture that still put stock in magic, the highly charismatic, entertaining, and at times infuriating Socrates appeared to be a sorcerer bewitching the aristocratic young men of Athens who followed him fanatically about the agora.
By all credible accounts, this exceedingly eccentric, self-styled radical truth-seeker had more than a whiff of the uncanny about him.  As Socrates himself explains in Plato's Republic, he was both blessed and burdened with a supernatural phenomenon in the form of a daimonion or inner spirit that always guided him: ""This began when I was a child.  It is a voice, and whenever it speaks, it turns me away from something I am about to do, but it never encourages me to do anything.""  An overtly rational thinker, Socrates nonetheless considered these warnings—or, in James Miller's words, ""the audible interdictions he experienced as irresistible""—to be infallible.  Such oracular injunctions were highly anomalous as tutelary spirits were thought to assume a more nuanced presence.  Some scholars have dismissed Socrates' recurring sign as a hallucination or psychological aberration.  Others have conjectured that the internal voice might be attributable to the cataleptic or trancelike episodes from which the philosopher purportedly suffered.  Indeed, as Miller notes, ""Socrates was storied for the abstracted states that overtook him""; not infrequently, his companions would see him stop in his tracks and stand still for hours, completely lost in thought.
As Socrates further insisted, it was only the protestations of this apotreptic voice that held him back from entering the political arena.  Even so, its personal admonitions could not spare him persecution.  Despite the political amnesty extended by the resurgent democracy that succeeded the interim pro-Spartan oligarchy, the thinker's notoriety and ambiguous allegiances aroused suspicions.  In 399 BCE, Socrates was brought before the court on trumped-up charges of impiety; these included willfully neglecting the traditional divinities, flagrantly introducing new gods to the city, and wittingly corrupting the youth.  Athenian society recognized no division between religious and civic duties, and capricious gods demanded constant appeasement through sacrifices and rituals.  Consequently, belief in a purely private deity—particularly a wholly benevolent deity conveying unequivocal messages—was inadmissible.  Worse, as Socrates' own testimony revealed, he honored this personal god's authority above even the laws of the city.  Hence, the philosopher's daimonion loomed over his indictment, conviction, and sentencing.
Nevertheless, in his defense speech as reconstructed by Plato in the Apology, Socrates maintained confidence in the protective nature and prophetic powers of his inner monitor.  He never questioned its affirmatory silence toward his predicament, remarking, ""The divine faculty would surely have opposed me had I been going to evil and not to good.""  Thus, Socrates acknowledged that his daimonion had its reasons, however inscrutable.  Variously described as malcontent and martyr, public nuisance and prophet, laughingstock and hero, the mercurial Athenian, like the sign that guided him, was difficult to fathom yet impossible to ignore.",Critical Analysis & Reasoning Skills,5,Humanities
61,Passage Title: What is Missing from Benevolent Dictatorship,"A common joke states: ""Democracy is the worst form of government, except for all the other ones.""  The motivations for disparaging democracy are clear enough: its inefficiency, the intransigence of competing political viewpoints, the seeming inability of so-called leaders to take necessary actions.  Moreover, the risk of arriving at decisions based on the lowest common denominator of average (or sub-average) citizens cannot be discounted.  Hence, especially if we are sufficiently assured of our own wisdom, we must also add the threat of ""mobocracy"" to this list of grievances.  Given these numerous flaws, it is tempting to wonder: might not a benevolent dictatorship be a superior form of governance?
Naturally, some caveats are in order.  To make the best possible case in favor of such a government, we must address (or stipulate away) its most obvious potential drawbacks.  For instance, we will specify that the dictator in question truly is benevolent and never acts from a corrupt or self-centered purpose.  We can remain undecided about whether this means the dictator is morally and/or intellectually perfect; however, we must assume at minimum that he or she is always motivated by a genuine belief about the best interests of the people.  Furthermore, we will also assume that when the dictator's reign ends, he or she is automatically replaced by someone of equal or greater benevolence.
These wise and kindly rulers make life stable and predictable.  When decisions are required, they are made immediately, without a laborious (and possibly futile) process of seeking consensus.  When laws must be drafted, they are aimed at the good of the people, without interference from partisan special interest groups.  When injustices arise, they are addressed, without preference given to the privileged and powerful.  At times, some citizens may disagree with a ruler's decisions for the country, but this fact is unimportant for two reasons.  First, the same is true under other forms of government.  Second, the citizens would likely be worse off if they could change those decisions, because all the ruler's actions are aimed at the citizens' best interest anyway.  In addition, a ruler can always reverse course if events do not turn out as he or she had planned.  Thus, all things are brought to fruition as they should be, with decisiveness and rapidity.  Have we now then, at least theoretically, described the perfect government?
I submit that we have not, because something important has been left out.  Note that the successes of this benevolent dictatorship are entirely of a consequentialist nature.  The right decisions are made, the right laws are drafted, the right actions are taken, all without the impediment of needing to persuade anyone of their rightness.  But by removing that impediment, one also largely removes the connection between people's welfare and their choices.  Even on the hypothesis that the citizens are better off due to the dictator's actions, those citizens can play no role in bringing about the beneficial states of affairs.  But there is an intrinsic value to possessing that capability.
A path that people have a hand in choosing is qualitatively different from a path that is laid out for them, even when both paths lead to the same destination.  Likewise, persuasion and consensus building have a value that goes beyond the mere arrival at a well-reasoned decision, even when one is on the losing side of a debate.  At bottom, democracy is founded on faith—faith that in a free contest of ideas, the good can be established and the true can be recognized.  The chance to intentionally strive for such goodness and truth, both individually and collectively, is in itself more valuable than the guarantee that they will be imposed from outside.",Critical Analysis & Reasoning Skills,5,Social Sciences
62,Passage Title: The Simulacrum,"Jorge Luis Borges' 1946 short story ""On Exactitude in Science"" plays with the well-known aphorism, ""The map is not the territory.""  In this paragraph-long fable, an empire's ambitious cartographers produce a map so spectacularly isomorphic it corresponds precisely in scale and detail with the kingdom it represents.  Perfect yet useless, the map is virtually indistinguishable from the landscape until it begins to decay.  In a 1981 treatise, Simulacra and Simulation, French sociologist Jean Baudrillard cites Borges' tale as ""the finest allegory"" of his own provocative proposition that, in the current era, we have lost the ability to distinguish between reality and its representation.  Thus, Baudrillard—known as an extreme cultural relativist and often mocked as ""the high priest of postmodernism""—postulates that ""it is no longer a question of either maps or territories""; rather, ""[s]omething has disappeared: the sovereign difference, between one and the other,"" that, he laments, once ""constituted the charm of abstraction.""  Indeed, our postmodernist world is saturated with signs, symbols, and images that are never neutral or transparent but, by mediating reality, come to construct it.  Consequently, in a sinister twist on the story, Baudrillard can assert that ""today it is the territory whose shreds slowly rot across the map.""  As he elaborates, ""[T]he territory no longer precedes the map, nor does it survive it.""  Instead, ""[i]t is…the map that precedes the territory…that engenders the territory.""  In other words, representation can be seen to precede, produce, and ultimately surpass reality.
But how can a copy be the antecedent of an original?  How does an image become so alienated from reality?  In Baudrillard's scheme, signs are subject to degeneration.  Initially, a sign reflects reality, substituting for an original; like a carefully crafted counterfeit, it is a faithful but identifiable copy.  In the next stage, the sign devolves into a less faithful copy of the original, thus distorting what it represents and masking reality.  In its penultimate phase, the representation no longer pretends to be the original but now instead pretends to be a copy; hence, it masks an absence of reality and erodes the distinction between the real and representation.  For instance, when images are mass produced, they propagate, taking on a life of their own and diminishing the authoritative aura of the original.  In the final stage of the sign's devolution, it can claim ""no relation to any reality whatsoever""; it no longer reflects or refers to reality but instead signifies nothing but signification.  It has become a pure simulation, a suspiciously specious semblance Baudrillard calls the simulacrum.
In Baudrillardian theory, when the distinction between representation and reality, sign and referent, original and reproduction disintegrates, the real disappears; we are left with copies of copies that no longer have—or never had—an original.  What remains, then, is a ""hyperreality"" of simulacra and simulation.  So, Baudrillard maintains, ""[s]imulation is no longer that of a territory, a referential being, or substance....  It is no longer a question of imitation, nor duplication, nor even parody.""  It is instead a question of substitution, of ""substituting the signs of the real for the real.""  As one writer describes it, ""The simulation is no longer a reflection of reality, nor a reference to it, but a creation of a new real by models that are not based on reality.""  This new real may be genetically engineered, as with cloning, or possibly computer generated.  Even something as essential as currency has been progressively replaced, first by checks and then by credit and debit cards, until finally only numbers, divorced from any concrete reality, are exchanged on money transfer apps.  Likewise, as reality dissolves into the hyperreality of simulacra, human experience devolves into ""a simulation of reality.""",Critical Analysis & Reasoning Skills,5,Humanities
63,Passage Title: Food Costs and Disease,"Because frequent consumption of unhealthy foods is strongly linked with cardiometabolic diseases, one way for governments to combat those afflictions may be to modify the eating habits of the general public.  Applying economic incentives or disincentives to various types of foods could potentially alter people's diets, leading to more positive health outcomes.
Utilizing national data from 2012 regarding food consumption, health, and economic status, Peñalvo et al. concluded that such price adjustments would help to prevent deaths related to cardiometabolic diseases.  According to their analysis, increasing the prices of unhealthy foods such as processed meats and sugary sodas by 10%, while reducing the prices of healthy foods such as fruit and vegetables by 10%, would prevent an estimated 3.4% of yearly deaths in the U.S.  Changing prices by 30% would have an even stronger effect, preventing an estimated 9.2% of yearly deaths.  This data comports with that found in other countries, such as ""previous modeling studies in South Africa and India, where a 20% SSB [sugar-sweetened beverage] tax was estimated to reduce diabetes prevalence by 4% over 20 years.""  The effects of price adjustments would be most pronounced on persons of lower socioeconomic status, as the researchers ""found an overall 18.2% higher price-responsiveness for low versus high SES [socioeconomic status] groups.""
This differential effect based on socioeconomic status contributes to concerns about such interventions, however.  In Harvard Public Health Review, Kates and Hayward ask: ""Well intentioned though they may be, at what point do these taxes overstep government influence on an individual's right to autonomy in decision-making?  On whom does the increased financial burden of this taxation fall?""  They note that taxes on sugar-sweetened beverages, for instance, ""are likely to have a greater impact on low-income individuals…because individuals in those settings are more likely to be beholden to cost when making decisions about food.""
However, ""well intentioned though they may be,"" the worries that Kates and Hayward express are to some extent misguided.  In particular, the idea that taxing unhealthy foods would burden those least able to afford it misses the point.  Although the increased taxes would affect anyone who continued to purchase the items despite the higher prices, the goal of raising prices on unhealthy foods is precisely to dissuade people from buying them.  As Kates and Hayward themselves remark, ""Those in low-income environments may also be the largest consumers of obesogenic foods and therefore most likely to benefit from such a lifestyle change indirectly posed by SSB taxes.""  As the goal of the taxes is to promote those lifestyle changes, the financial burden objection is a non-starter.
Given this recognition, the question regarding autonomy constitutes a more substantial issue.  Nevertheless, that concern also rests on a dubious assumption, as people's autonomy is not necessarily respected in the current situation either.  The fact that those of lower socioeconomic status are more likely to have poorer diets suggests that such persons' food choices are the result of financial constraint, not fully autonomous, rational deliberation.  Hence, by making healthy foods more affordable relative to unhealthy ones, government intervention might actually facilitate autonomous choices rather than hindering them.
On the other hand, suppose that the disproportionate consumption of unhealthy foods—and associated higher incidence of disease—among certain groups is not the result of financial hardship but rather the result of those persons' perceived self-interest.  If so, that would suggest that members of these groups are being encouraged to persist in harmful dietary habits for the sake of corporate profits.  In that case, violating autonomy for the sake of health may be permissible, as that practice would be morally preferable to the present system of corporate exploitation.",Critical Analysis & Reasoning Skills,5,Social Sciences
64,Passage Title: Arts of War: The Two Suns,"The name ""Sun Tzu"" is readily recognized by scholars and non-scholars alike; as the author of The Art of War, he is identifiable even to those who have never studied Chinese history or military strategy.  Much less familiar to the layman is his descendant Sun Bin, who one and a half centuries later (circa 350 BCE) extended the discussion of battlefield tactics with his own insights.  Although some deem Sun Bin's thinking to lack the philosophical depth displayed by his predecessor, the two strategists in fact bear many similarities, and we should not be overhasty in our evaluation of Sun Bin's contribution.
An instructive example can be found in a historical record of his advice to another commander concerning a horse race:
Sun Bin then said to Tian Ji, ""You just go ahead and make a large wager; I will see to it that you win.""…Just as the contest was to begin, Sun Bin counseled Tian Ji, ""Pit your third-best team against their finest, your finest against their second-best, and your second best against their third.""  When all three horse races were finished, though Tian Ji had lost the first race, his horses prevailed in the next two, in the end winning a thousand pieces of the king's gold.
It would be easy to dismiss this episode as a demonstration merely of cunning or desire for personal gain.  However, Sun Bin's tactic essentially aligns with Sun Tzu's famous maxim: ""If you know the enemy and know yourself, you need not fear the result of a hundred battles.""  Nothing had changed about the horses, but Sun Bin arranged their deployment in a way that ensured an overall victory.  Hence, the story is not out of place in a context of military tactics because it illustrates the adaptation of circumstance to one's advantage.
Similarly, while The Art of War remains of interest to the modern military mind, its study is even more often applied to non-martial contexts, such as commerce, politics, or simply how to live.  In one chapter, Sun Tzu gives the instruction to ""throw your soldiers into positions whence there is no escape, and they will prefer death to flight.  If they will face death, there is nothing they may not achieve.""  The military application is obvious, but the underlying idea extends much further: when one cannot give up, there is no choice but to strive for victory with full abandon.  The question then becomes whether one can play both general and soldier, throwing oneself into the inescapable position that must be confronted.
Sun Bin refers to the success or failure of an army when saying: ""If you take a road that goes nowhere, even Heaven and earth cannot make you prosper; if you take the right road, even Heaven and earth cannot waylay you.""  Nevertheless, his words apply far beyond a military perspective.  By situating warfare within ""the pattern of the heavens and the earth,"" Sun Bin conceives it as a microcosm of a larger and more abstract natural harmony.  To act in accordance with this harmony is to secure victory; to fail to do so is to ensure defeat.  Consequently, success in battle is achieved in the same way as success in any endeavor; as with the horse race, one must adapt to the natural propensities of a situation.  Ultimately, Sun Bin provides a compelling picture of battlefield tactics and strategy.  As with Sun Tzu's more famous insights, however, the true depth and value of Sun Bin's thinking lie in its broader scope and application.",Critical Analysis & Reasoning Skills,4,Humanities
65,Passage Title: Probability and The Universe,"The idea of probability is frequently misunderstood, in large part because of a conceptual confusion between objective probability and subjective probability.  The failure to make this distinction leads to an erroneous conflation of genuine possibility with what is in fact merely personal ignorance of outcome.  An example will clarify.
A standard die is rolled on a table, but the outcome of the roll is concealed.  Should an observer be asked the chance that a particular number was rolled—five, say—the natural response is 1/6.  However, this answer is incorrect.  To say there is a one-in-six chance that a five was rolled implies there is an equal chance that any of the other numbers were rolled.  But there is no equal chance, because the roll has already occurred.  Hence, the probability that the result of the roll is a five is either 100% or 0%, and the same is true for each of the other numbers.
It might be objected that such an analysis is an issue of semantics rather than a substantive claim.  For declaring the probability to be 1/6 is merely an expression that, for all we know, any number from 1 through 6 might have been rolled.  But the difference between for all we know and what is remains crucially important, because it forestalls a tendency to make scientific assertions from a perspective biased by human perception….
[T]hus, it should be clear that claims about the purpose of the universe rest on shaky ground.  In particular, we must be wary of inferences drawn from juxtaposing the existence of intelligent life with the genuinely improbable cosmological conditions that make such life possible.  Joseph Zycinski provides an instructive account of those conditions: ""If twenty billion years ago the rate of expansion were minimally smaller, the universe would have entered a stage of contraction at a time when temperatures of thousands of degrees were prevalent and life could not have appeared.  If the distribution of matter were more uniform the galaxies could not have formed.  If it were less uniform, instead of the presently observed stars, black holes would have formed from the collapsing matter.""  In short, the existence of life (let alone intelligent life) depends upon the initial conditions of the universe having conformed to an extremely narrow range of possible values.
It is tempting to go beyond Zycinski's factual point to draw deep cosmological, teleological, and even theological conclusions.  But no such conclusions follow.  The reason why a life-sustaining universe exists is that if it did not, there would be no one to wonder why a life-sustaining universe exists.  This fact is a function not of purpose, but of pre-requisite.  For instance, suppose again that a five was rolled on a die.  We now observe a five on its face, not because the five was ""meant to be"" but because one side of the die had to land face up.  Even if we imagine that the die has not six but six billion sides, that analysis is unaffected.  Yes, it was unlikely that any given value would be rolled, but some value had to be rolled.  That ""initial condition"" then set the parameters for what kind of events could possibly follow; in this case, the observation of the five.
Similarly, the existence of intelligent beings is evidence that certain physical laws obtained in the universe.  However, it is not evidence that those beings or laws were necessary or intended rather than essentially random.  The subjective probability of that outcome is irrelevant, because in this universe the objective probability of its occurrence is 100%.  Thus, the seemingly low initial chance that such a universe would exist is not in itself indicative of a purpose to its existence.",Critical Analysis & Reasoning Skills,6,Humanities
66,Passage Title: Leonardo's Bridge,"In 1482, a thirty-year-old Leonardo da Vinci left his art studio in Florence and traveled to Milan to seek employment at the court of Duke Ludovico Sforza.  A drafted letter of application to the duke detailed Leonardo's many useful skills, the majority of which concerned military developments such as ""methods for destroying any fortress"" and making ""unassailable armored chariots.""  Peacetime talents such as ""guiding water from one place to another"" or producing ""sculpture in marble, bronze and clay"" were mentioned only at the end of the letter.  Nevertheless, one of the most intriguing items is the entry stating, ""I have designed extremely light and strong bridges, adapted to be easily carried"": in other words, portable bridges.
Modern scholars question whether most of the devices mentioned in Leonardo's letter could really have been constructed.  For instance, Krull describes the fact that Leonardo never sent the letter as ""a good thing, as it was mostly bluff.""  Isaacson, though a bit more charitable, similarly declares that Leonardo's ""boasts were aspirational.""  Regarding the bridge, however, there is no doubt that the design is workable.
According to at least one account, Leonardo really did make such a bridge for Sforza.  More significantly, examples of the bridge have been built and used in contemporary times.  A sketch in one of Leonardo's notebooks illustrates the bridge's construction, which includes different sets of wooden beams cut to specific lengths.  Requiring no nails, ropes, or other fasteners, nor any tools, the bridge can be assembled by a mere handful of people, and even some children could potentially build a small version or model.
When the bridge is constructed correctly, each component helps to hold all the other components in place.  The original sketch calls for the beams to be notched so that the pieces of the bridge lock together.  However, in some cases this feature can be omitted, provided the wood is sufficiently rough.  This version of the bridge then stands and bears weight due to friction, gravity, and the physical integrity of the pieces.  In this way, the structure depends entirely on the interplay of physical forces upon its parts.
The design is modular and repeatable, meaning that a longer bridge can be made by simply adding another section, with the supports interlocked accordingly.  Given enough materials, time, and manpower, such a bridge could in principle be made to quite a great length.  However, in longer bridges the notches would in fact be needed, because some of the beams would be at steeper angles.
Despite the relative ease of constructing the bridge, there is one challenge: building it requires access to both sides of the region to be crossed, as well as the area in between.  Such a consideration might seem obvious in building any bridge, but it can still prove problematic.  If a bridge is meant to help cross a body of water, not all members of the group attempting to cross will receive its benefit, as some would have to stand in the water during the bridge's construction.  Of course, this assumes that it is safe to enter the water in the first place.  Crossing a chasm or a deep pit would pose even greater difficulties, as the bridge's prospective builders would, like the ancient Greek mathematician Archimedes, lack a place to stand.  Leonardo may have confronted that problem as well and considered how to make a bridge that was not only portable and freestanding, but constructible in sequence from one end to the other.  Although no evidence exists of such a design in the known pages of his notebooks, many pages have been lost to history and might one day be rediscovered.",Critical Analysis & Reasoning Skills,6,Humanities
67,Passage Title: Marital Naming Tradition,"Until the 1970s, many states mandated that women assume their husbands' surnames upon marriage.  Today, women are no longer bound by this constraint, and some, like journalist Ellen Goodman, have argued that future generations will view the tradition of married women changing their names as a kind of madness.  That tradition remains strongly ingrained in society, however, and the freedom to reject it comes with social costs.
Law professor Elizabeth Emens notes that although cases such as Kruzel v Podell and Dunn v Palermo established in 1975 that a married woman may legally decline to become ""Mrs. His Name"" (a term used by psychologist Jean M. Twenge), it is still customary for children's surnames to be conferred patrilinearly.  Hence, a prospective bride faces a dilemma of symbolic identity; she may either retain her nominal connection to her parents and past self or create a nominal connection to her husband and future children.  Meanwhile, Emens continues, ""her husband has the same name as his parents and his child and thus continuity across all three generations of his family.""  Alternatively, a woman might attempt to bridge both worlds through hyphenation, by which ""she alone bears the hassle of all the computer forms that apparently can't accommodate such a name, and the people who can't seem to remember it, and the people who think she's constantly trying to make a point about her independence.""
Other disadvantages are associated with societally atypical naming choices.  These difficulties range from the mild, like eliciting confusion when introducing oneself, to the more egregious, such as a woman whose new neighbors would not believe she and her husband were truly married.  Men also experience social backlash if they choose to adopt or incorporate their wives' names, such as movie reviewer Sam Van Hallgren, who acquired the ""Van"" from Carrie Van Deest.  Those cases are rare, however; thus, it is women who primarily endure these kinds of burdens.
Nevertheless, there are some who see the issue differently.  In a 1996 Good Housekeeping article, Peggy Noonan advocates for the value of name changing as reflecting the couple's commitment to marriage.  In particular, she writes that the typical ""bride in her 20s grew up in the Age of Divorce"" and that ""[t]his bride and her husband…may have fewer misconceptions than their parents about how important freedom and self-actualization are.  They may think other things are more important.""  Noonan goes on to suggest that ""for these brides, taking their husbands' names is a declaration not only of intention, but of faith…faith in yourself and your spouse.""  Noonan ends her article by applauding the trend of more couples choosing to share a married surname.
Emens criticizes this stance.  As she argues, equating commitment to marriage with a woman's willingness to change her name makes sense ""only when there is no realistic possibility of him or both changing"" to create a shared surname.  Legal scholar Kelly Snyder faults an argument from Laura Dawn Lewis on similar grounds, observing: ""It is unclear why these [reasons] should explain a woman's choice to change her name, but not a man's.""  Like Emens, Snyder stresses the importance of name to a person's sense of self as well as the legal inequities surrounding traditional marital naming.  In her view, the best way to promote a woman's freedom to make a genuine choice about her name is to facilitate men's ability to change names when marrying.  If men no longer face arduous legal and social challenges to changing their own names, then the idea of a woman doing so will no longer be seen as the natural default position.",Critical Analysis & Reasoning Skills,6,Social Sciences
68,Passage Title: Wabi-Cha: The Way of Tea,"The traditional tea ceremony evolved from multiple strands of Japanese culture.  Variously referred to as ""chanoyu,"" ""sado,"" or ""chado,"" meaning ""the way of tea,"" the ceremony is a highly codified and choreographed performance in which tea—typically a ground green variety known as ""matcha""—is prepared and presented to guests by a host.  The tea ceremony is practiced to promote the harmony of nature and humanity as well as to discipline the mind and calm the heart of those who seek enlightenment.
During the ninth century, a Buddhist monk named Eichu brought green tea to Japan from China, where it had already been cultivated for more than 1,000 years and used by monks to facilitate meditation.  Earlier in the century, the Chinese master Lu Yu had composed a treatise on the cultivation and preparation of tea called Cha Jing, or The Classic of Tea.  This treatise was heavily influenced by Buddhist ideas, which then impacted the development of the Japanese tea ceremony.  Toward the end of the twelfth century, another Japanese monk, Myoan Eisai, traveled to China to study philosophy and religion, returning with the seeds of green tea plants.  Eisai went on to build the first Zen Buddhist temple in his native land and was the first of his nation to cultivate tea purely for religious purposes.
A tea culture, or ""teaism,"" in Japan was initially popular with the ruling class, and the samurai adopted it as a status symbol.  Indeed, the tea ceremony was in its origins closely entwined with the political elite of the country; by the fifteenth century, however, it had begun to spread to all classes.  Murata Shuko, known as ""the father of the tea ceremony,"" had been largely responsible for moving the tea ceremony away from the political to become more ""transformative"" or spiritual in nature.  For instance, he greatly enhanced the simplicity of its presentation, making tea ceremonies less formal and more intimate.  Gatherings now took place in smaller tearooms or secluded teahouses rather than luxurious salons.
It was the famous tea master Sen no Rikyu who later elevated the tea ceremony to a virtual art form and codified its performance.  This meant that every action and gesture on the part of the preparer—using the kettle, gazing at the teacup, measuring the tea powder into a cup—constituted a procedure to be performed in a prescribed manner.  Even the actions of the guests evolved to become scripted in a precise ritual.  Rikyu's teachings fostered the development of the way of tea based on four cardinal principles—harmony, respect, purity, and tranquility—which were meant to be incorporated into the daily life of the tea practitioner.
Chiefly through the efforts of Shuko and Rikyu, the tea ceremony took on a distinctive artistic character and became known as wabi-cha.  The term ""wabi,"" or often ""wabi-sabi,"" refers to the quintessential Japanese aesthetic founded upon the three Buddhist marks of existence: impermanence, suffering, and emptiness.  Unlike the standard concept of beauty in the West that favors symmetry, proportion, and static perfection, wabi-sabi embraces the imperfect and celebrates the transient.  It exalts the rustic and the humble as exemplified by the pottery used in the tea ceremony, which is typically crude, ordinary looking, and even asymmetrical.  Moreover, it is thought that a chip or crack, and even the general wear and tear of repeated use, renders such objects more interesting and, hence, more aesthetically pleasing.  Outdoors, such beauty is found in the fleeting color of autumn leaves and the patina that forms on roof tiles after exposure to the elements.  The simplicity, humility, and naturalism epitomized by wabi-sabi thus became the hallmarks of the way of tea.",Critical Analysis & Reasoning Skills,5,Humanities
69,Passage Title: Lengthening the School Day,"There may be reasons to reject the idea of lengthening the school day.  None of them, however, are good reasons.  Rather, the supposed demerits of such a proposal fall easily in the face of its numerous financial and social benefits for families.
The greatest of these benefits lies in reducing the need for childcare.  It is a curious fact of American life that the adult's work schedule and the child's school schedule are misaligned.  Children rise with the sun to head to classes, only to be sent home again hours before parents return from their jobs.  In a society where, more often than not, both parents work, this discordance creates the need for an expensive arrangement to fill the gap in families' routines.  For instance, studies show that in 2016, childcare costs accounted for 9.5 to 17.5 percent of median family income, depending on the state.  Today, 40 percent of families nationwide spend over 15 percent of their income on childcare.  Transportation to and from care sites only adds to that expense.
An additional advantage of an extended school day would be to allow for greater diversity and depth in curricula.  Schools across the country have increasingly cut instruction in arts, music, and physical education (as well as recess) in order to meet objectives in reading and math.  While this unfortunate state of affairs can be partially blamed on overzealous attention to standardized tests, it points to the larger deleterious trend of narrowing students' instruction.  With a longer school day, such eliminated subjects can be restored, enriching students with a more well-rounded education.
To this proposal, however, critics may object that the added time would impose strain on educators.  Can we truly ask schoolteachers—already among the most overworked individuals in society—to endure even more hours in the classroom?  The answer is that a lengthened school day need not distress teachers nor add to their already cumbersome workload.  By providing for additional areas of study in the arts and humanities, the extension would give schools cause to hire new, perhaps specialized, faculty to offer these courses.  Moreover, the time could also be allocated to sports, academic clubs, and other extracurricular activities.
However, this point speaks to another objection, namely, the cost of adjusting the school day.  Whether through paying current teachers more or hiring new ones, implementing such a proposal would entail a significant financial expenditure.  There are at least two responses to this line of thinking.  First is that this increase in the cost of schooling would be offset and likely surpassed by the aforementioned savings in childcare.  Thus, while it is true that schools would require greater funding (likely necessitating higher property taxes), parents would ultimately pay the same or less overall, with greater educational opportunities for their children and fewer transportational burdens.  Second is that schools should be better funded regardless.  Recently, some schools—especially those in rural areas—have even reduced school weeks to only four days as a cost-saving measure.  It is beyond dispute that schools across the board both need and deserve a radically increased investment from citizens.  Lengthening the school day is simply one manifestation of how such funding should be utilized.
With this one change, states can coordinate the lives of parents and children, reduce the need for costly childcare, and expand curricular offerings.  These worthy and desirable aims provide a clear justification for extending the school day.",Critical Analysis & Reasoning Skills,6,Social Sciences
70,Passage Title: Frost and Cummings,"At the forefront of the modernist poetry movement were T. S. Eliot, Wallace Stevens, and others, such as Ezra Pound, whose injunction, ""Make it new!"" characterized their artistic approach.  Hindered by the same internal conflict plaguing many intellectuals within the cultural upheavals of the time, these poets felt, on one level, that old linguistic certainties had evaporated and new forms must be embraced.  Yet many of them, particularly the later modernist poets, maintained a sentimental attachment to all that had been lost from the cultural certitude of earlier eras, their despondency reflected in much of their work.  At the periphery of the movement were E. E. Cummings and Robert Frost, who were named among modernist poets largely for the chronological classification of their poetry and not necessarily for their style or means of artistic production.
A self-titled ""poet"" and ""painter,"" Cummings was beyond his time in his efforts to innovate.  He would become well known for his erratic applications of punctuation and syntax, and, later, for his visual configurations of words.  Much of Cummings' writing also used idiosyncratic similes and metaphors.  This style, which later evolved to include symbolism and allegory, caused even some of the most progressive modernists to dismiss his work as eccentric, self-indulgent, and lacking depth.  However, as a quintessential dissenter, Cummings remained unaffected by the reactions of his contemporaries and focused more on creating a graphic effect with words.
To Cummings, the artist was not one who discerns or describes, but one who feels.  He often contrasted the ""doing"" of others—scientists in particular—to the ""being"" of the artist.  Compared to what he defined as the ""nonartist,"" Cummings held that the artist must be original, self-reliant, and free to live according to his or her own truth.  For Cummings, this meant disentanglement from any shackles of societal standards, ranging from the man-made notions of reality and reason to traditional literary conventions.
Contrarily, Frost believed that a true poet could—and should—achieve poetic excellence without resorting to what he described as the ""new ways of being new.""  Consequently, his poetry was composed of ordinary sounds that derived from conventional language.  Although many contemporary critics dismissed his work as overly simplistic, or ""too near the level of talk,"" Frost remained unperturbed by such views, holding that poetry sprang from the natural intonations of a person's voice and should remain true to traditional forms.
Frost claimed that ""a poem begins with delight, it inclines to the impulse, it assumes direction with the first line laid down.""  This conception of a poem led Frost to elucidate that, behind the scenes, the poet must always approach writing intimately and organically, allowing the words to originate seamlessly from ""a lump in the throat, a homesickness, a loneliness.""  Provoking both poet and reader to recollect an oft-distant truth lingering in some obscure region of the brain, this practice culminates in wisdom that may be profound or just a momentary reprieve from the world.
Uncompromising and not always popular, Frost and Cummings nevertheless mingled with the literary elite and developed long-term acquaintances with Ezra Pound, who especially championed Frost's literary career.  Each shared with other modernist poets an opposition to the scientific rationalism and commercialist vulgarity that had infiltrated Western culture after the Industrial Revolution.  Yet both poets also continued to exhibit a commitment to the role of nonconformist and favored aspects of the Romantic poetry movement—namely, the significance of the individual's experience and the rejection of societal scrutiny—over the tormenting self-doubt that had beset their peers.  Thus, even in an age defined by its pure and absolute originality, Frost and Cummings occupied unique positions within the modernist poetry movement.",Critical Analysis & Reasoning Skills,5,Humanities
71,"Passage Title: ""The Inkblots""","For almost 100 years now, the psychological evaluation known as the Rorschach Inkblot Test has engendered much controversy, including skepticism about its value, questions about its scoring, and, especially, criticism of its interpretive methods as too subjective.  Thus, the Rorschach test, which emerged from the same early twentieth-century zeitgeist that produced Einstein's physics, Freudian psychoanalysis, and abstract art, seems one of modernity's most misbegotten children.  Destined never to be completely accepted or discredited, the test remains a perennial outlier in its field.  Nevertheless, the inkblots' mystery and aesthetic appeal have caused them to be indelibly printed on our cultural fabric.
The now iconic inkblots were introduced to the world by Swiss psychiatrist Hermann Rorschach in his 1921 book Psychodiagnostics.  As both director of the Herisau Asylum in Switzerland and an amateur artist, Rorschach was uniquely positioned to wed the new practice of psychoanalysis to the budding phenomenon of abstract art.  For instance, reading Freud's work on dream symbolism prompted him to recall his childhood passion for a game based on inkblot art called Klecksographie.  He was also cognizant that in a recently published dissertation, his colleague Szymon Hens had used inkblots to try to probe the imagination of research subjects; moreover, a few years earlier, the French psychologist and father of intelligence testing Alfred Binet had used them to measure creativity.
Motivated by these developments, the Herisau director decided to revisit that childhood pastime that had awakened his curiosity about how visual information is processed.  In particular, he wondered why different people saw different things in the same image.  Traditionally, psychoanalysts had relied on language for insights; however, as biographer Damion Searls reports, Rorschach's theories would exemplify the principle that ""who we are is a matter less of what we say than of what we see.""  Indeed, through a process of perception termed pareidolia, the mind projects meaning onto images, detecting in them familiar objects or shapes.  Consequently, what a person sees in an image reveals more about that person than about the image itself.
Rorschach experimented with countless inkblots, eventually selecting ten—five black on white, two also featuring some red, and three pastel-colored—to use with research subjects.  For these perfectly symmetrical images—each of which he was said to have ""meticulously designed to be as ambiguous and 'conflicted' as possible""—the primary question was always ""What do you see?""  Rorschach was especially careful to note how much attention individuals paid to various components of each inkblot (such as form, color, and a sense of movement) and whether they concentrated on details or the whole image.  Having observed that his patients with schizophrenia gave distinctly different responses from the control group, Rorschach envisioned his experiment as a diagnostic tool for the disease.  Nevertheless, he resisted the notion that its results could be used to assess personality.  In fact, until his untimely death from a ruptured appendix in 1922, Rorschach referred to his project as an ""interpretive form experiment"" rather than a test.  Ironically, however, by the 1960s, the Rorschach Inkblot Test was known chiefly as a personality assessment and had become the most frequently administered projective personality test in the US.
Rorschach's test has survived nearly incessant scrutiny, including a 2013 comprehensive study of all Rorschach test data and repeated revisions to its scoring, yet doubts about its validity and reliability persist.  Much like the inkblots themselves—which tantalize us with the possibility of divulging the secrets of who we are and how we see the world—the test has (for better or worse) defied attempts to fix its meaning.  Thus, what has been called ""the twentieth century's most visionary synthesis of art and science"" stands tempered by harsh criticism.",Critical Analysis & Reasoning Skills,4,Social Sciences
72,Passage Title: Writer's Block,"For many who dare to wield the pen, writer's block is not just an occupational hazard but a symptom of a far deeper malaise.  In the early nineteenth century, poets with Romantic proclivities equated creativity with an unseen and uncontrollable force that, like a fickle wind, either infused the writer or passed him by.  As the twentieth century approached, Symbolist poets who suffered verbal paralysis tended to blame their silence on the inadequacies of language itself.  They struggled, says one researcher, with ""how to get past its vague, cliché-crammed character and arrive at the nature of experience.""  Whatever its cause, a blockage's outcome is unpredictable: in 1874, the precocious Arthur Rimbaud, then age 20, ceased versifying altogether, while fellow Symbolist poet Paul Valéry deserted the métier for two decades.
But perhaps no one has delineated the ubiquitous writer's malady with more pathos than Viennese poet and dramatist Hugo von Hofmannsthal in his 1902 Ein Brief (A Letter).  This short but dense fictional work takes the form of an early seventeenth-century missive addressed to English philosopher and scientist Sir Francis Bacon.  Its imaginary author, a perturbed, twenty-six-year-old poet named Lord Philip Chandos, bemoans two years elapsed without writing.  Nostalgically he ponders the prelapsarian condition of ""continuous intoxication"" he once enjoyed: ""I conceived the whole of existence as one great unit: the spiritual and physical worlds seemed to form no contrast…and in everything I felt the presence of Nature.""
From these lofty heights, the previously prolific writer descended by stages into what he calls ""this extreme of despondency and feebleness [that is] now the permanent condition of my inner self.""  First, he found himself unable to discuss philosophy or morality because ""the abstract terms of which the tongue must avail itself…crumbled in my mouth like moldy mushrooms.""  Next, a disturbingly heightened consciousness, together with a fragmented sense of self, made expressing opinions or judgments in ordinary conversation nearly impossible.  As Chandos explains, he began to see everything that such conversations entailed ""from an uncanny closeness.""  Soon he discovered that ""everything disintegrated into parts; no longer would anything let itself be encompassed by one idea.""  Gradually, all language seemed to betray the poet: ""Single words floated round me; they congealed into eyes which stared at me and into which I was forced to stare back—whirlpools which gave me vertigo, and, reeling incessantly, led into the void.""
Even as abstract ideas eluded the effete poet, so did spiritual thoughts.  As Chandos laments to his distinguished addressee, ""I have been leading an existence which I fear you can hardly imagine, so lacking in spirit and thought is its flow.""  Although terrifyingly deficient, this condition was still ""not utterly bereft of gay and stimulating moments.""  In such instances, Chandos observes, a familiar object ""can suddenly…assume for me a character so exalted…that words seem too poor to describe it.""  But these moments overwhelmed him, ultimately failing to mitigate his linguistic plight or redeem ""a life of barely believable vacuity.""  His thinking waxed ""feverish,"" seeming to move through ""a medium more immediate, more liquid, more flowing than words.""  Finally, the erstwhile writer admits that, barring the emergence of a different kind of language, he will never write another book.
Hofmannsthal's poignant epistle portended a crisis of language so common among writers in the first half of the twentieth century it led Austrian psychoanalyst Edmund Bergler to coin the term ""writer's block"" in 1947.  Ein Brief also epitomized Hofmannsthal's own estrangement from his craft as three years earlier the muse had forsaken this poetic prodigy.  However, for Hofmannsthal, unlike the fictional Chandos or the real-life Rimbaud, rehabilitation followed inhibition: he would eventually win renown as the chief librettist of opera composer Richard Strauss.",Critical Analysis & Reasoning Skills,4,Humanities
73,Passage Title: Aran Islands,"The Aran Islands, a trio of carboniferous limestone slabs, likely more than 350 million years old, are situated at the mouth of Galway Bay off the western coast of Ireland, on what in ages past must have seemed the very periphery of the civilized world.  For those who have dared to test the habitability of these rocky fragments, one certainty has always awaited: challenge.  Indeed, the presumed prototype of the Aran economy has been one of bare subsistence, a veritable experiment in self-sufficiency, demanding resourcefulness, endurance, and, above all, austerity.
Of the three isles, Inishmore, the largest and westernmost, whose length forms a 9-mile breakwater against the Atlantic, has perhaps also been the most economically diverse and certainly the most historically colorful.  The island hosted the first monastic communities of Ireland and withstood the blunt force of Viking raiders and, centuries later, Cromwellian soldiers.  Following these invasions, the population—and the economy along with it—managed to flourish despite a landscape whose barrenness offered an almost equal shortage of arable land and surface water.  In addition, Inishmore's prolonged growing season, the result of year-round moderate temperatures, afforded an exquisite irony: the flora and fauna it supported, a curious admixture of that which could be found in alpine, Mediterranean, and arctic climes, were wonders for the naturalist to behold but worthless to inhabitants seeking to sustain themselves on the land's bounty.  Nevertheless, the island has long been able to defy its hardscrabble character and has demonstrated a talent for economic self-reinvention, all the while retaining its image as pristine, primitive, and inhospitable.
By the early 19th century, the denizens of Inishmore had learned to effectively counterbalance the meagerness of their island's natural resources by swiftly focusing on new and evermore efficient ways to exploit them.  For example, the abundance of seaweed off its shores supplied a remedy for the scarcity of soil.  Once most of the boulders were cleared away, the sand could be layered with the seaweed, which, as it rotted, transformed the granular matter into fertile earth in which common crops such as barley, rye, oats, and potatoes have since been cultivated.  More land also meant more livestock could be raised, and the grass-fed calves of the island are still prized by mainlanders.  Seaweed could also be harvested by the ton and burnt in kilns to produce kelp, the sale of which on the mainland continues to be a thriving, if seasonal, industry, often more reliable than fishing.
This is not, however, to say that life on the Aran isles was ever easy; in fact, the lifestyle of the inhabitants demanded a delicate equilibrium, and in the face of economic marginality most households resorted to occupational plurality—to include any combination of fishing, agriculture, kelp-making, boat-making, fowling, textiles, and manual labor.  Yet, despite this ever-shifting quest for resources, life on Inishmore even in the 1800s was a far cry from that depicted in Robert J. Flaherty's self-styled documentary film of 1934, Man of Aran, which dramatized the islanders' daily existence as a never-ending and desperate struggle against the elements, particularly the constant apocalyptic threat of the sea.
Eighty years later, this image of Aran life endures, perpetuating a myth that has arguably become a commodity as real as the Man of Aran sweater sold on Inishmore but typically knit elsewhere in Ireland—a symbol of the island's dual character, of its commercial nature that so patently contradicts its celebrated identity as isolated and self-sufficient.  Still, the persistence of this image of remoteness, exposure, and inexorable hardship, however fabricated or embellished, has helped the island reinvent its economy as one heavily reliant on the outside—particularly on a tourist industry that brings in as many as 2,000 visitors a day by ferry—rather than self-sufficient.",Critical Analysis & Reasoning Skills,6,Social Sciences
74,Passage Title: Wagner's Leitmotif,"Judging the opera of his day to be in a desperate state of decline, the 19th-century German composer Wilhelm Richard Wagner—arguably the most controversial, if not the most titanic, figure in the history of classical music—set out to revitalize and revolutionize the genre.  In his 1851 book-length essay, Opera and Drama, Wagner adopted the concept of Gesamtkunstwerk, or ""total work of art,"" leading him to envision the ideal form of opera as a synthesis of the poetic, visual, musical, and dramatic arts.  Thus, rather than the opera's drama being subordinate to the music, as was typical in Wagner's time, the music would now be integrated with the drama.
Subscribing to this aesthetic theory of Gesamtkunstwerk, Wagner produced his most celebrated operatic achievement, the Ring of the Nibelung, comprising four music dramas: The Rhinegold, The Valkyrie, Siegfried, and Twilight of the Gods.  With the creation of this monumental work, commonly called the Ring cycle, Wagner also reinvented the musical language of opera.  In contrast to the practices of his predecessors and contemporaries, like Carl Maria von Weber and Giacomo Meyerbeer, Wagner blurred the lines between the traditional operatic components of recitative (musical dialogue) and aria (solo) and virtually eliminated large choral segments.  Additionally, he amplified the role of the orchestra.
However, the most influential Wagnerian innovation was the evolution and systematic application of what would later be known as the leitmotif, a recurring musical phrase or melody closely associated with a particular character, place, or object in the drama.  The composer referred to these repeating phrases as ""melodic moments,"" ""ground themes,"" or ""carriers of feeling,"" and subjected them to endless modulation (changes in musical key) and variation.  They could, as Hans von Wolzogen explains, be ""developed, interwoven, assimilated with each other, and multifariously transformed.""  Wagner also strategically employed them as ""motifs of memory"" that reminded the audience of previous scenes in the drama.  Thus, leitmotifs not only served as dramatic devices to signal the appearance of different characters but provided the internal musical logic and unifying mechanism for the entire operatic cycle.  Through liberal use of leitmotifs, Wagner was able to generate something Michael Halliwell describes as ""a dense, constantly evolving, and fully enclosed dramatic world"" whose musical complexity was staggering.
Unlike most opera composers, Wagner wrote not only the musical scores to his works but also the libretti (the lyrics and narrative), which he dubbed ""poems.""  The Ring cycle's libretti were based on the medieval epics of Norse and Germanic mythology, such as the Eddas, the Volsung, and the Nibelungenlied.  Furthermore, the composer found a structural prototype for his masterpiece in the ancient Greek festival of the god Dionysus, for which playwrights produced four related dramas to be performed on successive days.  Wagner's own magnum opus would debut over four nights in August 1876 at the Festspielhaus, a state-of-the-art opera house built in Bayreuth, Germany, at the composer's behest.
In a life and career marked by controversy and scandal, Wagner endured debt, poverty, broken marriages, and political exile.  Yet his accomplishments in opera surpassed that of any other composer, including his Italian counterpart Giuseppe Verdi.  In fact, few areas of European and world culture have escaped Wagner's influence.  Most notably, his Ring cycle combined story with visual elements and dramatic music to anticipate the not-yet-invented art of cinema.  Indeed, early film score composers were schooled in Wagnerian techniques; they understood the versatile and subtle power of the leitmotif to unify all aspects of the drama and influence the audience emotionally.  Nevertheless, moviegoers today would struggle to identify Wagner's innovative use of the leitmotif as the progenitor of musical film scores and, particularly, of the popular phenomenon of the ""theme song.""",Critical Analysis & Reasoning Skills,5,Humanities
75,Passage Title: The Panama Canal,"To sail from the east of North America to its western coast was not impossible in the nineteenth century, but sailors may well have wished it was.  The voyage required ships to brave the frigid waters between South America and Antarctica, whose treacherous depths claimed as many as fifteen thousand bodies.  This abode of ninety-foot waves and buffeting gales marked only the mid-point in a circuit of the Americas, and the men who survived that reckoning still faced several more weeks at sea.
Understandably, the desire for a shortcut was strong.  Previous exploration had revealed no naturally occurring waterway through Central America, but what if such a passage could be built?  These considerations were the genesis of what would become the Panama Canal.
The first to attempt such a feat were the French, under the leadership of Ferdinand de Lesseps.  Having formerly engineered the Suez Canal connecting the Mediterranean and Red Seas, Lesseps was a natural choice to head what was presumed to be a similar endeavor.  With the enthusiastic support of his country and the permission of the Colombian government (which controlled Panama at that time), Lesseps and his team began work in 1880, 11 years after the Suez Canal had been completed.
However, the mountainous jungles and swamps of Panama proved a pernicious contrast to the flat sands of Egypt.  Equipment toppled down hillsides or was swallowed into the mud, while frequent rains washed excavated dirt mounds back into the pits from which they had been extracted.  More devastating was the human toll.  Living conditions were squalid and miserable, as workers were ravaged by rampant yellow fever and malaria.  Over twenty thousand lives were lost, with no end in sight to the scourge of disease or to the faltering project.  Ultimately, the construction company went bankrupt, some of its members were imprisoned, and the elderly Lesseps died.
With the failure of the French, the stage was set for the United States to try its hand.  Some American engineers recommended that the canal should be built through Nicaragua instead of Panama; although the former country was much wider, its terrain was significantly more hospitable.  An additional drawback to choosing Panama was that the Colombian government would not agree to let the U.S. control the canal once it was built.  Nonetheless, the U.S. decided to build in Panama.  Shortly thereafter, Panamanian dissident Manuel Amador led a rebellion against the Colombian government, while a U.S. warship happened to be sailing off the coast.  Victory was swift; as writer Janet Pascal notes: ""The entire revolution lasted only a few hours.  Only one shopkeeper and a donkey were killed.""  Afterward, newly independent Panama granted construction rights and control of the future canal to the United States.
Unfortunately, the U.S. claimed the French's abandoned equipment and facilities while failing to address their logistical difficulties and incidence of disease.  After a year of futility, wiser minds devised a new course.  Acting on the uncommon but accurate belief that mosquitoes spread malaria and yellow fever, Americans exterminated those pests and thus the maladies they caused.  In conjunction with new living facilities and improved transportation, this action provided the ability to sustain a healthy workforce and build the canal based on proper planning.
The project spanned the American presidencies of Theodore Roosevelt, William Howard Taft, and Woodrow Wilson, reaching completion in 1914.  Traversing the breadth of Panama and incorporating a series of locks to raise and lower ships to different elevations, the Panama Canal finally made it possible to sail from the Atlantic Ocean to the Pacific without circumnavigating South America.  It remains in use to this day, more than a century after its construction.",Critical Analysis & Reasoning Skills,4,Social Sciences
76,Passage Title: Psychic Ethics,"What is a lie?  Most would say that a lie is simply a statement that is false, or that the act of lying is to tell someone something false.  Despite the popularity of that definition, it is not really accurate, as telling someone a falsehood is not in fact the same thing as lying.  Even more strangely, in some cases a lie is true—not in a metaphorical sense, but literally.
To see why, consider a thought experiment.  Let's say that your co-worker, Mark, tells you that there is a meeting in the conference room at 11:00 AM.  Around 10:55 AM you walk in, but no one else is present.  You wait five minutes, ten minutes…by 11:15 AM, still no one else has arrived.  Did your co-worker lie to you?  You already know his statement was false (because there was no meeting), but that knowledge is insufficient.  To determine whether Mark was lying, what you need to know is whether he believed there was going to be a meeting.  If he did, then he didn't lie to you; he was just mistaken.  Essentially, if someone tries to convince you of something he thinks is false, then he is lying—regardless of whether his statement really is false or, unbeknownst to him, happens to be true.  But if he says something he believes is true but is actually false, then he is not lying.  Thus, what makes a statement a lie is not whether it is false, but whether it is meant to deceive.
A related and unusual example comes from real life.  Sarah was hired by a psychic hotline to tell the fortunes of those who called in.  When speaking to a customer she would use a tarot deck in the traditional way, then describe the cards' perceived implications for the person's future.  However, her employer did not want the psychics to provide ""genuine"" psychic readings, but instead to address customers using approved scripts based on corporate assumptions of what callers wanted to hear.  Because Sarah was giving the customers ""real"" psychic readings instead of the company's pre-fabricated ones, her employment was terminated.
The intriguing part of this case is the odd juxtaposition of relevant ethical factors.  In one sense, what Sarah was doing is inherently fraudulent.  Fortune-telling does not work; thus, by conveying to customers that she could tell them the future, Sarah was saying something that is manifestly untrue and purporting to sell something that she could not possibly provide.  Those seeking to purchase knowledge about their future could not—except perhaps by lucky coincidence—receive what they hoped to buy.  In another sense, however, Sarah's actions were not fraudulent at all, because she herself believed in the power of tarot; one can be factually wrong without being morally wrong.  To the extent that Sarah sincerely tried to give her callers what she thought were legitimate psychic readings, she displayed honesty and integrity.
Certainly, she acted in good faith while her employer clearly did not.  The hotline claimed to sell accurate predictions from ""real psychics"" while knowingly providing nothing of the sort.  Obviously, there is no such thing as a real psychic, and the predictions are nothing but superstition.  But even a disclaimer like ""for entertainment purposes only"" does not justify a business model that takes advantage of a customer's false belief or misrepresents what it supposedly offers.  Because Sarah honestly tried to give genuine psychic readings, her actions were ethical, whereas her employer's actions were unethical.  Her product may have been fake, but her honesty was real.",Critical Analysis & Reasoning Skills,6,Humanities
77,Passage Title: The Mirror Stage,"Certain schools of thought that arose in the mid- to late 20th century and have since been labeled ""poststructuralist"" are well known for their propensity to ""problematize,"" ""destabilize,"" or ""radicalize"" the concepts of previous thinkers.  For example, the controversial French poststructuralist and psychoanalyst Jacques Lacan sought to reenvision the concept, first introduced by Freud in the early 1900s, of the ego as the autonomous agent of the psyche.  Hence in his seminal 1949 essay, ""The Mirror Stage as Formative of the 'I' Function as Revealed in the Psychoanalytic Experience,"" Lacan variously describes the formation and consequent function of the ego as a schismatic event of misrecognition, an epiphenomenon of narcissism, and a process of dialectical tension.  That process sets up an interchange between identification and alienation, generating confusion between being a subject as opposed to an object, between self and other, between what is ""me"" and ""not me.""
The drama of the ego's emergence in what Lacan has designated ""the Mirror Stage"" explains its contrary nature.  Somewhere between 6 and 18 months of age, an infant will likely encounter his (or her) own reflection in a mirror.  At this phase of development, the child lacks any sense of a unified self.  However, when encouraged by an adult, he is able to recognize his reflection in the glass and, at the same time, comes to understand that the specular image looking back at him is, in fact, not himself.  In other words, the image provides the infant with the first glimpse of himself as an object, as ""other.""  According to Lacan, the initial fascination and pleasure the child experiences upon self-recognition dissolves into confusion at being unable to distinguish between what is and what is not himself.  Therefore, such recognition amounts to a form of misrecognition, the French term for which, méconnaissance, implies an essential misunderstanding or lack of knowledge.
Furthermore, while the reflection in the mirror appears coherent, coordinated, and whole, the child himself continues to perceive his body as uncoordinated and fragmentary, resulting in a profound sense of incongruity and disharmony.  By exuding mastery and harmony, the mirror image functions as a gestalt: it holds up an ""Ideal-I"" or future self that promises completeness and perfection—a condition, Lacan believes, that is ultimately unattainable, though we will likely spend the rest of our lives in pursuit of it.  Overall, the infant's reaction to the image or Ideal-I is narcissistic; he is attracted to it as a model but feels aggression toward it as a rival.
Alienation, lack, absence, and conflict are thus constitutive of the Lacanian ego.  The child's image—and, hence, the emerging ego—marks an ontological gap that turns into a repository for the projections and desires, whether conscious or unconscious, of parents and others.  Consequently, this image, as Adrian Johnston notes, is ""always already overwritten"" with words that are not the child's own.  In Johnston's view, the ego could be more properly described as ""a coagulation of inter- and trans-subjective alien influences.""  Rather than a ""fluid and autonomous subject,"" it becomes a rigid and ""heteronomous"" entity.  Derived from the desires of others, the ego is, in Lacanian terms, ""extimate,"" or internally exterior.  Both alien and alienating, it is an irreducible contradiction.
This ego paradigm diverges from the more familiar and cohesive Freudian notion of the ego as the sovereign organizer of the personality, the rational mediator between internal drives and social pressures that in the interests of self-preservation will also resort to deceptions and defense mechanisms.  From a Lacanian perspective, the ego is intrinsically and irreparably divided; it chiefly serves to support the fictional construct we identify—or rather, misrecognize—as the ""self.""",Critical Analysis & Reasoning Skills,4,Social Sciences
78,"Passage Title: ""Resistance to Reduction"": William James and The Varieties of Religious Experience","Well known for his radical empiricism, pluralism, and pragmatism, William James fervently defended the primacy of raw experience against the reductive theorizing of philosophy.  Commonly considered the founder of American psychology, the brother of novelist Henry James favored what might be deemed the ""thickness"" of concrete reality over the thinness of abstraction.  Hence, he subscribed to a principle he called ""total acceptance"" that welcomed ""the contents of every kind of experience.""  In yearning to restore to reality ""all its vagueness and unkemptness,"" he contended that nothing should be subordinated to theories or facilely explained away.  It was with this mindset that James agreed to deliver the Gifford Lectures on religion—later published as The Varieties of Religious Experience: A Study in Human Nature—at the University of Edinburgh in 1901.
From the start, James conceived of his study as a psychological inquiry, declaring that ""religious feelings and religious impulses must be its subject.""  Consequently, he ""propose[d] to ignore the institutional branch"" of religion, confining himself as far as possible to ""personal religion pure and simple.""  Furthermore, in this unique exploration, James primarily endeavored to prove that personal experience—that is, the inner religious life of the individual—was the scaffolding upon which the outer religious life of the world was built.  In other words, the power of organized religion was derived from direct, personal communications from the divine.  Therefore, rather than analyze the fixed patterns found in theologies or doctrines, James decided to examine the personal experiences from which such patterns had originally been instituted.  Having discovered accounts of such experiences in literature and biography, he sorted them into broad categories.  Amongst these classifications, he identified the mystical state of feeling at one with the divine as the central experience, referring to it as ""the mother-sea and fountain-head of all religion.""
Nevertheless, James—whose ancestors had been strict Calvinists, whose father had been an eccentric and erratic follower of obscure, utopian sects, and who had himself delved into spiritualism—denied there was any such thing as ""the religious sentiment,"" elementary religious emotion, formula, or essence.  Such oversimplifications and summations were to him suspect, if not dangerous.  As historian Jacques Barzun observes, ""this Jamesian resistance to reduction"" caused the radical empiricist to champion personal religious experience.  However, James did admit that, seduced by the commonalities in the narratives, he was at times tempted to develop a science of religion.  Yet, where other thinkers equated the ultimate nature of reality with monism, James insisted on a plurality of individual realities that overlapped but never merged to form a single larger reality.  Thus, rather than marshal a sustained argument in The Varieties, he carefully compiled personal testimonies of divine encounters, allowing a plethora of different voices to speak on the nature of such experience.
In addition to emphasizing the empirical and pluralistic aspects of religious experience, James took a pragmatic approach that evaluated such encounters according to their consequences.  In particular, he looked at the beliefs they engendered and how these were manifested in the life of the belief-holder.  He determined that the mystical state was the most profoundly transformative of religious experiences.  To the myriad detractors, doubters, and reductionists who would dismiss such states as forms of hysteria or side effects of superstition, he responded that ""the most important way to discern the real from the unreal"" and ""to distinguish pathology from truly divine states"" was not to expose their ""root causes"" but to assess the ""fruits"" they yielded.
Hence, by wielding what Eugene Taylor terms a ""final tripartite metaphysics of radical empiricism, pluralism, and pragmatism,"" James rescued religious feelings and impulses from reductive thinking and so defended religious experience against philosophical theorizing.",Critical Analysis & Reasoning Skills,4,Social Sciences
79,Passage Title: Blackmail: Just a Dirty Business?,"Both blackmail and extortion are crimes of coercion: one party attempts to gain by threatening action that would be injurious to another, unless a specified demand is met (usually for financial payment).  However, with extortion the threatened action itself is also illegal.  For example, suggesting that one will vandalize property unless the owner pays a fee would be extortion, because it is unlawful to damage what belongs to another.  On the other hand, promising to expose a secret unless the subject pays would generally be blackmail, because in most cases there is nothing criminal about sharing factual information.  But in light of this distinction, an interesting question arises: Why is blackmail a crime?
This question is more difficult than it may appear, and legal theorists, philosophers, and even economists have attempted to provide answers.  The ""puzzle"" or ""paradox"" of blackmail is to explain the following: How does offering to choose an otherwise legal option (eg, keeping silent), in exchange for compensation, make that choice a criminal act?  Granted, blackmail may strike us as dirty and underhanded—it feels like something that should be illegal.  For obvious reasons, however, it would be preferable if we could specify a firm basis for its criminality.
To this end, a comparison to the corporate world is helpful.  Certain types of financial coercion are a normal, expected part of business transactions; for instance, a retailer might threaten to purchase goods from a different wholesaler unless the current one lowers the price per unit.  This sort of tactic is not only legitimate but, in an overall sense, good for all parties.  If the threat to take business elsewhere had not been announced, the original wholesaler would simply lose the business.  Instead, the wholesaler can choose whether to change its terms in order to preserve the sale.
Similarly, a few scholars have argued that blackmail should be legal because it actually benefits its target.  Rather than having a secret revealed against his wishes, the target is afforded the opportunity to choose what to him is a more favorable outcome—merely paying a fee.  Thus, like the retailer above, the blackmailer has increased the options available to the ""victim,"" placing him in a better situation than he would have been otherwise.
Still, the majority position is to reject these scholars' view.  One common suggestion is that coercing someone through blackmail is illegitimate because it involves appropriating what belongs to other people.  If Adam threatens to reveal Bob's secret to Charlie, only Bob and Charlie have a natural stake in whether the information is exposed.  Adam is a third party to the situation; thus his connection to its outcome is artificial.  A variation of this view uses more rights-based language.  If a relationship exists between Bob and Charlie, then they may reasonably have certain ""claims"" to information regarding each other; by contrast, Adam's involvement is purely parasitic on that relationship.
However, the most compelling argument relates to the ongoing effects of blackmail.  Law professor Richard Epstein stresses that the ""business model"" of successful blackmail is essentially participation in a kind of fraud—not against the blackmailed party, but against the party from whom information is concealed.  If Charlie's well-being depends on whether he knows particular facts about Bob, then Bob's attempt to conceal that information is injurious to Charlie.  As such, Adam can only blackmail Bob by perpetuating Bob's deceit.  Moreover, the presence or potential of blackmail often incentivizes additional harmful or criminal activity by the perpetrator, the target, or both.  Thus, blackmail's effects are not only acute but systemic and pervasive.  As Epstein remarks, ""Blackmail is made a crime not only because of what it is, but because of what it necessarily leads to.""",Critical Analysis & Reasoning Skills,4,Social Sciences
80,Passage Title: White House Reconstruction,"The destruction of the White House was not the fault of its original builders.  When James Hoban designed a presidential palace to reflect George Washington's vision for a residence in 1792, he upheld the best architectural standards of his day.  By the time of the 1948–1952 renovation, however, there were plenty of people to blame for the White House's condition.  A combination of poor planning, carelessness, and neglect throughout the intervening years caused a catastrophic deterioration.
Robert Klara recounts a history of changes to the White House that gradually compromised its structural integrity, beginning as far back as 1833 when Andrew Jackson had pipes installed for running water.  Subsequent presidents added modern conveniences in pace with technological developments: gas lighting for James Polk, a telephone system for Rutherford Hayes, and air conditioning for Chester Arthur, among others.  None of these advancements could have been anticipated by Hoban, and they had the dual effect of increasing the load borne by the house while decreasing its ability to bear that burden.  The builders responsible for the many additions performed ""careless and foolish maneuvers that a first-year architecture student would never have made,"" such as installing doorways in load-bearing walls or destroying the keystones of a brick arch.  Deep cuts were carved into crucial support beams to make room for pipes, wiring, and air ducts, weakening the beams and concentrating weight that would otherwise have been more evenly distributed.
By the turn of the twentieth century, the structural failings of the White House had become difficult to ignore.  The building was likely beyond saving at that point, and President Roosevelt didn't help matters by allowing architect Charles McKim only four months to make repairs.  McKim reinforced the second floor as well as he could within the time allotted, but his efforts only delayed the inevitable.  In 1925, Calvin Coolidge scoffed at reports that the White House roof was unsafe until ""[a] chunk of it broke off and bonked President Coolidge in the head.""  Poetic justice notwithstanding, the repairs to which he only then begrudgingly agreed had a disastrous side effect: the weight of the roof was shifted away from the stronger outer walls to the weaker inner ones, causing enormous structural strain.
The need to reconstruct the mansion reached a breaking point during the Harry Truman administration.  Walls continued to sink and pull away from ceilings; heavy chandeliers swung dangerously from movement on the floor above them; the entire East Room required scaffolding to prevent the collapse of its ceiling.  More dramatic incidents involved daughter Margaret Truman's piano.  Not only did the floor of her sitting room begin to sway up and down while she was playing a duet with a friend, but no steps were taken to move the piano away from the weak floor afterward.  Predictably, White House residents and staff were later horrified when one of the piano's legs crashed through, puncturing the overtaxed wood and sending debris cascading into the room below.  As for Margaret, Klara relates how she called the Steinway company to ask for ""a man who could come over to rescue a piano.""
That event was the crescendo in a long string of architectural warnings.  Ultimately, there was little choice but to tear down and rebuild the White House.  The exterior façade was preserved, as were a few of the interior's furnishings and decorations.  Still, the extensive transformation left many feeling that the White House was no longer the same.",Critical Analysis & Reasoning Skills,5,Humanities
81,Passage Title: Boethius and Foreknowledge,"The philosophical Dilemma of Freedom and Foreknowledge concerns the apparent incompatibility between human free will and God's knowledge of the future.  The dilemma proceeds as follows: Free will requires the ability to choose from among alternative possible actions.  But if God knows in advance what a person will do, then there are no possible alternatives, only a single guaranteed outcome conforming to God's prior knowledge.  Consequently, no act is performed freely but instead occurs of necessity.  Hence, the dilemma seems to require that one abandon either belief in free will or the belief that God possesses knowledge of the future.
Various attempts have been made to resolve this dilemma, the most famous being the solution offered by the Roman consul Boethius.  A gifted scholar, Boethius initially enjoyed the favor of the barbarian patrician Theodoric, who ruled the western half of the Roman Empire in the early sixth century CE.  However, Boethius fell under suspicion of sympathy to the Eastern empire and disloyalty to Theodoric, leading ultimately to his execution in 524 or 525.  It was during the imprisonment preceding this fate that he wrote The Consolation of Philosophy, so named because it personifies Philosophy as a comforting spirit who engages the reader in conversation.  Boethius receives her instruction, attaining peace amidst his suffering through the wisdom she imparts.
The spirit's solution to the dilemma rests partially on rejecting the notion that God truly has foreknowledge as humans understand it.  As described in Book V of Boethius' work, human existence ""still lacks the future, while already having lost the past.""  Hence, the concept of foreknowledge arises from humanity's experience of events as a temporal succession.  But for God, who exists eternally, events are not temporal.  Past, present, and future are to God's mind what the impressions of a single moment are to human senses; God grasps the entirety of time as an immediate perception.  Therefore, Philosophy concludes that when considering God's understanding of what we would call future events, ""it will be more correct to think of it not as a kind of foreknowledge of the future, but as the knowledge of a never ending presence.""
This conception of God's eternality places divine knowledge of the ""future"" on par with human knowledge of the present.  That equivalence is important because…it would seem to eliminate the grounds for thinking that God's knowledge of future actions precludes their being freely chosen.  For example, suppose that one looks out a window and observes another person walking by.  The pedestrian's act of walking was not necessitated by the fact of being observed; rather, her choice to walk is what makes the observation possible.  If God's knowledge of future events is akin to such an observation, then it too confers no necessity on human actions.  It is only our experience of time which gives rise to the illusion that such knowledge interferes with free will.  Purportedly, then, God's timelessness resolves the Freedom and Foreknowledge Dilemma.
Although there is much to admire in Boethius' reasoning, some philosophers note that a similar dilemma can be generated without referring to God.  On their view of the problem, God is essentially irrelevant because the threat to free will does not come from foreknowledge itself; it comes from the possibility that statements about future actions are true even before those actions occur.  Thus, the Freedom and Foreknowledge Dilemma would be subsumed under a broader Dilemma of Freedom and Future Truths.  Unfortunately, bounded in time as we are, that issue must remain beyond our present discussion….
Boethius' death was brutal and, from our perspective, probably undeserved.  But perhaps in his final moments he found consolation in the thought that his fate had not been his fate.",Critical Analysis & Reasoning Skills,4,Humanities
82,Passage Title: Sumptuary Laws,"In America's early democratic republic, the superficial signifiers of social status that had long distinguished the aristocracy in Europe were no longer supposed to apply.  However, despite political theory and an emphasis on rhetoric that implied democratic equality, many among the late eighteenth-century social elite flaunted their status and at the same time condemned any pretense by the lower classes to a higher social position.
For the American ""nobility,"" the immoderation that had marked public life since before the Revolution assumed epic proportions in the later decades of the century.  From the viewpoint of society's top tier, social pretense, revealed through presumptive choices of costume or coiffure, was becoming far too common.  Sumptuary legislation—largely a holdover from the colonial era—was accordingly reinstituted in some states as a means to regulate behavior and a remedy for inappropriate displays of extravagance.  Indeed, libertine attitudes so dominated the population that George Mason IV, a congressional representative from the Commonwealth of Virginia, proposed that the federal government impose sumptuary legislation on all strata of society, including the affluent who would have traditionally been excluded from such laws.  However, the motion did not pass, with such social trends proving too deeply rooted.  Nevertheless, Mason was hardly alone in his alarm at the specter of mounting excess.
The more established members of the republican gentility had long freely indulged in decadent consumption without attracting much attention.  Nevertheless, most of these older elites criticized their own younger generation, whose coming-of-age was characterized by more overt disregard for the Puritan austerity advocated by their forebears.  The self-gratifying taste for grande cuisine and propensities for haute couture of the upper-class youth were perceived as decidedly improper conduct by their elders.  Moreover, as this ostentatious comportment worsened, it influenced those from the inferior echelons of society to adopt similar displays.
The presumptuous behavior mimicked by the lower classes led to an uproar by those in positions of power, who swiftly moved to modify societal conduct.  In particular, dissonance swelled in Massachusetts, where, within Boston's most select circles, a palpable anxiety over the maintenance of social hierarchies and their rightful exterior signifiers arose.  For the elite few who made up America's haute monde, any posturing of manner or appropriation of dress disrupted the customary rigidity of the social hierarchy, which, to their dismay, had now become increasingly fluid.  There was a communal uneasiness, which grew into national dread, that artful impostors might slip through the boundaries of the social strata undetected.  Yet, vexingly, counterfeits were becoming more difficult to recognize.  Consequently, in order to restrict impersonation, those of questionable lineage faced the threat of ostracization for an act as simple as sitting for a portrait.
At the same time, a movement materialized that had Boston as its epicenter.  The new paradigm mandated that eighteenth-century gentlemen as well as ladies project positive, even edifying, public images, as their privileged position behooved.  Therefore, a distinctly American variety of noblesse oblige, though not without its detractors, spread throughout the new democratic republic.  In metropolitan centers across the new nation, the attributes of honor, modesty, and public virtue assumed increasing importance among the ""aristocracy.""  Keeping up appearances—that were oftentimes not all they seemed—became paramount in order to set examples of permissible public behavior for the benefit of those of humble birth or who were less affluent.  Meanwhile, the bulk of the American population, consisting of those with little wealth or status, did their best not to emulate their ""role models.""",Critical Analysis & Reasoning Skills,5,Social Sciences
83,"Passage Title: Kant, Kantianism, and Traditional Political Theory","Although the primary canon in the history of political thought largely ignores the views of Immanuel Kant, he nevertheless was as important a political thinker as Rousseau, Locke, or even Aristotle.  Working in the context of the Prussian Enlightenment, Kant championed the idea that the only innate right of human beings is freedom.  Fittingly, the centerpiece of his political theory is that freedom can be the only basis for a just political state.
A political theory that advances freedom as its core tenet may appear inherently problematic.  The very existence of a political state would seem to require limitations on the freedoms of those within it.  However, this presents no serious problem for Kant's theory.  The Kantian notion of political freedom—which stands in stark contrast to the notion of transcendental freedom widely associated with Kant's nonpolitical writings—is restricted to the external relations between human persons.  In its political sense, freedom refers to the innate right of every person to independence from being constrained by another's choice.  In other words, every person has the innate right to make choices freely but only so long as these choices do not interfere with those of others.  Laws that do not allow for political freedom are forbidden in Kant's view because within a just civil condition, the promulgation of a law requires that an entire people agree to be subjected to it.
There are more substantive problems facing Kant's political theory.  Most notable among these concerns is Kant's promotion of welfare legislation.  As he says, ""The government is authorized to constrain the wealthy to provide the means of sustenance to those who are unable to provide for even their most necessary and basic needs.""  Given Kant's notion of political freedom, to enact such legislation would be catastrophic.  Welfare legislation violates Kant's own criterion regarding laws promulgated within a just civil condition: Governmental interference of this kind is a prime example of legislation on which an entire people will be divided.
This was an egregious error on Kant's part.  At the same time, proponents of a Kantian conception of the state need neither embrace this result nor dismiss the possibility of consistently incorporating welfare legislation into their preferred brand of political theory.  Although Kant did not explicitly advocate it himself, a Kantian may hold that public provision of welfare is necessary for the state to fulfill its primary function—namely, protecting the liberties of its citizens.  For instance, if failing to adopt welfare legislation would threaten the continued existence, security, or stability of the state, then Kantians may justify such legislation on the grounds that it serves an instrumentally necessary role in upholding the fundamental aims and principles of Kantianism itself.  It is interesting to note that followers of Kant will likely have independent reasons to accept something along these lines.  Kant himself held that rational entities—such as persons, corporations, and states—have a duty of self-preservation, and thus are obligated to take measures to ensure their continued existence, security, and stability.
There are still other issues surrounding a Kantian theory of the state.  These will need to be dealt with separately, but they should not go overlooked.  Breaking down the barriers that leave Kant outside the primary canon of political theory is a laudable task, which will take much effort to accomplish.",Critical Analysis & Reasoning Skills,4,Social Sciences
