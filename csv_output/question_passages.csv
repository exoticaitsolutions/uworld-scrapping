id,title,content,subject,question_count,topic
1,Passage Title: Wagner's Leitmotif,"Judging the opera of his day to be in a desperate state of decline, the 19th-century German composer Wilhelm Richard Wagner—arguably the most controversial, if not the most titanic, figure in the history of classical music—set out to revitalize and revolutionize the genre.  In his 1851 book-length essay, Opera and Drama, Wagner adopted the concept of Gesamtkunstwerk, or ""total work of art,"" leading him to envision the ideal form of opera as a synthesis of the poetic, visual, musical, and dramatic arts.  Thus, rather than the opera's drama being subordinate to the music, as was typical in Wagner's time, the music would now be integrated with the drama.
Subscribing to this aesthetic theory of Gesamtkunstwerk, Wagner produced his most celebrated operatic achievement, the Ring of the Nibelung, comprising four music dramas: The Rhinegold, The Valkyrie, Siegfried, and Twilight of the Gods.  With the creation of this monumental work, commonly called the Ring cycle, Wagner also reinvented the musical language of opera.  In contrast to the practices of his predecessors and contemporaries, like Carl Maria von Weber and Giacomo Meyerbeer, Wagner blurred the lines between the traditional operatic components of recitative (musical dialogue) and aria (solo) and virtually eliminated large choral segments.  Additionally, he amplified the role of the orchestra.
However, the most influential Wagnerian innovation was the evolution and systematic application of what would later be known as the leitmotif, a recurring musical phrase or melody closely associated with a particular character, place, or object in the drama.  The composer referred to these repeating phrases as ""melodic moments,"" ""ground themes,"" or ""carriers of feeling,"" and subjected them to endless modulation (changes in musical key) and variation.  They could, as Hans von Wolzogen explains, be ""developed, interwoven, assimilated with each other, and multifariously transformed.""  Wagner also strategically employed them as ""motifs of memory"" that reminded the audience of previous scenes in the drama.  Thus, leitmotifs not only served as dramatic devices to signal the appearance of different characters but provided the internal musical logic and unifying mechanism for the entire operatic cycle.  Through liberal use of leitmotifs, Wagner was able to generate something Michael Halliwell describes as ""a dense, constantly evolving, and fully enclosed dramatic world"" whose musical complexity was staggering.
Unlike most opera composers, Wagner wrote not only the musical scores to his works but also the libretti (the lyrics and narrative), which he dubbed ""poems.""  The Ring cycle's libretti were based on the medieval epics of Norse and Germanic mythology, such as the Eddas, the Volsung, and the Nibelungenlied.  Furthermore, the composer found a structural prototype for his masterpiece in the ancient Greek festival of the god Dionysus, for which playwrights produced four related dramas to be performed on successive days.  Wagner's own magnum opus would debut over four nights in August 1876 at the Festspielhaus, a state-of-the-art opera house built in Bayreuth, Germany, at the composer's behest.
In a life and career marked by controversy and scandal, Wagner endured debt, poverty, broken marriages, and political exile.  Yet his accomplishments in opera surpassed that of any other composer, including his Italian counterpart Giuseppe Verdi.  In fact, few areas of European and world culture have escaped Wagner's influence.  Most notably, his Ring cycle combined story with visual elements and dramatic music to anticipate the not-yet-invented art of cinema.  Indeed, early film score composers were schooled in Wagnerian techniques; they understood the versatile and subtle power of the leitmotif to unify all aspects of the drama and influence the audience emotionally.  Nevertheless, moviegoers today would struggle to identify Wagner's innovative use of the leitmotif as the progenitor of musical film scores and, particularly, of the popular phenomenon of the ""theme song.""",Critical Analysis & Reasoning Skills,5,Humanities
2,Passage Title: Wagner's Leitmotif,"Judging the opera of his day to be in a desperate state of decline, the 19th-century German composer Wilhelm Richard Wagner—arguably the most controversial, if not the most titanic, figure in the history of classical music—set out to revitalize and revolutionize the genre.  In his 1851 book-length essay, Opera and Drama, Wagner adopted the concept of Gesamtkunstwerk, or ""total work of art,"" leading him to envision the ideal form of opera as a synthesis of the poetic, visual, musical, and dramatic arts.  Thus, rather than the opera's drama being subordinate to the music, as was typical in Wagner's time, the music would now be integrated with the drama.
Subscribing to this aesthetic theory of Gesamtkunstwerk, Wagner produced his most celebrated operatic achievement, the Ring of the Nibelung, comprising four music dramas: The Rhinegold, The Valkyrie, Siegfried, and Twilight of the Gods.  With the creation of this monumental work, commonly called the Ring cycle, Wagner also reinvented the musical language of opera.  In contrast to the practices of his predecessors and contemporaries, like Carl Maria von Weber and Giacomo Meyerbeer, Wagner blurred the lines between the traditional operatic components of recitative (musical dialogue) and aria (solo) and virtually eliminated large choral segments.  Additionally, he amplified the role of the orchestra.
However, the most influential Wagnerian innovation was the evolution and systematic application of what would later be known as the leitmotif, a recurring musical phrase or melody closely associated with a particular character, place, or object in the drama.  The composer referred to these repeating phrases as ""melodic moments,"" ""ground themes,"" or ""carriers of feeling,"" and subjected them to endless modulation (changes in musical key) and variation.  They could, as Hans von Wolzogen explains, be ""developed, interwoven, assimilated with each other, and multifariously transformed.""  Wagner also strategically employed them as ""motifs of memory"" that reminded the audience of previous scenes in the drama.  Thus, leitmotifs not only served as dramatic devices to signal the appearance of different characters but provided the internal musical logic and unifying mechanism for the entire operatic cycle.  Through liberal use of leitmotifs, Wagner was able to generate something Michael Halliwell describes as ""a dense, constantly evolving, and fully enclosed dramatic world"" whose musical complexity was staggering.
Unlike most opera composers, Wagner wrote not only the musical scores to his works but also the libretti (the lyrics and narrative), which he dubbed ""poems.""  The Ring cycle's libretti were based on the medieval epics of Norse and Germanic mythology, such as the Eddas, the Volsung, and the Nibelungenlied.  Furthermore, the composer found a structural prototype for his masterpiece in the ancient Greek festival of the god Dionysus, for which playwrights produced four related dramas to be performed on successive days.  Wagner's own magnum opus would debut over four nights in August 1876 at the Festspielhaus, a state-of-the-art opera house built in Bayreuth, Germany, at the composer's behest.
In a life and career marked by controversy and scandal, Wagner endured debt, poverty, broken marriages, and political exile.  Yet his accomplishments in opera surpassed that of any other composer, including his Italian counterpart Giuseppe Verdi.  In fact, few areas of European and world culture have escaped Wagner's influence.  Most notably, his Ring cycle combined story with visual elements and dramatic music to anticipate the not-yet-invented art of cinema.  Indeed, early film score composers were schooled in Wagnerian techniques; they understood the versatile and subtle power of the leitmotif to unify all aspects of the drama and influence the audience emotionally.  Nevertheless, moviegoers today would struggle to identify Wagner's innovative use of the leitmotif as the progenitor of musical film scores and, particularly, of the popular phenomenon of the ""theme song.""",Critical Analysis & Reasoning Skills,5,Humanities
3,Passage Title: Wagner's Leitmotif,"Judging the opera of his day to be in a desperate state of decline, the 19th-century German composer Wilhelm Richard Wagner—arguably the most controversial, if not the most titanic, figure in the history of classical music—set out to revitalize and revolutionize the genre.  In his 1851 book-length essay, Opera and Drama, Wagner adopted the concept of Gesamtkunstwerk, or ""total work of art,"" leading him to envision the ideal form of opera as a synthesis of the poetic, visual, musical, and dramatic arts.  Thus, rather than the opera's drama being subordinate to the music, as was typical in Wagner's time, the music would now be integrated with the drama.
Subscribing to this aesthetic theory of Gesamtkunstwerk, Wagner produced his most celebrated operatic achievement, the Ring of the Nibelung, comprising four music dramas: The Rhinegold, The Valkyrie, Siegfried, and Twilight of the Gods.  With the creation of this monumental work, commonly called the Ring cycle, Wagner also reinvented the musical language of opera.  In contrast to the practices of his predecessors and contemporaries, like Carl Maria von Weber and Giacomo Meyerbeer, Wagner blurred the lines between the traditional operatic components of recitative (musical dialogue) and aria (solo) and virtually eliminated large choral segments.  Additionally, he amplified the role of the orchestra.
However, the most influential Wagnerian innovation was the evolution and systematic application of what would later be known as the leitmotif, a recurring musical phrase or melody closely associated with a particular character, place, or object in the drama.  The composer referred to these repeating phrases as ""melodic moments,"" ""ground themes,"" or ""carriers of feeling,"" and subjected them to endless modulation (changes in musical key) and variation.  They could, as Hans von Wolzogen explains, be ""developed, interwoven, assimilated with each other, and multifariously transformed.""  Wagner also strategically employed them as ""motifs of memory"" that reminded the audience of previous scenes in the drama.  Thus, leitmotifs not only served as dramatic devices to signal the appearance of different characters but provided the internal musical logic and unifying mechanism for the entire operatic cycle.  Through liberal use of leitmotifs, Wagner was able to generate something Michael Halliwell describes as ""a dense, constantly evolving, and fully enclosed dramatic world"" whose musical complexity was staggering.
Unlike most opera composers, Wagner wrote not only the musical scores to his works but also the libretti (the lyrics and narrative), which he dubbed ""poems.""  The Ring cycle's libretti were based on the medieval epics of Norse and Germanic mythology, such as the Eddas, the Volsung, and the Nibelungenlied.  Furthermore, the composer found a structural prototype for his masterpiece in the ancient Greek festival of the god Dionysus, for which playwrights produced four related dramas to be performed on successive days.  Wagner's own magnum opus would debut over four nights in August 1876 at the Festspielhaus, a state-of-the-art opera house built in Bayreuth, Germany, at the composer's behest.
In a life and career marked by controversy and scandal, Wagner endured debt, poverty, broken marriages, and political exile.  Yet his accomplishments in opera surpassed that of any other composer, including his Italian counterpart Giuseppe Verdi.  In fact, few areas of European and world culture have escaped Wagner's influence.  Most notably, his Ring cycle combined story with visual elements and dramatic music to anticipate the not-yet-invented art of cinema.  Indeed, early film score composers were schooled in Wagnerian techniques; they understood the versatile and subtle power of the leitmotif to unify all aspects of the drama and influence the audience emotionally.  Nevertheless, moviegoers today would struggle to identify Wagner's innovative use of the leitmotif as the progenitor of musical film scores and, particularly, of the popular phenomenon of the ""theme song.""",Critical Analysis & Reasoning Skills,5,Humanities
4,Passage Title: Wagner's Leitmotif,"Judging the opera of his day to be in a desperate state of decline, the 19th-century German composer Wilhelm Richard Wagner—arguably the most controversial, if not the most titanic, figure in the history of classical music—set out to revitalize and revolutionize the genre.  In his 1851 book-length essay, Opera and Drama, Wagner adopted the concept of Gesamtkunstwerk, or ""total work of art,"" leading him to envision the ideal form of opera as a synthesis of the poetic, visual, musical, and dramatic arts.  Thus, rather than the opera's drama being subordinate to the music, as was typical in Wagner's time, the music would now be integrated with the drama.
Subscribing to this aesthetic theory of Gesamtkunstwerk, Wagner produced his most celebrated operatic achievement, the Ring of the Nibelung, comprising four music dramas: The Rhinegold, The Valkyrie, Siegfried, and Twilight of the Gods.  With the creation of this monumental work, commonly called the Ring cycle, Wagner also reinvented the musical language of opera.  In contrast to the practices of his predecessors and contemporaries, like Carl Maria von Weber and Giacomo Meyerbeer, Wagner blurred the lines between the traditional operatic components of recitative (musical dialogue) and aria (solo) and virtually eliminated large choral segments.  Additionally, he amplified the role of the orchestra.
However, the most influential Wagnerian innovation was the evolution and systematic application of what would later be known as the leitmotif, a recurring musical phrase or melody closely associated with a particular character, place, or object in the drama.  The composer referred to these repeating phrases as ""melodic moments,"" ""ground themes,"" or ""carriers of feeling,"" and subjected them to endless modulation (changes in musical key) and variation.  They could, as Hans von Wolzogen explains, be ""developed, interwoven, assimilated with each other, and multifariously transformed.""  Wagner also strategically employed them as ""motifs of memory"" that reminded the audience of previous scenes in the drama.  Thus, leitmotifs not only served as dramatic devices to signal the appearance of different characters but provided the internal musical logic and unifying mechanism for the entire operatic cycle.  Through liberal use of leitmotifs, Wagner was able to generate something Michael Halliwell describes as ""a dense, constantly evolving, and fully enclosed dramatic world"" whose musical complexity was staggering.
Unlike most opera composers, Wagner wrote not only the musical scores to his works but also the libretti (the lyrics and narrative), which he dubbed ""poems.""  The Ring cycle's libretti were based on the medieval epics of Norse and Germanic mythology, such as the Eddas, the Volsung, and the Nibelungenlied.  Furthermore, the composer found a structural prototype for his masterpiece in the ancient Greek festival of the god Dionysus, for which playwrights produced four related dramas to be performed on successive days.  Wagner's own magnum opus would debut over four nights in August 1876 at the Festspielhaus, a state-of-the-art opera house built in Bayreuth, Germany, at the composer's behest.
In a life and career marked by controversy and scandal, Wagner endured debt, poverty, broken marriages, and political exile.  Yet his accomplishments in opera surpassed that of any other composer, including his Italian counterpart Giuseppe Verdi.  In fact, few areas of European and world culture have escaped Wagner's influence.  Most notably, his Ring cycle combined story with visual elements and dramatic music to anticipate the not-yet-invented art of cinema.  Indeed, early film score composers were schooled in Wagnerian techniques; they understood the versatile and subtle power of the leitmotif to unify all aspects of the drama and influence the audience emotionally.  Nevertheless, moviegoers today would struggle to identify Wagner's innovative use of the leitmotif as the progenitor of musical film scores and, particularly, of the popular phenomenon of the ""theme song.""",Critical Analysis & Reasoning Skills,5,Humanities
5,Passage Title: Wagner's Leitmotif,"Judging the opera of his day to be in a desperate state of decline, the 19th-century German composer Wilhelm Richard Wagner—arguably the most controversial, if not the most titanic, figure in the history of classical music—set out to revitalize and revolutionize the genre.  In his 1851 book-length essay, Opera and Drama, Wagner adopted the concept of Gesamtkunstwerk, or ""total work of art,"" leading him to envision the ideal form of opera as a synthesis of the poetic, visual, musical, and dramatic arts.  Thus, rather than the opera's drama being subordinate to the music, as was typical in Wagner's time, the music would now be integrated with the drama.
Subscribing to this aesthetic theory of Gesamtkunstwerk, Wagner produced his most celebrated operatic achievement, the Ring of the Nibelung, comprising four music dramas: The Rhinegold, The Valkyrie, Siegfried, and Twilight of the Gods.  With the creation of this monumental work, commonly called the Ring cycle, Wagner also reinvented the musical language of opera.  In contrast to the practices of his predecessors and contemporaries, like Carl Maria von Weber and Giacomo Meyerbeer, Wagner blurred the lines between the traditional operatic components of recitative (musical dialogue) and aria (solo) and virtually eliminated large choral segments.  Additionally, he amplified the role of the orchestra.
However, the most influential Wagnerian innovation was the evolution and systematic application of what would later be known as the leitmotif, a recurring musical phrase or melody closely associated with a particular character, place, or object in the drama.  The composer referred to these repeating phrases as ""melodic moments,"" ""ground themes,"" or ""carriers of feeling,"" and subjected them to endless modulation (changes in musical key) and variation.  They could, as Hans von Wolzogen explains, be ""developed, interwoven, assimilated with each other, and multifariously transformed.""  Wagner also strategically employed them as ""motifs of memory"" that reminded the audience of previous scenes in the drama.  Thus, leitmotifs not only served as dramatic devices to signal the appearance of different characters but provided the internal musical logic and unifying mechanism for the entire operatic cycle.  Through liberal use of leitmotifs, Wagner was able to generate something Michael Halliwell describes as ""a dense, constantly evolving, and fully enclosed dramatic world"" whose musical complexity was staggering.
Unlike most opera composers, Wagner wrote not only the musical scores to his works but also the libretti (the lyrics and narrative), which he dubbed ""poems.""  The Ring cycle's libretti were based on the medieval epics of Norse and Germanic mythology, such as the Eddas, the Volsung, and the Nibelungenlied.  Furthermore, the composer found a structural prototype for his masterpiece in the ancient Greek festival of the god Dionysus, for which playwrights produced four related dramas to be performed on successive days.  Wagner's own magnum opus would debut over four nights in August 1876 at the Festspielhaus, a state-of-the-art opera house built in Bayreuth, Germany, at the composer's behest.
In a life and career marked by controversy and scandal, Wagner endured debt, poverty, broken marriages, and political exile.  Yet his accomplishments in opera surpassed that of any other composer, including his Italian counterpart Giuseppe Verdi.  In fact, few areas of European and world culture have escaped Wagner's influence.  Most notably, his Ring cycle combined story with visual elements and dramatic music to anticipate the not-yet-invented art of cinema.  Indeed, early film score composers were schooled in Wagnerian techniques; they understood the versatile and subtle power of the leitmotif to unify all aspects of the drama and influence the audience emotionally.  Nevertheless, moviegoers today would struggle to identify Wagner's innovative use of the leitmotif as the progenitor of musical film scores and, particularly, of the popular phenomenon of the ""theme song.""",Critical Analysis & Reasoning Skills,5,Humanities
6,Passage Title: The Panama Canal,"To sail from the east of North America to its western coast was not impossible in the nineteenth century, but sailors may well have wished it was.  The voyage required ships to brave the frigid waters between South America and Antarctica, whose treacherous depths claimed as many as fifteen thousand bodies.  This abode of ninety-foot waves and buffeting gales marked only the mid-point in a circuit of the Americas, and the men who survived that reckoning still faced several more weeks at sea.
Understandably, the desire for a shortcut was strong.  Previous exploration had revealed no naturally occurring waterway through Central America, but what if such a passage could be built?  These considerations were the genesis of what would become the Panama Canal.
The first to attempt such a feat were the French, under the leadership of Ferdinand de Lesseps.  Having formerly engineered the Suez Canal connecting the Mediterranean and Red Seas, Lesseps was a natural choice to head what was presumed to be a similar endeavor.  With the enthusiastic support of his country and the permission of the Colombian government (which controlled Panama at that time), Lesseps and his team began work in 1880, 11 years after the Suez Canal had been completed.
However, the mountainous jungles and swamps of Panama proved a pernicious contrast to the flat sands of Egypt.  Equipment toppled down hillsides or was swallowed into the mud, while frequent rains washed excavated dirt mounds back into the pits from which they had been extracted.  More devastating was the human toll.  Living conditions were squalid and miserable, as workers were ravaged by rampant yellow fever and malaria.  Over twenty thousand lives were lost, with no end in sight to the scourge of disease or to the faltering project.  Ultimately, the construction company went bankrupt, some of its members were imprisoned, and the elderly Lesseps died.
With the failure of the French, the stage was set for the United States to try its hand.  Some American engineers recommended that the canal should be built through Nicaragua instead of Panama; although the former country was much wider, its terrain was significantly more hospitable.  An additional drawback to choosing Panama was that the Colombian government would not agree to let the U.S. control the canal once it was built.  Nonetheless, the U.S. decided to build in Panama.  Shortly thereafter, Panamanian dissident Manuel Amador led a rebellion against the Colombian government, while a U.S. warship happened to be sailing off the coast.  Victory was swift; as writer Janet Pascal notes: ""The entire revolution lasted only a few hours.  Only one shopkeeper and a donkey were killed.""  Afterward, newly independent Panama granted construction rights and control of the future canal to the United States.
Unfortunately, the U.S. claimed the French's abandoned equipment and facilities while failing to address their logistical difficulties and incidence of disease.  After a year of futility, wiser minds devised a new course.  Acting on the uncommon but accurate belief that mosquitoes spread malaria and yellow fever, Americans exterminated those pests and thus the maladies they caused.  In conjunction with new living facilities and improved transportation, this action provided the ability to sustain a healthy workforce and build the canal based on proper planning.
The project spanned the American presidencies of Theodore Roosevelt, William Howard Taft, and Woodrow Wilson, reaching completion in 1914.  Traversing the breadth of Panama and incorporating a series of locks to raise and lower ships to different elevations, the Panama Canal finally made it possible to sail from the Atlantic Ocean to the Pacific without circumnavigating South America.  It remains in use to this day, more than a century after its construction.",Critical Analysis & Reasoning Skills,10,Social Sciences
7,Passage Title: The Panama Canal,"To sail from the east of North America to its western coast was not impossible in the nineteenth century, but sailors may well have wished it was.  The voyage required ships to brave the frigid waters between South America and Antarctica, whose treacherous depths claimed as many as fifteen thousand bodies.  This abode of ninety-foot waves and buffeting gales marked only the mid-point in a circuit of the Americas, and the men who survived that reckoning still faced several more weeks at sea.
Understandably, the desire for a shortcut was strong.  Previous exploration had revealed no naturally occurring waterway through Central America, but what if such a passage could be built?  These considerations were the genesis of what would become the Panama Canal.
The first to attempt such a feat were the French, under the leadership of Ferdinand de Lesseps.  Having formerly engineered the Suez Canal connecting the Mediterranean and Red Seas, Lesseps was a natural choice to head what was presumed to be a similar endeavor.  With the enthusiastic support of his country and the permission of the Colombian government (which controlled Panama at that time), Lesseps and his team began work in 1880, 11 years after the Suez Canal had been completed.
However, the mountainous jungles and swamps of Panama proved a pernicious contrast to the flat sands of Egypt.  Equipment toppled down hillsides or was swallowed into the mud, while frequent rains washed excavated dirt mounds back into the pits from which they had been extracted.  More devastating was the human toll.  Living conditions were squalid and miserable, as workers were ravaged by rampant yellow fever and malaria.  Over twenty thousand lives were lost, with no end in sight to the scourge of disease or to the faltering project.  Ultimately, the construction company went bankrupt, some of its members were imprisoned, and the elderly Lesseps died.
With the failure of the French, the stage was set for the United States to try its hand.  Some American engineers recommended that the canal should be built through Nicaragua instead of Panama; although the former country was much wider, its terrain was significantly more hospitable.  An additional drawback to choosing Panama was that the Colombian government would not agree to let the U.S. control the canal once it was built.  Nonetheless, the U.S. decided to build in Panama.  Shortly thereafter, Panamanian dissident Manuel Amador led a rebellion against the Colombian government, while a U.S. warship happened to be sailing off the coast.  Victory was swift; as writer Janet Pascal notes: ""The entire revolution lasted only a few hours.  Only one shopkeeper and a donkey were killed.""  Afterward, newly independent Panama granted construction rights and control of the future canal to the United States.
Unfortunately, the U.S. claimed the French's abandoned equipment and facilities while failing to address their logistical difficulties and incidence of disease.  After a year of futility, wiser minds devised a new course.  Acting on the uncommon but accurate belief that mosquitoes spread malaria and yellow fever, Americans exterminated those pests and thus the maladies they caused.  In conjunction with new living facilities and improved transportation, this action provided the ability to sustain a healthy workforce and build the canal based on proper planning.
The project spanned the American presidencies of Theodore Roosevelt, William Howard Taft, and Woodrow Wilson, reaching completion in 1914.  Traversing the breadth of Panama and incorporating a series of locks to raise and lower ships to different elevations, the Panama Canal finally made it possible to sail from the Atlantic Ocean to the Pacific without circumnavigating South America.  It remains in use to this day, more than a century after its construction.",Critical Analysis & Reasoning Skills,10,Social Sciences
8,Passage Title: The Panama Canal,"To sail from the east of North America to its western coast was not impossible in the nineteenth century, but sailors may well have wished it was.  The voyage required ships to brave the frigid waters between South America and Antarctica, whose treacherous depths claimed as many as fifteen thousand bodies.  This abode of ninety-foot waves and buffeting gales marked only the mid-point in a circuit of the Americas, and the men who survived that reckoning still faced several more weeks at sea.
Understandably, the desire for a shortcut was strong.  Previous exploration had revealed no naturally occurring waterway through Central America, but what if such a passage could be built?  These considerations were the genesis of what would become the Panama Canal.
The first to attempt such a feat were the French, under the leadership of Ferdinand de Lesseps.  Having formerly engineered the Suez Canal connecting the Mediterranean and Red Seas, Lesseps was a natural choice to head what was presumed to be a similar endeavor.  With the enthusiastic support of his country and the permission of the Colombian government (which controlled Panama at that time), Lesseps and his team began work in 1880, 11 years after the Suez Canal had been completed.
However, the mountainous jungles and swamps of Panama proved a pernicious contrast to the flat sands of Egypt.  Equipment toppled down hillsides or was swallowed into the mud, while frequent rains washed excavated dirt mounds back into the pits from which they had been extracted.  More devastating was the human toll.  Living conditions were squalid and miserable, as workers were ravaged by rampant yellow fever and malaria.  Over twenty thousand lives were lost, with no end in sight to the scourge of disease or to the faltering project.  Ultimately, the construction company went bankrupt, some of its members were imprisoned, and the elderly Lesseps died.
With the failure of the French, the stage was set for the United States to try its hand.  Some American engineers recommended that the canal should be built through Nicaragua instead of Panama; although the former country was much wider, its terrain was significantly more hospitable.  An additional drawback to choosing Panama was that the Colombian government would not agree to let the U.S. control the canal once it was built.  Nonetheless, the U.S. decided to build in Panama.  Shortly thereafter, Panamanian dissident Manuel Amador led a rebellion against the Colombian government, while a U.S. warship happened to be sailing off the coast.  Victory was swift; as writer Janet Pascal notes: ""The entire revolution lasted only a few hours.  Only one shopkeeper and a donkey were killed.""  Afterward, newly independent Panama granted construction rights and control of the future canal to the United States.
Unfortunately, the U.S. claimed the French's abandoned equipment and facilities while failing to address their logistical difficulties and incidence of disease.  After a year of futility, wiser minds devised a new course.  Acting on the uncommon but accurate belief that mosquitoes spread malaria and yellow fever, Americans exterminated those pests and thus the maladies they caused.  In conjunction with new living facilities and improved transportation, this action provided the ability to sustain a healthy workforce and build the canal based on proper planning.
The project spanned the American presidencies of Theodore Roosevelt, William Howard Taft, and Woodrow Wilson, reaching completion in 1914.  Traversing the breadth of Panama and incorporating a series of locks to raise and lower ships to different elevations, the Panama Canal finally made it possible to sail from the Atlantic Ocean to the Pacific without circumnavigating South America.  It remains in use to this day, more than a century after its construction.",Critical Analysis & Reasoning Skills,10,Social Sciences
9,Passage Title: The Panama Canal,"To sail from the east of North America to its western coast was not impossible in the nineteenth century, but sailors may well have wished it was.  The voyage required ships to brave the frigid waters between South America and Antarctica, whose treacherous depths claimed as many as fifteen thousand bodies.  This abode of ninety-foot waves and buffeting gales marked only the mid-point in a circuit of the Americas, and the men who survived that reckoning still faced several more weeks at sea.
Understandably, the desire for a shortcut was strong.  Previous exploration had revealed no naturally occurring waterway through Central America, but what if such a passage could be built?  These considerations were the genesis of what would become the Panama Canal.
The first to attempt such a feat were the French, under the leadership of Ferdinand de Lesseps.  Having formerly engineered the Suez Canal connecting the Mediterranean and Red Seas, Lesseps was a natural choice to head what was presumed to be a similar endeavor.  With the enthusiastic support of his country and the permission of the Colombian government (which controlled Panama at that time), Lesseps and his team began work in 1880, 11 years after the Suez Canal had been completed.
However, the mountainous jungles and swamps of Panama proved a pernicious contrast to the flat sands of Egypt.  Equipment toppled down hillsides or was swallowed into the mud, while frequent rains washed excavated dirt mounds back into the pits from which they had been extracted.  More devastating was the human toll.  Living conditions were squalid and miserable, as workers were ravaged by rampant yellow fever and malaria.  Over twenty thousand lives were lost, with no end in sight to the scourge of disease or to the faltering project.  Ultimately, the construction company went bankrupt, some of its members were imprisoned, and the elderly Lesseps died.
With the failure of the French, the stage was set for the United States to try its hand.  Some American engineers recommended that the canal should be built through Nicaragua instead of Panama; although the former country was much wider, its terrain was significantly more hospitable.  An additional drawback to choosing Panama was that the Colombian government would not agree to let the U.S. control the canal once it was built.  Nonetheless, the U.S. decided to build in Panama.  Shortly thereafter, Panamanian dissident Manuel Amador led a rebellion against the Colombian government, while a U.S. warship happened to be sailing off the coast.  Victory was swift; as writer Janet Pascal notes: ""The entire revolution lasted only a few hours.  Only one shopkeeper and a donkey were killed.""  Afterward, newly independent Panama granted construction rights and control of the future canal to the United States.
Unfortunately, the U.S. claimed the French's abandoned equipment and facilities while failing to address their logistical difficulties and incidence of disease.  After a year of futility, wiser minds devised a new course.  Acting on the uncommon but accurate belief that mosquitoes spread malaria and yellow fever, Americans exterminated those pests and thus the maladies they caused.  In conjunction with new living facilities and improved transportation, this action provided the ability to sustain a healthy workforce and build the canal based on proper planning.
The project spanned the American presidencies of Theodore Roosevelt, William Howard Taft, and Woodrow Wilson, reaching completion in 1914.  Traversing the breadth of Panama and incorporating a series of locks to raise and lower ships to different elevations, the Panama Canal finally made it possible to sail from the Atlantic Ocean to the Pacific without circumnavigating South America.  It remains in use to this day, more than a century after its construction.",Critical Analysis & Reasoning Skills,10,Social Sciences
10,Passage Title: The Panama Canal,"To sail from the east of North America to its western coast was not impossible in the nineteenth century, but sailors may well have wished it was.  The voyage required ships to brave the frigid waters between South America and Antarctica, whose treacherous depths claimed as many as fifteen thousand bodies.  This abode of ninety-foot waves and buffeting gales marked only the mid-point in a circuit of the Americas, and the men who survived that reckoning still faced several more weeks at sea.
Understandably, the desire for a shortcut was strong.  Previous exploration had revealed no naturally occurring waterway through Central America, but what if such a passage could be built?  These considerations were the genesis of what would become the Panama Canal.
The first to attempt such a feat were the French, under the leadership of Ferdinand de Lesseps.  Having formerly engineered the Suez Canal connecting the Mediterranean and Red Seas, Lesseps was a natural choice to head what was presumed to be a similar endeavor.  With the enthusiastic support of his country and the permission of the Colombian government (which controlled Panama at that time), Lesseps and his team began work in 1880, 11 years after the Suez Canal had been completed.
However, the mountainous jungles and swamps of Panama proved a pernicious contrast to the flat sands of Egypt.  Equipment toppled down hillsides or was swallowed into the mud, while frequent rains washed excavated dirt mounds back into the pits from which they had been extracted.  More devastating was the human toll.  Living conditions were squalid and miserable, as workers were ravaged by rampant yellow fever and malaria.  Over twenty thousand lives were lost, with no end in sight to the scourge of disease or to the faltering project.  Ultimately, the construction company went bankrupt, some of its members were imprisoned, and the elderly Lesseps died.
With the failure of the French, the stage was set for the United States to try its hand.  Some American engineers recommended that the canal should be built through Nicaragua instead of Panama; although the former country was much wider, its terrain was significantly more hospitable.  An additional drawback to choosing Panama was that the Colombian government would not agree to let the U.S. control the canal once it was built.  Nonetheless, the U.S. decided to build in Panama.  Shortly thereafter, Panamanian dissident Manuel Amador led a rebellion against the Colombian government, while a U.S. warship happened to be sailing off the coast.  Victory was swift; as writer Janet Pascal notes: ""The entire revolution lasted only a few hours.  Only one shopkeeper and a donkey were killed.""  Afterward, newly independent Panama granted construction rights and control of the future canal to the United States.
Unfortunately, the U.S. claimed the French's abandoned equipment and facilities while failing to address their logistical difficulties and incidence of disease.  After a year of futility, wiser minds devised a new course.  Acting on the uncommon but accurate belief that mosquitoes spread malaria and yellow fever, Americans exterminated those pests and thus the maladies they caused.  In conjunction with new living facilities and improved transportation, this action provided the ability to sustain a healthy workforce and build the canal based on proper planning.
The project spanned the American presidencies of Theodore Roosevelt, William Howard Taft, and Woodrow Wilson, reaching completion in 1914.  Traversing the breadth of Panama and incorporating a series of locks to raise and lower ships to different elevations, the Panama Canal finally made it possible to sail from the Atlantic Ocean to the Pacific without circumnavigating South America.  It remains in use to this day, more than a century after its construction.",Critical Analysis & Reasoning Skills,10,Social Sciences
11,Passage Title: Psychic Ethics,"What is a lie?  Most would say that a lie is simply a statement that is false, or that the act of lying is to tell someone something false.  Despite the popularity of that definition, it is not really accurate, as telling someone a falsehood is not in fact the same thing as lying.  Even more strangely, in some cases a lie is true—not in a metaphorical sense, but literally.
To see why, consider a thought experiment.  Let's say that your co-worker, Mark, tells you that there is a meeting in the conference room at 11:00 AM.  Around 10:55 AM you walk in, but no one else is present.  You wait five minutes, ten minutes…by 11:15 AM, still no one else has arrived.  Did your co-worker lie to you?  You already know his statement was false (because there was no meeting), but that knowledge is insufficient.  To determine whether Mark was lying, what you need to know is whether he believed there was going to be a meeting.  If he did, then he didn't lie to you; he was just mistaken.  Essentially, if someone tries to convince you of something he thinks is false, then he is lying—regardless of whether his statement really is false or, unbeknownst to him, happens to be true.  But if he says something he believes is true but is actually false, then he is not lying.  Thus, what makes a statement a lie is not whether it is false, but whether it is meant to deceive.
A related and unusual example comes from real life.  Sarah was hired by a psychic hotline to tell the fortunes of those who called in.  When speaking to a customer she would use a tarot deck in the traditional way, then describe the cards' perceived implications for the person's future.  However, her employer did not want the psychics to provide ""genuine"" psychic readings, but instead to address customers using approved scripts based on corporate assumptions of what callers wanted to hear.  Because Sarah was giving the customers ""real"" psychic readings instead of the company's pre-fabricated ones, her employment was terminated.
The intriguing part of this case is the odd juxtaposition of relevant ethical factors.  In one sense, what Sarah was doing is inherently fraudulent.  Fortune-telling does not work; thus, by conveying to customers that she could tell them the future, Sarah was saying something that is manifestly untrue and purporting to sell something that she could not possibly provide.  Those seeking to purchase knowledge about their future could not—except perhaps by lucky coincidence—receive what they hoped to buy.  In another sense, however, Sarah's actions were not fraudulent at all, because she herself believed in the power of tarot; one can be factually wrong without being morally wrong.  To the extent that Sarah sincerely tried to give her callers what she thought were legitimate psychic readings, she displayed honesty and integrity.
Certainly, she acted in good faith while her employer clearly did not.  The hotline claimed to sell accurate predictions from ""real psychics"" while knowingly providing nothing of the sort.  Obviously, there is no such thing as a real psychic, and the predictions are nothing but superstition.  But even a disclaimer like ""for entertainment purposes only"" does not justify a business model that takes advantage of a customer's false belief or misrepresents what it supposedly offers.  Because Sarah honestly tried to give genuine psychic readings, her actions were ethical, whereas her employer's actions were unethical.  Her product may have been fake, but her honesty was real.",Critical Analysis & Reasoning Skills,17,Humanities
12,Passage Title: Psychic Ethics,"What is a lie?  Most would say that a lie is simply a statement that is false, or that the act of lying is to tell someone something false.  Despite the popularity of that definition, it is not really accurate, as telling someone a falsehood is not in fact the same thing as lying.  Even more strangely, in some cases a lie is true—not in a metaphorical sense, but literally.
To see why, consider a thought experiment.  Let's say that your co-worker, Mark, tells you that there is a meeting in the conference room at 11:00 AM.  Around 10:55 AM you walk in, but no one else is present.  You wait five minutes, ten minutes…by 11:15 AM, still no one else has arrived.  Did your co-worker lie to you?  You already know his statement was false (because there was no meeting), but that knowledge is insufficient.  To determine whether Mark was lying, what you need to know is whether he believed there was going to be a meeting.  If he did, then he didn't lie to you; he was just mistaken.  Essentially, if someone tries to convince you of something he thinks is false, then he is lying—regardless of whether his statement really is false or, unbeknownst to him, happens to be true.  But if he says something he believes is true but is actually false, then he is not lying.  Thus, what makes a statement a lie is not whether it is false, but whether it is meant to deceive.
A related and unusual example comes from real life.  Sarah was hired by a psychic hotline to tell the fortunes of those who called in.  When speaking to a customer she would use a tarot deck in the traditional way, then describe the cards' perceived implications for the person's future.  However, her employer did not want the psychics to provide ""genuine"" psychic readings, but instead to address customers using approved scripts based on corporate assumptions of what callers wanted to hear.  Because Sarah was giving the customers ""real"" psychic readings instead of the company's pre-fabricated ones, her employment was terminated.
The intriguing part of this case is the odd juxtaposition of relevant ethical factors.  In one sense, what Sarah was doing is inherently fraudulent.  Fortune-telling does not work; thus, by conveying to customers that she could tell them the future, Sarah was saying something that is manifestly untrue and purporting to sell something that she could not possibly provide.  Those seeking to purchase knowledge about their future could not—except perhaps by lucky coincidence—receive what they hoped to buy.  In another sense, however, Sarah's actions were not fraudulent at all, because she herself believed in the power of tarot; one can be factually wrong without being morally wrong.  To the extent that Sarah sincerely tried to give her callers what she thought were legitimate psychic readings, she displayed honesty and integrity.
Certainly, she acted in good faith while her employer clearly did not.  The hotline claimed to sell accurate predictions from ""real psychics"" while knowingly providing nothing of the sort.  Obviously, there is no such thing as a real psychic, and the predictions are nothing but superstition.  But even a disclaimer like ""for entertainment purposes only"" does not justify a business model that takes advantage of a customer's false belief or misrepresents what it supposedly offers.  Because Sarah honestly tried to give genuine psychic readings, her actions were ethical, whereas her employer's actions were unethical.  Her product may have been fake, but her honesty was real.",Critical Analysis & Reasoning Skills,17,Humanities
13,Passage Title: Psychic Ethics,"What is a lie?  Most would say that a lie is simply a statement that is false, or that the act of lying is to tell someone something false.  Despite the popularity of that definition, it is not really accurate, as telling someone a falsehood is not in fact the same thing as lying.  Even more strangely, in some cases a lie is true—not in a metaphorical sense, but literally.
To see why, consider a thought experiment.  Let's say that your co-worker, Mark, tells you that there is a meeting in the conference room at 11:00 AM.  Around 10:55 AM you walk in, but no one else is present.  You wait five minutes, ten minutes…by 11:15 AM, still no one else has arrived.  Did your co-worker lie to you?  You already know his statement was false (because there was no meeting), but that knowledge is insufficient.  To determine whether Mark was lying, what you need to know is whether he believed there was going to be a meeting.  If he did, then he didn't lie to you; he was just mistaken.  Essentially, if someone tries to convince you of something he thinks is false, then he is lying—regardless of whether his statement really is false or, unbeknownst to him, happens to be true.  But if he says something he believes is true but is actually false, then he is not lying.  Thus, what makes a statement a lie is not whether it is false, but whether it is meant to deceive.
A related and unusual example comes from real life.  Sarah was hired by a psychic hotline to tell the fortunes of those who called in.  When speaking to a customer she would use a tarot deck in the traditional way, then describe the cards' perceived implications for the person's future.  However, her employer did not want the psychics to provide ""genuine"" psychic readings, but instead to address customers using approved scripts based on corporate assumptions of what callers wanted to hear.  Because Sarah was giving the customers ""real"" psychic readings instead of the company's pre-fabricated ones, her employment was terminated.
The intriguing part of this case is the odd juxtaposition of relevant ethical factors.  In one sense, what Sarah was doing is inherently fraudulent.  Fortune-telling does not work; thus, by conveying to customers that she could tell them the future, Sarah was saying something that is manifestly untrue and purporting to sell something that she could not possibly provide.  Those seeking to purchase knowledge about their future could not—except perhaps by lucky coincidence—receive what they hoped to buy.  In another sense, however, Sarah's actions were not fraudulent at all, because she herself believed in the power of tarot; one can be factually wrong without being morally wrong.  To the extent that Sarah sincerely tried to give her callers what she thought were legitimate psychic readings, she displayed honesty and integrity.
Certainly, she acted in good faith while her employer clearly did not.  The hotline claimed to sell accurate predictions from ""real psychics"" while knowingly providing nothing of the sort.  Obviously, there is no such thing as a real psychic, and the predictions are nothing but superstition.  But even a disclaimer like ""for entertainment purposes only"" does not justify a business model that takes advantage of a customer's false belief or misrepresents what it supposedly offers.  Because Sarah honestly tried to give genuine psychic readings, her actions were ethical, whereas her employer's actions were unethical.  Her product may have been fake, but her honesty was real.",Critical Analysis & Reasoning Skills,17,Humanities
14,Passage Title: Psychic Ethics,"What is a lie?  Most would say that a lie is simply a statement that is false, or that the act of lying is to tell someone something false.  Despite the popularity of that definition, it is not really accurate, as telling someone a falsehood is not in fact the same thing as lying.  Even more strangely, in some cases a lie is true—not in a metaphorical sense, but literally.
To see why, consider a thought experiment.  Let's say that your co-worker, Mark, tells you that there is a meeting in the conference room at 11:00 AM.  Around 10:55 AM you walk in, but no one else is present.  You wait five minutes, ten minutes…by 11:15 AM, still no one else has arrived.  Did your co-worker lie to you?  You already know his statement was false (because there was no meeting), but that knowledge is insufficient.  To determine whether Mark was lying, what you need to know is whether he believed there was going to be a meeting.  If he did, then he didn't lie to you; he was just mistaken.  Essentially, if someone tries to convince you of something he thinks is false, then he is lying—regardless of whether his statement really is false or, unbeknownst to him, happens to be true.  But if he says something he believes is true but is actually false, then he is not lying.  Thus, what makes a statement a lie is not whether it is false, but whether it is meant to deceive.
A related and unusual example comes from real life.  Sarah was hired by a psychic hotline to tell the fortunes of those who called in.  When speaking to a customer she would use a tarot deck in the traditional way, then describe the cards' perceived implications for the person's future.  However, her employer did not want the psychics to provide ""genuine"" psychic readings, but instead to address customers using approved scripts based on corporate assumptions of what callers wanted to hear.  Because Sarah was giving the customers ""real"" psychic readings instead of the company's pre-fabricated ones, her employment was terminated.
The intriguing part of this case is the odd juxtaposition of relevant ethical factors.  In one sense, what Sarah was doing is inherently fraudulent.  Fortune-telling does not work; thus, by conveying to customers that she could tell them the future, Sarah was saying something that is manifestly untrue and purporting to sell something that she could not possibly provide.  Those seeking to purchase knowledge about their future could not—except perhaps by lucky coincidence—receive what they hoped to buy.  In another sense, however, Sarah's actions were not fraudulent at all, because she herself believed in the power of tarot; one can be factually wrong without being morally wrong.  To the extent that Sarah sincerely tried to give her callers what she thought were legitimate psychic readings, she displayed honesty and integrity.
Certainly, she acted in good faith while her employer clearly did not.  The hotline claimed to sell accurate predictions from ""real psychics"" while knowingly providing nothing of the sort.  Obviously, there is no such thing as a real psychic, and the predictions are nothing but superstition.  But even a disclaimer like ""for entertainment purposes only"" does not justify a business model that takes advantage of a customer's false belief or misrepresents what it supposedly offers.  Because Sarah honestly tried to give genuine psychic readings, her actions were ethical, whereas her employer's actions were unethical.  Her product may have been fake, but her honesty was real.",Critical Analysis & Reasoning Skills,17,Humanities
15,Passage Title: Psychic Ethics,"What is a lie?  Most would say that a lie is simply a statement that is false, or that the act of lying is to tell someone something false.  Despite the popularity of that definition, it is not really accurate, as telling someone a falsehood is not in fact the same thing as lying.  Even more strangely, in some cases a lie is true—not in a metaphorical sense, but literally.
To see why, consider a thought experiment.  Let's say that your co-worker, Mark, tells you that there is a meeting in the conference room at 11:00 AM.  Around 10:55 AM you walk in, but no one else is present.  You wait five minutes, ten minutes…by 11:15 AM, still no one else has arrived.  Did your co-worker lie to you?  You already know his statement was false (because there was no meeting), but that knowledge is insufficient.  To determine whether Mark was lying, what you need to know is whether he believed there was going to be a meeting.  If he did, then he didn't lie to you; he was just mistaken.  Essentially, if someone tries to convince you of something he thinks is false, then he is lying—regardless of whether his statement really is false or, unbeknownst to him, happens to be true.  But if he says something he believes is true but is actually false, then he is not lying.  Thus, what makes a statement a lie is not whether it is false, but whether it is meant to deceive.
A related and unusual example comes from real life.  Sarah was hired by a psychic hotline to tell the fortunes of those who called in.  When speaking to a customer she would use a tarot deck in the traditional way, then describe the cards' perceived implications for the person's future.  However, her employer did not want the psychics to provide ""genuine"" psychic readings, but instead to address customers using approved scripts based on corporate assumptions of what callers wanted to hear.  Because Sarah was giving the customers ""real"" psychic readings instead of the company's pre-fabricated ones, her employment was terminated.
The intriguing part of this case is the odd juxtaposition of relevant ethical factors.  In one sense, what Sarah was doing is inherently fraudulent.  Fortune-telling does not work; thus, by conveying to customers that she could tell them the future, Sarah was saying something that is manifestly untrue and purporting to sell something that she could not possibly provide.  Those seeking to purchase knowledge about their future could not—except perhaps by lucky coincidence—receive what they hoped to buy.  In another sense, however, Sarah's actions were not fraudulent at all, because she herself believed in the power of tarot; one can be factually wrong without being morally wrong.  To the extent that Sarah sincerely tried to give her callers what she thought were legitimate psychic readings, she displayed honesty and integrity.
Certainly, she acted in good faith while her employer clearly did not.  The hotline claimed to sell accurate predictions from ""real psychics"" while knowingly providing nothing of the sort.  Obviously, there is no such thing as a real psychic, and the predictions are nothing but superstition.  But even a disclaimer like ""for entertainment purposes only"" does not justify a business model that takes advantage of a customer's false belief or misrepresents what it supposedly offers.  Because Sarah honestly tried to give genuine psychic readings, her actions were ethical, whereas her employer's actions were unethical.  Her product may have been fake, but her honesty was real.",Critical Analysis & Reasoning Skills,17,Humanities
16,Passage Title: Psychic Ethics,"What is a lie?  Most would say that a lie is simply a statement that is false, or that the act of lying is to tell someone something false.  Despite the popularity of that definition, it is not really accurate, as telling someone a falsehood is not in fact the same thing as lying.  Even more strangely, in some cases a lie is true—not in a metaphorical sense, but literally.
To see why, consider a thought experiment.  Let's say that your co-worker, Mark, tells you that there is a meeting in the conference room at 11:00 AM.  Around 10:55 AM you walk in, but no one else is present.  You wait five minutes, ten minutes…by 11:15 AM, still no one else has arrived.  Did your co-worker lie to you?  You already know his statement was false (because there was no meeting), but that knowledge is insufficient.  To determine whether Mark was lying, what you need to know is whether he believed there was going to be a meeting.  If he did, then he didn't lie to you; he was just mistaken.  Essentially, if someone tries to convince you of something he thinks is false, then he is lying—regardless of whether his statement really is false or, unbeknownst to him, happens to be true.  But if he says something he believes is true but is actually false, then he is not lying.  Thus, what makes a statement a lie is not whether it is false, but whether it is meant to deceive.
A related and unusual example comes from real life.  Sarah was hired by a psychic hotline to tell the fortunes of those who called in.  When speaking to a customer she would use a tarot deck in the traditional way, then describe the cards' perceived implications for the person's future.  However, her employer did not want the psychics to provide ""genuine"" psychic readings, but instead to address customers using approved scripts based on corporate assumptions of what callers wanted to hear.  Because Sarah was giving the customers ""real"" psychic readings instead of the company's pre-fabricated ones, her employment was terminated.
The intriguing part of this case is the odd juxtaposition of relevant ethical factors.  In one sense, what Sarah was doing is inherently fraudulent.  Fortune-telling does not work; thus, by conveying to customers that she could tell them the future, Sarah was saying something that is manifestly untrue and purporting to sell something that she could not possibly provide.  Those seeking to purchase knowledge about their future could not—except perhaps by lucky coincidence—receive what they hoped to buy.  In another sense, however, Sarah's actions were not fraudulent at all, because she herself believed in the power of tarot; one can be factually wrong without being morally wrong.  To the extent that Sarah sincerely tried to give her callers what she thought were legitimate psychic readings, she displayed honesty and integrity.
Certainly, she acted in good faith while her employer clearly did not.  The hotline claimed to sell accurate predictions from ""real psychics"" while knowingly providing nothing of the sort.  Obviously, there is no such thing as a real psychic, and the predictions are nothing but superstition.  But even a disclaimer like ""for entertainment purposes only"" does not justify a business model that takes advantage of a customer's false belief or misrepresents what it supposedly offers.  Because Sarah honestly tried to give genuine psychic readings, her actions were ethical, whereas her employer's actions were unethical.  Her product may have been fake, but her honesty was real.",Critical Analysis & Reasoning Skills,17,Humanities
17,Passage Title: Psychic Ethics,"What is a lie?  Most would say that a lie is simply a statement that is false, or that the act of lying is to tell someone something false.  Despite the popularity of that definition, it is not really accurate, as telling someone a falsehood is not in fact the same thing as lying.  Even more strangely, in some cases a lie is true—not in a metaphorical sense, but literally.
To see why, consider a thought experiment.  Let's say that your co-worker, Mark, tells you that there is a meeting in the conference room at 11:00 AM.  Around 10:55 AM you walk in, but no one else is present.  You wait five minutes, ten minutes…by 11:15 AM, still no one else has arrived.  Did your co-worker lie to you?  You already know his statement was false (because there was no meeting), but that knowledge is insufficient.  To determine whether Mark was lying, what you need to know is whether he believed there was going to be a meeting.  If he did, then he didn't lie to you; he was just mistaken.  Essentially, if someone tries to convince you of something he thinks is false, then he is lying—regardless of whether his statement really is false or, unbeknownst to him, happens to be true.  But if he says something he believes is true but is actually false, then he is not lying.  Thus, what makes a statement a lie is not whether it is false, but whether it is meant to deceive.
A related and unusual example comes from real life.  Sarah was hired by a psychic hotline to tell the fortunes of those who called in.  When speaking to a customer she would use a tarot deck in the traditional way, then describe the cards' perceived implications for the person's future.  However, her employer did not want the psychics to provide ""genuine"" psychic readings, but instead to address customers using approved scripts based on corporate assumptions of what callers wanted to hear.  Because Sarah was giving the customers ""real"" psychic readings instead of the company's pre-fabricated ones, her employment was terminated.
The intriguing part of this case is the odd juxtaposition of relevant ethical factors.  In one sense, what Sarah was doing is inherently fraudulent.  Fortune-telling does not work; thus, by conveying to customers that she could tell them the future, Sarah was saying something that is manifestly untrue and purporting to sell something that she could not possibly provide.  Those seeking to purchase knowledge about their future could not—except perhaps by lucky coincidence—receive what they hoped to buy.  In another sense, however, Sarah's actions were not fraudulent at all, because she herself believed in the power of tarot; one can be factually wrong without being morally wrong.  To the extent that Sarah sincerely tried to give her callers what she thought were legitimate psychic readings, she displayed honesty and integrity.
Certainly, she acted in good faith while her employer clearly did not.  The hotline claimed to sell accurate predictions from ""real psychics"" while knowingly providing nothing of the sort.  Obviously, there is no such thing as a real psychic, and the predictions are nothing but superstition.  But even a disclaimer like ""for entertainment purposes only"" does not justify a business model that takes advantage of a customer's false belief or misrepresents what it supposedly offers.  Because Sarah honestly tried to give genuine psychic readings, her actions were ethical, whereas her employer's actions were unethical.  Her product may have been fake, but her honesty was real.",Critical Analysis & Reasoning Skills,17,Humanities
18,Passage Title: The Mirror Stage,"Certain schools of thought that arose in the mid- to late 20th century and have since been labeled ""poststructuralist"" are well known for their propensity to ""problematize,"" ""destabilize,"" or ""radicalize"" the concepts of previous thinkers.  For example, the controversial French poststructuralist and psychoanalyst Jacques Lacan sought to reenvision the concept, first introduced by Freud in the early 1900s, of the ego as the autonomous agent of the psyche.  Hence in his seminal 1949 essay, ""The Mirror Stage as Formative of the 'I' Function as Revealed in the Psychoanalytic Experience,"" Lacan variously describes the formation and consequent function of the ego as a schismatic event of misrecognition, an epiphenomenon of narcissism, and a process of dialectical tension.  That process sets up an interchange between identification and alienation, generating confusion between being a subject as opposed to an object, between self and other, between what is ""me"" and ""not me.""
The drama of the ego's emergence in what Lacan has designated ""the Mirror Stage"" explains its contrary nature.  Somewhere between 6 and 18 months of age, an infant will likely encounter his (or her) own reflection in a mirror.  At this phase of development, the child lacks any sense of a unified self.  However, when encouraged by an adult, he is able to recognize his reflection in the glass and, at the same time, comes to understand that the specular image looking back at him is, in fact, not himself.  In other words, the image provides the infant with the first glimpse of himself as an object, as ""other.""  According to Lacan, the initial fascination and pleasure the child experiences upon self-recognition dissolves into confusion at being unable to distinguish between what is and what is not himself.  Therefore, such recognition amounts to a form of misrecognition, the French term for which, méconnaissance, implies an essential misunderstanding or lack of knowledge.
Furthermore, while the reflection in the mirror appears coherent, coordinated, and whole, the child himself continues to perceive his body as uncoordinated and fragmentary, resulting in a profound sense of incongruity and disharmony.  By exuding mastery and harmony, the mirror image functions as a gestalt: it holds up an ""Ideal-I"" or future self that promises completeness and perfection—a condition, Lacan believes, that is ultimately unattainable, though we will likely spend the rest of our lives in pursuit of it.  Overall, the infant's reaction to the image or Ideal-I is narcissistic; he is attracted to it as a model but feels aggression toward it as a rival.
Alienation, lack, absence, and conflict are thus constitutive of the Lacanian ego.  The child's image—and, hence, the emerging ego—marks an ontological gap that turns into a repository for the projections and desires, whether conscious or unconscious, of parents and others.  Consequently, this image, as Adrian Johnston notes, is ""always already overwritten"" with words that are not the child's own.  In Johnston's view, the ego could be more properly described as ""a coagulation of inter- and trans-subjective alien influences.""  Rather than a ""fluid and autonomous subject,"" it becomes a rigid and ""heteronomous"" entity.  Derived from the desires of others, the ego is, in Lacanian terms, ""extimate,"" or internally exterior.  Both alien and alienating, it is an irreducible contradiction.
This ego paradigm diverges from the more familiar and cohesive Freudian notion of the ego as the sovereign organizer of the personality, the rational mediator between internal drives and social pressures that in the interests of self-preservation will also resort to deceptions and defense mechanisms.  From a Lacanian perspective, the ego is intrinsically and irreparably divided; it chiefly serves to support the fictional construct we identify—or rather, misrecognize—as the ""self.""",Critical Analysis & Reasoning Skills,22,Social Sciences
19,Passage Title: The Mirror Stage,"Certain schools of thought that arose in the mid- to late 20th century and have since been labeled ""poststructuralist"" are well known for their propensity to ""problematize,"" ""destabilize,"" or ""radicalize"" the concepts of previous thinkers.  For example, the controversial French poststructuralist and psychoanalyst Jacques Lacan sought to reenvision the concept, first introduced by Freud in the early 1900s, of the ego as the autonomous agent of the psyche.  Hence in his seminal 1949 essay, ""The Mirror Stage as Formative of the 'I' Function as Revealed in the Psychoanalytic Experience,"" Lacan variously describes the formation and consequent function of the ego as a schismatic event of misrecognition, an epiphenomenon of narcissism, and a process of dialectical tension.  That process sets up an interchange between identification and alienation, generating confusion between being a subject as opposed to an object, between self and other, between what is ""me"" and ""not me.""
The drama of the ego's emergence in what Lacan has designated ""the Mirror Stage"" explains its contrary nature.  Somewhere between 6 and 18 months of age, an infant will likely encounter his (or her) own reflection in a mirror.  At this phase of development, the child lacks any sense of a unified self.  However, when encouraged by an adult, he is able to recognize his reflection in the glass and, at the same time, comes to understand that the specular image looking back at him is, in fact, not himself.  In other words, the image provides the infant with the first glimpse of himself as an object, as ""other.""  According to Lacan, the initial fascination and pleasure the child experiences upon self-recognition dissolves into confusion at being unable to distinguish between what is and what is not himself.  Therefore, such recognition amounts to a form of misrecognition, the French term for which, méconnaissance, implies an essential misunderstanding or lack of knowledge.
Furthermore, while the reflection in the mirror appears coherent, coordinated, and whole, the child himself continues to perceive his body as uncoordinated and fragmentary, resulting in a profound sense of incongruity and disharmony.  By exuding mastery and harmony, the mirror image functions as a gestalt: it holds up an ""Ideal-I"" or future self that promises completeness and perfection—a condition, Lacan believes, that is ultimately unattainable, though we will likely spend the rest of our lives in pursuit of it.  Overall, the infant's reaction to the image or Ideal-I is narcissistic; he is attracted to it as a model but feels aggression toward it as a rival.
Alienation, lack, absence, and conflict are thus constitutive of the Lacanian ego.  The child's image—and, hence, the emerging ego—marks an ontological gap that turns into a repository for the projections and desires, whether conscious or unconscious, of parents and others.  Consequently, this image, as Adrian Johnston notes, is ""always already overwritten"" with words that are not the child's own.  In Johnston's view, the ego could be more properly described as ""a coagulation of inter- and trans-subjective alien influences.""  Rather than a ""fluid and autonomous subject,"" it becomes a rigid and ""heteronomous"" entity.  Derived from the desires of others, the ego is, in Lacanian terms, ""extimate,"" or internally exterior.  Both alien and alienating, it is an irreducible contradiction.
This ego paradigm diverges from the more familiar and cohesive Freudian notion of the ego as the sovereign organizer of the personality, the rational mediator between internal drives and social pressures that in the interests of self-preservation will also resort to deceptions and defense mechanisms.  From a Lacanian perspective, the ego is intrinsically and irreparably divided; it chiefly serves to support the fictional construct we identify—or rather, misrecognize—as the ""self.""",Critical Analysis & Reasoning Skills,22,Social Sciences
20,Passage Title: The Mirror Stage,"Certain schools of thought that arose in the mid- to late 20th century and have since been labeled ""poststructuralist"" are well known for their propensity to ""problematize,"" ""destabilize,"" or ""radicalize"" the concepts of previous thinkers.  For example, the controversial French poststructuralist and psychoanalyst Jacques Lacan sought to reenvision the concept, first introduced by Freud in the early 1900s, of the ego as the autonomous agent of the psyche.  Hence in his seminal 1949 essay, ""The Mirror Stage as Formative of the 'I' Function as Revealed in the Psychoanalytic Experience,"" Lacan variously describes the formation and consequent function of the ego as a schismatic event of misrecognition, an epiphenomenon of narcissism, and a process of dialectical tension.  That process sets up an interchange between identification and alienation, generating confusion between being a subject as opposed to an object, between self and other, between what is ""me"" and ""not me.""
The drama of the ego's emergence in what Lacan has designated ""the Mirror Stage"" explains its contrary nature.  Somewhere between 6 and 18 months of age, an infant will likely encounter his (or her) own reflection in a mirror.  At this phase of development, the child lacks any sense of a unified self.  However, when encouraged by an adult, he is able to recognize his reflection in the glass and, at the same time, comes to understand that the specular image looking back at him is, in fact, not himself.  In other words, the image provides the infant with the first glimpse of himself as an object, as ""other.""  According to Lacan, the initial fascination and pleasure the child experiences upon self-recognition dissolves into confusion at being unable to distinguish between what is and what is not himself.  Therefore, such recognition amounts to a form of misrecognition, the French term for which, méconnaissance, implies an essential misunderstanding or lack of knowledge.
Furthermore, while the reflection in the mirror appears coherent, coordinated, and whole, the child himself continues to perceive his body as uncoordinated and fragmentary, resulting in a profound sense of incongruity and disharmony.  By exuding mastery and harmony, the mirror image functions as a gestalt: it holds up an ""Ideal-I"" or future self that promises completeness and perfection—a condition, Lacan believes, that is ultimately unattainable, though we will likely spend the rest of our lives in pursuit of it.  Overall, the infant's reaction to the image or Ideal-I is narcissistic; he is attracted to it as a model but feels aggression toward it as a rival.
Alienation, lack, absence, and conflict are thus constitutive of the Lacanian ego.  The child's image—and, hence, the emerging ego—marks an ontological gap that turns into a repository for the projections and desires, whether conscious or unconscious, of parents and others.  Consequently, this image, as Adrian Johnston notes, is ""always already overwritten"" with words that are not the child's own.  In Johnston's view, the ego could be more properly described as ""a coagulation of inter- and trans-subjective alien influences.""  Rather than a ""fluid and autonomous subject,"" it becomes a rigid and ""heteronomous"" entity.  Derived from the desires of others, the ego is, in Lacanian terms, ""extimate,"" or internally exterior.  Both alien and alienating, it is an irreducible contradiction.
This ego paradigm diverges from the more familiar and cohesive Freudian notion of the ego as the sovereign organizer of the personality, the rational mediator between internal drives and social pressures that in the interests of self-preservation will also resort to deceptions and defense mechanisms.  From a Lacanian perspective, the ego is intrinsically and irreparably divided; it chiefly serves to support the fictional construct we identify—or rather, misrecognize—as the ""self.""",Critical Analysis & Reasoning Skills,22,Social Sciences
21,Passage Title: The Mirror Stage,"Certain schools of thought that arose in the mid- to late 20th century and have since been labeled ""poststructuralist"" are well known for their propensity to ""problematize,"" ""destabilize,"" or ""radicalize"" the concepts of previous thinkers.  For example, the controversial French poststructuralist and psychoanalyst Jacques Lacan sought to reenvision the concept, first introduced by Freud in the early 1900s, of the ego as the autonomous agent of the psyche.  Hence in his seminal 1949 essay, ""The Mirror Stage as Formative of the 'I' Function as Revealed in the Psychoanalytic Experience,"" Lacan variously describes the formation and consequent function of the ego as a schismatic event of misrecognition, an epiphenomenon of narcissism, and a process of dialectical tension.  That process sets up an interchange between identification and alienation, generating confusion between being a subject as opposed to an object, between self and other, between what is ""me"" and ""not me.""
The drama of the ego's emergence in what Lacan has designated ""the Mirror Stage"" explains its contrary nature.  Somewhere between 6 and 18 months of age, an infant will likely encounter his (or her) own reflection in a mirror.  At this phase of development, the child lacks any sense of a unified self.  However, when encouraged by an adult, he is able to recognize his reflection in the glass and, at the same time, comes to understand that the specular image looking back at him is, in fact, not himself.  In other words, the image provides the infant with the first glimpse of himself as an object, as ""other.""  According to Lacan, the initial fascination and pleasure the child experiences upon self-recognition dissolves into confusion at being unable to distinguish between what is and what is not himself.  Therefore, such recognition amounts to a form of misrecognition, the French term for which, méconnaissance, implies an essential misunderstanding or lack of knowledge.
Furthermore, while the reflection in the mirror appears coherent, coordinated, and whole, the child himself continues to perceive his body as uncoordinated and fragmentary, resulting in a profound sense of incongruity and disharmony.  By exuding mastery and harmony, the mirror image functions as a gestalt: it holds up an ""Ideal-I"" or future self that promises completeness and perfection—a condition, Lacan believes, that is ultimately unattainable, though we will likely spend the rest of our lives in pursuit of it.  Overall, the infant's reaction to the image or Ideal-I is narcissistic; he is attracted to it as a model but feels aggression toward it as a rival.
Alienation, lack, absence, and conflict are thus constitutive of the Lacanian ego.  The child's image—and, hence, the emerging ego—marks an ontological gap that turns into a repository for the projections and desires, whether conscious or unconscious, of parents and others.  Consequently, this image, as Adrian Johnston notes, is ""always already overwritten"" with words that are not the child's own.  In Johnston's view, the ego could be more properly described as ""a coagulation of inter- and trans-subjective alien influences.""  Rather than a ""fluid and autonomous subject,"" it becomes a rigid and ""heteronomous"" entity.  Derived from the desires of others, the ego is, in Lacanian terms, ""extimate,"" or internally exterior.  Both alien and alienating, it is an irreducible contradiction.
This ego paradigm diverges from the more familiar and cohesive Freudian notion of the ego as the sovereign organizer of the personality, the rational mediator between internal drives and social pressures that in the interests of self-preservation will also resort to deceptions and defense mechanisms.  From a Lacanian perspective, the ego is intrinsically and irreparably divided; it chiefly serves to support the fictional construct we identify—or rather, misrecognize—as the ""self.""",Critical Analysis & Reasoning Skills,22,Social Sciences
22,Passage Title: The Mirror Stage,"Certain schools of thought that arose in the mid- to late 20th century and have since been labeled ""poststructuralist"" are well known for their propensity to ""problematize,"" ""destabilize,"" or ""radicalize"" the concepts of previous thinkers.  For example, the controversial French poststructuralist and psychoanalyst Jacques Lacan sought to reenvision the concept, first introduced by Freud in the early 1900s, of the ego as the autonomous agent of the psyche.  Hence in his seminal 1949 essay, ""The Mirror Stage as Formative of the 'I' Function as Revealed in the Psychoanalytic Experience,"" Lacan variously describes the formation and consequent function of the ego as a schismatic event of misrecognition, an epiphenomenon of narcissism, and a process of dialectical tension.  That process sets up an interchange between identification and alienation, generating confusion between being a subject as opposed to an object, between self and other, between what is ""me"" and ""not me.""
The drama of the ego's emergence in what Lacan has designated ""the Mirror Stage"" explains its contrary nature.  Somewhere between 6 and 18 months of age, an infant will likely encounter his (or her) own reflection in a mirror.  At this phase of development, the child lacks any sense of a unified self.  However, when encouraged by an adult, he is able to recognize his reflection in the glass and, at the same time, comes to understand that the specular image looking back at him is, in fact, not himself.  In other words, the image provides the infant with the first glimpse of himself as an object, as ""other.""  According to Lacan, the initial fascination and pleasure the child experiences upon self-recognition dissolves into confusion at being unable to distinguish between what is and what is not himself.  Therefore, such recognition amounts to a form of misrecognition, the French term for which, méconnaissance, implies an essential misunderstanding or lack of knowledge.
Furthermore, while the reflection in the mirror appears coherent, coordinated, and whole, the child himself continues to perceive his body as uncoordinated and fragmentary, resulting in a profound sense of incongruity and disharmony.  By exuding mastery and harmony, the mirror image functions as a gestalt: it holds up an ""Ideal-I"" or future self that promises completeness and perfection—a condition, Lacan believes, that is ultimately unattainable, though we will likely spend the rest of our lives in pursuit of it.  Overall, the infant's reaction to the image or Ideal-I is narcissistic; he is attracted to it as a model but feels aggression toward it as a rival.
Alienation, lack, absence, and conflict are thus constitutive of the Lacanian ego.  The child's image—and, hence, the emerging ego—marks an ontological gap that turns into a repository for the projections and desires, whether conscious or unconscious, of parents and others.  Consequently, this image, as Adrian Johnston notes, is ""always already overwritten"" with words that are not the child's own.  In Johnston's view, the ego could be more properly described as ""a coagulation of inter- and trans-subjective alien influences.""  Rather than a ""fluid and autonomous subject,"" it becomes a rigid and ""heteronomous"" entity.  Derived from the desires of others, the ego is, in Lacanian terms, ""extimate,"" or internally exterior.  Both alien and alienating, it is an irreducible contradiction.
This ego paradigm diverges from the more familiar and cohesive Freudian notion of the ego as the sovereign organizer of the personality, the rational mediator between internal drives and social pressures that in the interests of self-preservation will also resort to deceptions and defense mechanisms.  From a Lacanian perspective, the ego is intrinsically and irreparably divided; it chiefly serves to support the fictional construct we identify—or rather, misrecognize—as the ""self.""",Critical Analysis & Reasoning Skills,22,Social Sciences
23,"Passage Title: ""Resistance to Reduction"": William James and The Varieties of Religious Experience","Well known for his radical empiricism, pluralism, and pragmatism, William James fervently defended the primacy of raw experience against the reductive theorizing of philosophy.  Commonly considered the founder of American psychology, the brother of novelist Henry James favored what might be deemed the ""thickness"" of concrete reality over the thinness of abstraction.  Hence, he subscribed to a principle he called ""total acceptance"" that welcomed ""the contents of every kind of experience.""  In yearning to restore to reality ""all its vagueness and unkemptness,"" he contended that nothing should be subordinated to theories or facilely explained away.  It was with this mindset that James agreed to deliver the Gifford Lectures on religion—later published as The Varieties of Religious Experience: A Study in Human Nature—at the University of Edinburgh in 1901.
From the start, James conceived of his study as a psychological inquiry, declaring that ""religious feelings and religious impulses must be its subject.""  Consequently, he ""propose[d] to ignore the institutional branch"" of religion, confining himself as far as possible to ""personal religion pure and simple.""  Furthermore, in this unique exploration, James primarily endeavored to prove that personal experience—that is, the inner religious life of the individual—was the scaffolding upon which the outer religious life of the world was built.  In other words, the power of organized religion was derived from direct, personal communications from the divine.  Therefore, rather than analyze the fixed patterns found in theologies or doctrines, James decided to examine the personal experiences from which such patterns had originally been instituted.  Having discovered accounts of such experiences in literature and biography, he sorted them into broad categories.  Amongst these classifications, he identified the mystical state of feeling at one with the divine as the central experience, referring to it as ""the mother-sea and fountain-head of all religion.""
Nevertheless, James—whose ancestors had been strict Calvinists, whose father had been an eccentric and erratic follower of obscure, utopian sects, and who had himself delved into spiritualism—denied there was any such thing as ""the religious sentiment,"" elementary religious emotion, formula, or essence.  Such oversimplifications and summations were to him suspect, if not dangerous.  As historian Jacques Barzun observes, ""this Jamesian resistance to reduction"" caused the radical empiricist to champion personal religious experience.  However, James did admit that, seduced by the commonalities in the narratives, he was at times tempted to develop a science of religion.  Yet, where other thinkers equated the ultimate nature of reality with monism, James insisted on a plurality of individual realities that overlapped but never merged to form a single larger reality.  Thus, rather than marshal a sustained argument in The Varieties, he carefully compiled personal testimonies of divine encounters, allowing a plethora of different voices to speak on the nature of such experience.
In addition to emphasizing the empirical and pluralistic aspects of religious experience, James took a pragmatic approach that evaluated such encounters according to their consequences.  In particular, he looked at the beliefs they engendered and how these were manifested in the life of the belief-holder.  He determined that the mystical state was the most profoundly transformative of religious experiences.  To the myriad detractors, doubters, and reductionists who would dismiss such states as forms of hysteria or side effects of superstition, he responded that ""the most important way to discern the real from the unreal"" and ""to distinguish pathology from truly divine states"" was not to expose their ""root causes"" but to assess the ""fruits"" they yielded.
Hence, by wielding what Eugene Taylor terms a ""final tripartite metaphysics of radical empiricism, pluralism, and pragmatism,"" James rescued religious feelings and impulses from reductive thinking and so defended religious experience against philosophical theorizing.",Critical Analysis & Reasoning Skills,27,Social Sciences
24,"Passage Title: ""Resistance to Reduction"": William James and The Varieties of Religious Experience","Well known for his radical empiricism, pluralism, and pragmatism, William James fervently defended the primacy of raw experience against the reductive theorizing of philosophy.  Commonly considered the founder of American psychology, the brother of novelist Henry James favored what might be deemed the ""thickness"" of concrete reality over the thinness of abstraction.  Hence, he subscribed to a principle he called ""total acceptance"" that welcomed ""the contents of every kind of experience.""  In yearning to restore to reality ""all its vagueness and unkemptness,"" he contended that nothing should be subordinated to theories or facilely explained away.  It was with this mindset that James agreed to deliver the Gifford Lectures on religion—later published as The Varieties of Religious Experience: A Study in Human Nature—at the University of Edinburgh in 1901.
From the start, James conceived of his study as a psychological inquiry, declaring that ""religious feelings and religious impulses must be its subject.""  Consequently, he ""propose[d] to ignore the institutional branch"" of religion, confining himself as far as possible to ""personal religion pure and simple.""  Furthermore, in this unique exploration, James primarily endeavored to prove that personal experience—that is, the inner religious life of the individual—was the scaffolding upon which the outer religious life of the world was built.  In other words, the power of organized religion was derived from direct, personal communications from the divine.  Therefore, rather than analyze the fixed patterns found in theologies or doctrines, James decided to examine the personal experiences from which such patterns had originally been instituted.  Having discovered accounts of such experiences in literature and biography, he sorted them into broad categories.  Amongst these classifications, he identified the mystical state of feeling at one with the divine as the central experience, referring to it as ""the mother-sea and fountain-head of all religion.""
Nevertheless, James—whose ancestors had been strict Calvinists, whose father had been an eccentric and erratic follower of obscure, utopian sects, and who had himself delved into spiritualism—denied there was any such thing as ""the religious sentiment,"" elementary religious emotion, formula, or essence.  Such oversimplifications and summations were to him suspect, if not dangerous.  As historian Jacques Barzun observes, ""this Jamesian resistance to reduction"" caused the radical empiricist to champion personal religious experience.  However, James did admit that, seduced by the commonalities in the narratives, he was at times tempted to develop a science of religion.  Yet, where other thinkers equated the ultimate nature of reality with monism, James insisted on a plurality of individual realities that overlapped but never merged to form a single larger reality.  Thus, rather than marshal a sustained argument in The Varieties, he carefully compiled personal testimonies of divine encounters, allowing a plethora of different voices to speak on the nature of such experience.
In addition to emphasizing the empirical and pluralistic aspects of religious experience, James took a pragmatic approach that evaluated such encounters according to their consequences.  In particular, he looked at the beliefs they engendered and how these were manifested in the life of the belief-holder.  He determined that the mystical state was the most profoundly transformative of religious experiences.  To the myriad detractors, doubters, and reductionists who would dismiss such states as forms of hysteria or side effects of superstition, he responded that ""the most important way to discern the real from the unreal"" and ""to distinguish pathology from truly divine states"" was not to expose their ""root causes"" but to assess the ""fruits"" they yielded.
Hence, by wielding what Eugene Taylor terms a ""final tripartite metaphysics of radical empiricism, pluralism, and pragmatism,"" James rescued religious feelings and impulses from reductive thinking and so defended religious experience against philosophical theorizing.",Critical Analysis & Reasoning Skills,27,Social Sciences
25,"Passage Title: ""Resistance to Reduction"": William James and The Varieties of Religious Experience","Well known for his radical empiricism, pluralism, and pragmatism, William James fervently defended the primacy of raw experience against the reductive theorizing of philosophy.  Commonly considered the founder of American psychology, the brother of novelist Henry James favored what might be deemed the ""thickness"" of concrete reality over the thinness of abstraction.  Hence, he subscribed to a principle he called ""total acceptance"" that welcomed ""the contents of every kind of experience.""  In yearning to restore to reality ""all its vagueness and unkemptness,"" he contended that nothing should be subordinated to theories or facilely explained away.  It was with this mindset that James agreed to deliver the Gifford Lectures on religion—later published as The Varieties of Religious Experience: A Study in Human Nature—at the University of Edinburgh in 1901.
From the start, James conceived of his study as a psychological inquiry, declaring that ""religious feelings and religious impulses must be its subject.""  Consequently, he ""propose[d] to ignore the institutional branch"" of religion, confining himself as far as possible to ""personal religion pure and simple.""  Furthermore, in this unique exploration, James primarily endeavored to prove that personal experience—that is, the inner religious life of the individual—was the scaffolding upon which the outer religious life of the world was built.  In other words, the power of organized religion was derived from direct, personal communications from the divine.  Therefore, rather than analyze the fixed patterns found in theologies or doctrines, James decided to examine the personal experiences from which such patterns had originally been instituted.  Having discovered accounts of such experiences in literature and biography, he sorted them into broad categories.  Amongst these classifications, he identified the mystical state of feeling at one with the divine as the central experience, referring to it as ""the mother-sea and fountain-head of all religion.""
Nevertheless, James—whose ancestors had been strict Calvinists, whose father had been an eccentric and erratic follower of obscure, utopian sects, and who had himself delved into spiritualism—denied there was any such thing as ""the religious sentiment,"" elementary religious emotion, formula, or essence.  Such oversimplifications and summations were to him suspect, if not dangerous.  As historian Jacques Barzun observes, ""this Jamesian resistance to reduction"" caused the radical empiricist to champion personal religious experience.  However, James did admit that, seduced by the commonalities in the narratives, he was at times tempted to develop a science of religion.  Yet, where other thinkers equated the ultimate nature of reality with monism, James insisted on a plurality of individual realities that overlapped but never merged to form a single larger reality.  Thus, rather than marshal a sustained argument in The Varieties, he carefully compiled personal testimonies of divine encounters, allowing a plethora of different voices to speak on the nature of such experience.
In addition to emphasizing the empirical and pluralistic aspects of religious experience, James took a pragmatic approach that evaluated such encounters according to their consequences.  In particular, he looked at the beliefs they engendered and how these were manifested in the life of the belief-holder.  He determined that the mystical state was the most profoundly transformative of religious experiences.  To the myriad detractors, doubters, and reductionists who would dismiss such states as forms of hysteria or side effects of superstition, he responded that ""the most important way to discern the real from the unreal"" and ""to distinguish pathology from truly divine states"" was not to expose their ""root causes"" but to assess the ""fruits"" they yielded.
Hence, by wielding what Eugene Taylor terms a ""final tripartite metaphysics of radical empiricism, pluralism, and pragmatism,"" James rescued religious feelings and impulses from reductive thinking and so defended religious experience against philosophical theorizing.",Critical Analysis & Reasoning Skills,27,Social Sciences
26,"Passage Title: ""Resistance to Reduction"": William James and The Varieties of Religious Experience","Well known for his radical empiricism, pluralism, and pragmatism, William James fervently defended the primacy of raw experience against the reductive theorizing of philosophy.  Commonly considered the founder of American psychology, the brother of novelist Henry James favored what might be deemed the ""thickness"" of concrete reality over the thinness of abstraction.  Hence, he subscribed to a principle he called ""total acceptance"" that welcomed ""the contents of every kind of experience.""  In yearning to restore to reality ""all its vagueness and unkemptness,"" he contended that nothing should be subordinated to theories or facilely explained away.  It was with this mindset that James agreed to deliver the Gifford Lectures on religion—later published as The Varieties of Religious Experience: A Study in Human Nature—at the University of Edinburgh in 1901.
From the start, James conceived of his study as a psychological inquiry, declaring that ""religious feelings and religious impulses must be its subject.""  Consequently, he ""propose[d] to ignore the institutional branch"" of religion, confining himself as far as possible to ""personal religion pure and simple.""  Furthermore, in this unique exploration, James primarily endeavored to prove that personal experience—that is, the inner religious life of the individual—was the scaffolding upon which the outer religious life of the world was built.  In other words, the power of organized religion was derived from direct, personal communications from the divine.  Therefore, rather than analyze the fixed patterns found in theologies or doctrines, James decided to examine the personal experiences from which such patterns had originally been instituted.  Having discovered accounts of such experiences in literature and biography, he sorted them into broad categories.  Amongst these classifications, he identified the mystical state of feeling at one with the divine as the central experience, referring to it as ""the mother-sea and fountain-head of all religion.""
Nevertheless, James—whose ancestors had been strict Calvinists, whose father had been an eccentric and erratic follower of obscure, utopian sects, and who had himself delved into spiritualism—denied there was any such thing as ""the religious sentiment,"" elementary religious emotion, formula, or essence.  Such oversimplifications and summations were to him suspect, if not dangerous.  As historian Jacques Barzun observes, ""this Jamesian resistance to reduction"" caused the radical empiricist to champion personal religious experience.  However, James did admit that, seduced by the commonalities in the narratives, he was at times tempted to develop a science of religion.  Yet, where other thinkers equated the ultimate nature of reality with monism, James insisted on a plurality of individual realities that overlapped but never merged to form a single larger reality.  Thus, rather than marshal a sustained argument in The Varieties, he carefully compiled personal testimonies of divine encounters, allowing a plethora of different voices to speak on the nature of such experience.
In addition to emphasizing the empirical and pluralistic aspects of religious experience, James took a pragmatic approach that evaluated such encounters according to their consequences.  In particular, he looked at the beliefs they engendered and how these were manifested in the life of the belief-holder.  He determined that the mystical state was the most profoundly transformative of religious experiences.  To the myriad detractors, doubters, and reductionists who would dismiss such states as forms of hysteria or side effects of superstition, he responded that ""the most important way to discern the real from the unreal"" and ""to distinguish pathology from truly divine states"" was not to expose their ""root causes"" but to assess the ""fruits"" they yielded.
Hence, by wielding what Eugene Taylor terms a ""final tripartite metaphysics of radical empiricism, pluralism, and pragmatism,"" James rescued religious feelings and impulses from reductive thinking and so defended religious experience against philosophical theorizing.",Critical Analysis & Reasoning Skills,27,Social Sciences
27,"Passage Title: ""Resistance to Reduction"": William James and The Varieties of Religious Experience","Well known for his radical empiricism, pluralism, and pragmatism, William James fervently defended the primacy of raw experience against the reductive theorizing of philosophy.  Commonly considered the founder of American psychology, the brother of novelist Henry James favored what might be deemed the ""thickness"" of concrete reality over the thinness of abstraction.  Hence, he subscribed to a principle he called ""total acceptance"" that welcomed ""the contents of every kind of experience.""  In yearning to restore to reality ""all its vagueness and unkemptness,"" he contended that nothing should be subordinated to theories or facilely explained away.  It was with this mindset that James agreed to deliver the Gifford Lectures on religion—later published as The Varieties of Religious Experience: A Study in Human Nature—at the University of Edinburgh in 1901.
From the start, James conceived of his study as a psychological inquiry, declaring that ""religious feelings and religious impulses must be its subject.""  Consequently, he ""propose[d] to ignore the institutional branch"" of religion, confining himself as far as possible to ""personal religion pure and simple.""  Furthermore, in this unique exploration, James primarily endeavored to prove that personal experience—that is, the inner religious life of the individual—was the scaffolding upon which the outer religious life of the world was built.  In other words, the power of organized religion was derived from direct, personal communications from the divine.  Therefore, rather than analyze the fixed patterns found in theologies or doctrines, James decided to examine the personal experiences from which such patterns had originally been instituted.  Having discovered accounts of such experiences in literature and biography, he sorted them into broad categories.  Amongst these classifications, he identified the mystical state of feeling at one with the divine as the central experience, referring to it as ""the mother-sea and fountain-head of all religion.""
Nevertheless, James—whose ancestors had been strict Calvinists, whose father had been an eccentric and erratic follower of obscure, utopian sects, and who had himself delved into spiritualism—denied there was any such thing as ""the religious sentiment,"" elementary religious emotion, formula, or essence.  Such oversimplifications and summations were to him suspect, if not dangerous.  As historian Jacques Barzun observes, ""this Jamesian resistance to reduction"" caused the radical empiricist to champion personal religious experience.  However, James did admit that, seduced by the commonalities in the narratives, he was at times tempted to develop a science of religion.  Yet, where other thinkers equated the ultimate nature of reality with monism, James insisted on a plurality of individual realities that overlapped but never merged to form a single larger reality.  Thus, rather than marshal a sustained argument in The Varieties, he carefully compiled personal testimonies of divine encounters, allowing a plethora of different voices to speak on the nature of such experience.
In addition to emphasizing the empirical and pluralistic aspects of religious experience, James took a pragmatic approach that evaluated such encounters according to their consequences.  In particular, he looked at the beliefs they engendered and how these were manifested in the life of the belief-holder.  He determined that the mystical state was the most profoundly transformative of religious experiences.  To the myriad detractors, doubters, and reductionists who would dismiss such states as forms of hysteria or side effects of superstition, he responded that ""the most important way to discern the real from the unreal"" and ""to distinguish pathology from truly divine states"" was not to expose their ""root causes"" but to assess the ""fruits"" they yielded.
Hence, by wielding what Eugene Taylor terms a ""final tripartite metaphysics of radical empiricism, pluralism, and pragmatism,"" James rescued religious feelings and impulses from reductive thinking and so defended religious experience against philosophical theorizing.",Critical Analysis & Reasoning Skills,27,Social Sciences
28,Passage Title: Blackmail: Just a Dirty Business?,"Both blackmail and extortion are crimes of coercion: one party attempts to gain by threatening action that would be injurious to another, unless a specified demand is met (usually for financial payment).  However, with extortion the threatened action itself is also illegal.  For example, suggesting that one will vandalize property unless the owner pays a fee would be extortion, because it is unlawful to damage what belongs to another.  On the other hand, promising to expose a secret unless the subject pays would generally be blackmail, because in most cases there is nothing criminal about sharing factual information.  But in light of this distinction, an interesting question arises: Why is blackmail a crime?
This question is more difficult than it may appear, and legal theorists, philosophers, and even economists have attempted to provide answers.  The ""puzzle"" or ""paradox"" of blackmail is to explain the following: How does offering to choose an otherwise legal option (eg, keeping silent), in exchange for compensation, make that choice a criminal act?  Granted, blackmail may strike us as dirty and underhanded—it feels like something that should be illegal.  For obvious reasons, however, it would be preferable if we could specify a firm basis for its criminality.
To this end, a comparison to the corporate world is helpful.  Certain types of financial coercion are a normal, expected part of business transactions; for instance, a retailer might threaten to purchase goods from a different wholesaler unless the current one lowers the price per unit.  This sort of tactic is not only legitimate but, in an overall sense, good for all parties.  If the threat to take business elsewhere had not been announced, the original wholesaler would simply lose the business.  Instead, the wholesaler can choose whether to change its terms in order to preserve the sale.
Similarly, a few scholars have argued that blackmail should be legal because it actually benefits its target.  Rather than having a secret revealed against his wishes, the target is afforded the opportunity to choose what to him is a more favorable outcome—merely paying a fee.  Thus, like the retailer above, the blackmailer has increased the options available to the ""victim,"" placing him in a better situation than he would have been otherwise.
Still, the majority position is to reject these scholars' view.  One common suggestion is that coercing someone through blackmail is illegitimate because it involves appropriating what belongs to other people.  If Adam threatens to reveal Bob's secret to Charlie, only Bob and Charlie have a natural stake in whether the information is exposed.  Adam is a third party to the situation; thus his connection to its outcome is artificial.  A variation of this view uses more rights-based language.  If a relationship exists between Bob and Charlie, then they may reasonably have certain ""claims"" to information regarding each other; by contrast, Adam's involvement is purely parasitic on that relationship.
However, the most compelling argument relates to the ongoing effects of blackmail.  Law professor Richard Epstein stresses that the ""business model"" of successful blackmail is essentially participation in a kind of fraud—not against the blackmailed party, but against the party from whom information is concealed.  If Charlie's well-being depends on whether he knows particular facts about Bob, then Bob's attempt to conceal that information is injurious to Charlie.  As such, Adam can only blackmail Bob by perpetuating Bob's deceit.  Moreover, the presence or potential of blackmail often incentivizes additional harmful or criminal activity by the perpetrator, the target, or both.  Thus, blackmail's effects are not only acute but systemic and pervasive.  As Epstein remarks, ""Blackmail is made a crime not only because of what it is, but because of what it necessarily leads to.""",Critical Analysis & Reasoning Skills,32,Social Sciences
29,Passage Title: Blackmail: Just a Dirty Business?,"Both blackmail and extortion are crimes of coercion: one party attempts to gain by threatening action that would be injurious to another, unless a specified demand is met (usually for financial payment).  However, with extortion the threatened action itself is also illegal.  For example, suggesting that one will vandalize property unless the owner pays a fee would be extortion, because it is unlawful to damage what belongs to another.  On the other hand, promising to expose a secret unless the subject pays would generally be blackmail, because in most cases there is nothing criminal about sharing factual information.  But in light of this distinction, an interesting question arises: Why is blackmail a crime?
This question is more difficult than it may appear, and legal theorists, philosophers, and even economists have attempted to provide answers.  The ""puzzle"" or ""paradox"" of blackmail is to explain the following: How does offering to choose an otherwise legal option (eg, keeping silent), in exchange for compensation, make that choice a criminal act?  Granted, blackmail may strike us as dirty and underhanded—it feels like something that should be illegal.  For obvious reasons, however, it would be preferable if we could specify a firm basis for its criminality.
To this end, a comparison to the corporate world is helpful.  Certain types of financial coercion are a normal, expected part of business transactions; for instance, a retailer might threaten to purchase goods from a different wholesaler unless the current one lowers the price per unit.  This sort of tactic is not only legitimate but, in an overall sense, good for all parties.  If the threat to take business elsewhere had not been announced, the original wholesaler would simply lose the business.  Instead, the wholesaler can choose whether to change its terms in order to preserve the sale.
Similarly, a few scholars have argued that blackmail should be legal because it actually benefits its target.  Rather than having a secret revealed against his wishes, the target is afforded the opportunity to choose what to him is a more favorable outcome—merely paying a fee.  Thus, like the retailer above, the blackmailer has increased the options available to the ""victim,"" placing him in a better situation than he would have been otherwise.
Still, the majority position is to reject these scholars' view.  One common suggestion is that coercing someone through blackmail is illegitimate because it involves appropriating what belongs to other people.  If Adam threatens to reveal Bob's secret to Charlie, only Bob and Charlie have a natural stake in whether the information is exposed.  Adam is a third party to the situation; thus his connection to its outcome is artificial.  A variation of this view uses more rights-based language.  If a relationship exists between Bob and Charlie, then they may reasonably have certain ""claims"" to information regarding each other; by contrast, Adam's involvement is purely parasitic on that relationship.
However, the most compelling argument relates to the ongoing effects of blackmail.  Law professor Richard Epstein stresses that the ""business model"" of successful blackmail is essentially participation in a kind of fraud—not against the blackmailed party, but against the party from whom information is concealed.  If Charlie's well-being depends on whether he knows particular facts about Bob, then Bob's attempt to conceal that information is injurious to Charlie.  As such, Adam can only blackmail Bob by perpetuating Bob's deceit.  Moreover, the presence or potential of blackmail often incentivizes additional harmful or criminal activity by the perpetrator, the target, or both.  Thus, blackmail's effects are not only acute but systemic and pervasive.  As Epstein remarks, ""Blackmail is made a crime not only because of what it is, but because of what it necessarily leads to.""",Critical Analysis & Reasoning Skills,32,Social Sciences
30,Passage Title: Blackmail: Just a Dirty Business?,"Both blackmail and extortion are crimes of coercion: one party attempts to gain by threatening action that would be injurious to another, unless a specified demand is met (usually for financial payment).  However, with extortion the threatened action itself is also illegal.  For example, suggesting that one will vandalize property unless the owner pays a fee would be extortion, because it is unlawful to damage what belongs to another.  On the other hand, promising to expose a secret unless the subject pays would generally be blackmail, because in most cases there is nothing criminal about sharing factual information.  But in light of this distinction, an interesting question arises: Why is blackmail a crime?
This question is more difficult than it may appear, and legal theorists, philosophers, and even economists have attempted to provide answers.  The ""puzzle"" or ""paradox"" of blackmail is to explain the following: How does offering to choose an otherwise legal option (eg, keeping silent), in exchange for compensation, make that choice a criminal act?  Granted, blackmail may strike us as dirty and underhanded—it feels like something that should be illegal.  For obvious reasons, however, it would be preferable if we could specify a firm basis for its criminality.
To this end, a comparison to the corporate world is helpful.  Certain types of financial coercion are a normal, expected part of business transactions; for instance, a retailer might threaten to purchase goods from a different wholesaler unless the current one lowers the price per unit.  This sort of tactic is not only legitimate but, in an overall sense, good for all parties.  If the threat to take business elsewhere had not been announced, the original wholesaler would simply lose the business.  Instead, the wholesaler can choose whether to change its terms in order to preserve the sale.
Similarly, a few scholars have argued that blackmail should be legal because it actually benefits its target.  Rather than having a secret revealed against his wishes, the target is afforded the opportunity to choose what to him is a more favorable outcome—merely paying a fee.  Thus, like the retailer above, the blackmailer has increased the options available to the ""victim,"" placing him in a better situation than he would have been otherwise.
Still, the majority position is to reject these scholars' view.  One common suggestion is that coercing someone through blackmail is illegitimate because it involves appropriating what belongs to other people.  If Adam threatens to reveal Bob's secret to Charlie, only Bob and Charlie have a natural stake in whether the information is exposed.  Adam is a third party to the situation; thus his connection to its outcome is artificial.  A variation of this view uses more rights-based language.  If a relationship exists between Bob and Charlie, then they may reasonably have certain ""claims"" to information regarding each other; by contrast, Adam's involvement is purely parasitic on that relationship.
However, the most compelling argument relates to the ongoing effects of blackmail.  Law professor Richard Epstein stresses that the ""business model"" of successful blackmail is essentially participation in a kind of fraud—not against the blackmailed party, but against the party from whom information is concealed.  If Charlie's well-being depends on whether he knows particular facts about Bob, then Bob's attempt to conceal that information is injurious to Charlie.  As such, Adam can only blackmail Bob by perpetuating Bob's deceit.  Moreover, the presence or potential of blackmail often incentivizes additional harmful or criminal activity by the perpetrator, the target, or both.  Thus, blackmail's effects are not only acute but systemic and pervasive.  As Epstein remarks, ""Blackmail is made a crime not only because of what it is, but because of what it necessarily leads to.""",Critical Analysis & Reasoning Skills,32,Social Sciences
31,Passage Title: Blackmail: Just a Dirty Business?,"Both blackmail and extortion are crimes of coercion: one party attempts to gain by threatening action that would be injurious to another, unless a specified demand is met (usually for financial payment).  However, with extortion the threatened action itself is also illegal.  For example, suggesting that one will vandalize property unless the owner pays a fee would be extortion, because it is unlawful to damage what belongs to another.  On the other hand, promising to expose a secret unless the subject pays would generally be blackmail, because in most cases there is nothing criminal about sharing factual information.  But in light of this distinction, an interesting question arises: Why is blackmail a crime?
This question is more difficult than it may appear, and legal theorists, philosophers, and even economists have attempted to provide answers.  The ""puzzle"" or ""paradox"" of blackmail is to explain the following: How does offering to choose an otherwise legal option (eg, keeping silent), in exchange for compensation, make that choice a criminal act?  Granted, blackmail may strike us as dirty and underhanded—it feels like something that should be illegal.  For obvious reasons, however, it would be preferable if we could specify a firm basis for its criminality.
To this end, a comparison to the corporate world is helpful.  Certain types of financial coercion are a normal, expected part of business transactions; for instance, a retailer might threaten to purchase goods from a different wholesaler unless the current one lowers the price per unit.  This sort of tactic is not only legitimate but, in an overall sense, good for all parties.  If the threat to take business elsewhere had not been announced, the original wholesaler would simply lose the business.  Instead, the wholesaler can choose whether to change its terms in order to preserve the sale.
Similarly, a few scholars have argued that blackmail should be legal because it actually benefits its target.  Rather than having a secret revealed against his wishes, the target is afforded the opportunity to choose what to him is a more favorable outcome—merely paying a fee.  Thus, like the retailer above, the blackmailer has increased the options available to the ""victim,"" placing him in a better situation than he would have been otherwise.
Still, the majority position is to reject these scholars' view.  One common suggestion is that coercing someone through blackmail is illegitimate because it involves appropriating what belongs to other people.  If Adam threatens to reveal Bob's secret to Charlie, only Bob and Charlie have a natural stake in whether the information is exposed.  Adam is a third party to the situation; thus his connection to its outcome is artificial.  A variation of this view uses more rights-based language.  If a relationship exists between Bob and Charlie, then they may reasonably have certain ""claims"" to information regarding each other; by contrast, Adam's involvement is purely parasitic on that relationship.
However, the most compelling argument relates to the ongoing effects of blackmail.  Law professor Richard Epstein stresses that the ""business model"" of successful blackmail is essentially participation in a kind of fraud—not against the blackmailed party, but against the party from whom information is concealed.  If Charlie's well-being depends on whether he knows particular facts about Bob, then Bob's attempt to conceal that information is injurious to Charlie.  As such, Adam can only blackmail Bob by perpetuating Bob's deceit.  Moreover, the presence or potential of blackmail often incentivizes additional harmful or criminal activity by the perpetrator, the target, or both.  Thus, blackmail's effects are not only acute but systemic and pervasive.  As Epstein remarks, ""Blackmail is made a crime not only because of what it is, but because of what it necessarily leads to.""",Critical Analysis & Reasoning Skills,32,Social Sciences
32,Passage Title: Blackmail: Just a Dirty Business?,"Both blackmail and extortion are crimes of coercion: one party attempts to gain by threatening action that would be injurious to another, unless a specified demand is met (usually for financial payment).  However, with extortion the threatened action itself is also illegal.  For example, suggesting that one will vandalize property unless the owner pays a fee would be extortion, because it is unlawful to damage what belongs to another.  On the other hand, promising to expose a secret unless the subject pays would generally be blackmail, because in most cases there is nothing criminal about sharing factual information.  But in light of this distinction, an interesting question arises: Why is blackmail a crime?
This question is more difficult than it may appear, and legal theorists, philosophers, and even economists have attempted to provide answers.  The ""puzzle"" or ""paradox"" of blackmail is to explain the following: How does offering to choose an otherwise legal option (eg, keeping silent), in exchange for compensation, make that choice a criminal act?  Granted, blackmail may strike us as dirty and underhanded—it feels like something that should be illegal.  For obvious reasons, however, it would be preferable if we could specify a firm basis for its criminality.
To this end, a comparison to the corporate world is helpful.  Certain types of financial coercion are a normal, expected part of business transactions; for instance, a retailer might threaten to purchase goods from a different wholesaler unless the current one lowers the price per unit.  This sort of tactic is not only legitimate but, in an overall sense, good for all parties.  If the threat to take business elsewhere had not been announced, the original wholesaler would simply lose the business.  Instead, the wholesaler can choose whether to change its terms in order to preserve the sale.
Similarly, a few scholars have argued that blackmail should be legal because it actually benefits its target.  Rather than having a secret revealed against his wishes, the target is afforded the opportunity to choose what to him is a more favorable outcome—merely paying a fee.  Thus, like the retailer above, the blackmailer has increased the options available to the ""victim,"" placing him in a better situation than he would have been otherwise.
Still, the majority position is to reject these scholars' view.  One common suggestion is that coercing someone through blackmail is illegitimate because it involves appropriating what belongs to other people.  If Adam threatens to reveal Bob's secret to Charlie, only Bob and Charlie have a natural stake in whether the information is exposed.  Adam is a third party to the situation; thus his connection to its outcome is artificial.  A variation of this view uses more rights-based language.  If a relationship exists between Bob and Charlie, then they may reasonably have certain ""claims"" to information regarding each other; by contrast, Adam's involvement is purely parasitic on that relationship.
However, the most compelling argument relates to the ongoing effects of blackmail.  Law professor Richard Epstein stresses that the ""business model"" of successful blackmail is essentially participation in a kind of fraud—not against the blackmailed party, but against the party from whom information is concealed.  If Charlie's well-being depends on whether he knows particular facts about Bob, then Bob's attempt to conceal that information is injurious to Charlie.  As such, Adam can only blackmail Bob by perpetuating Bob's deceit.  Moreover, the presence or potential of blackmail often incentivizes additional harmful or criminal activity by the perpetrator, the target, or both.  Thus, blackmail's effects are not only acute but systemic and pervasive.  As Epstein remarks, ""Blackmail is made a crime not only because of what it is, but because of what it necessarily leads to.""",Critical Analysis & Reasoning Skills,32,Social Sciences
33,Passage Title: White House Reconstruction,"The destruction of the White House was not the fault of its original builders.  When James Hoban designed a presidential palace to reflect George Washington's vision for a residence in 1792, he upheld the best architectural standards of his day.  By the time of the 1948–1952 renovation, however, there were plenty of people to blame for the White House's condition.  A combination of poor planning, carelessness, and neglect throughout the intervening years caused a catastrophic deterioration.
Robert Klara recounts a history of changes to the White House that gradually compromised its structural integrity, beginning as far back as 1833 when Andrew Jackson had pipes installed for running water.  Subsequent presidents added modern conveniences in pace with technological developments: gas lighting for James Polk, a telephone system for Rutherford Hayes, and air conditioning for Chester Arthur, among others.  None of these advancements could have been anticipated by Hoban, and they had the dual effect of increasing the load borne by the house while decreasing its ability to bear that burden.  The builders responsible for the many additions performed ""careless and foolish maneuvers that a first-year architecture student would never have made,"" such as installing doorways in load-bearing walls or destroying the keystones of a brick arch.  Deep cuts were carved into crucial support beams to make room for pipes, wiring, and air ducts, weakening the beams and concentrating weight that would otherwise have been more evenly distributed.
By the turn of the twentieth century, the structural failings of the White House had become difficult to ignore.  The building was likely beyond saving at that point, and President Roosevelt didn't help matters by allowing architect Charles McKim only four months to make repairs.  McKim reinforced the second floor as well as he could within the time allotted, but his efforts only delayed the inevitable.  In 1925, Calvin Coolidge scoffed at reports that the White House roof was unsafe until ""[a] chunk of it broke off and bonked President Coolidge in the head.""  Poetic justice notwithstanding, the repairs to which he only then begrudgingly agreed had a disastrous side effect: the weight of the roof was shifted away from the stronger outer walls to the weaker inner ones, causing enormous structural strain.
The need to reconstruct the mansion reached a breaking point during the Harry Truman administration.  Walls continued to sink and pull away from ceilings; heavy chandeliers swung dangerously from movement on the floor above them; the entire East Room required scaffolding to prevent the collapse of its ceiling.  More dramatic incidents involved daughter Margaret Truman's piano.  Not only did the floor of her sitting room begin to sway up and down while she was playing a duet with a friend, but no steps were taken to move the piano away from the weak floor afterward.  Predictably, White House residents and staff were later horrified when one of the piano's legs crashed through, puncturing the overtaxed wood and sending debris cascading into the room below.  As for Margaret, Klara relates how she called the Steinway company to ask for ""a man who could come over to rescue a piano.""
That event was the crescendo in a long string of architectural warnings.  Ultimately, there was little choice but to tear down and rebuild the White House.  The exterior façade was preserved, as were a few of the interior's furnishings and decorations.  Still, the extensive transformation left many feeling that the White House was no longer the same.",Critical Analysis & Reasoning Skills,38,Humanities
34,Passage Title: White House Reconstruction,"The destruction of the White House was not the fault of its original builders.  When James Hoban designed a presidential palace to reflect George Washington's vision for a residence in 1792, he upheld the best architectural standards of his day.  By the time of the 1948–1952 renovation, however, there were plenty of people to blame for the White House's condition.  A combination of poor planning, carelessness, and neglect throughout the intervening years caused a catastrophic deterioration.
Robert Klara recounts a history of changes to the White House that gradually compromised its structural integrity, beginning as far back as 1833 when Andrew Jackson had pipes installed for running water.  Subsequent presidents added modern conveniences in pace with technological developments: gas lighting for James Polk, a telephone system for Rutherford Hayes, and air conditioning for Chester Arthur, among others.  None of these advancements could have been anticipated by Hoban, and they had the dual effect of increasing the load borne by the house while decreasing its ability to bear that burden.  The builders responsible for the many additions performed ""careless and foolish maneuvers that a first-year architecture student would never have made,"" such as installing doorways in load-bearing walls or destroying the keystones of a brick arch.  Deep cuts were carved into crucial support beams to make room for pipes, wiring, and air ducts, weakening the beams and concentrating weight that would otherwise have been more evenly distributed.
By the turn of the twentieth century, the structural failings of the White House had become difficult to ignore.  The building was likely beyond saving at that point, and President Roosevelt didn't help matters by allowing architect Charles McKim only four months to make repairs.  McKim reinforced the second floor as well as he could within the time allotted, but his efforts only delayed the inevitable.  In 1925, Calvin Coolidge scoffed at reports that the White House roof was unsafe until ""[a] chunk of it broke off and bonked President Coolidge in the head.""  Poetic justice notwithstanding, the repairs to which he only then begrudgingly agreed had a disastrous side effect: the weight of the roof was shifted away from the stronger outer walls to the weaker inner ones, causing enormous structural strain.
The need to reconstruct the mansion reached a breaking point during the Harry Truman administration.  Walls continued to sink and pull away from ceilings; heavy chandeliers swung dangerously from movement on the floor above them; the entire East Room required scaffolding to prevent the collapse of its ceiling.  More dramatic incidents involved daughter Margaret Truman's piano.  Not only did the floor of her sitting room begin to sway up and down while she was playing a duet with a friend, but no steps were taken to move the piano away from the weak floor afterward.  Predictably, White House residents and staff were later horrified when one of the piano's legs crashed through, puncturing the overtaxed wood and sending debris cascading into the room below.  As for Margaret, Klara relates how she called the Steinway company to ask for ""a man who could come over to rescue a piano.""
That event was the crescendo in a long string of architectural warnings.  Ultimately, there was little choice but to tear down and rebuild the White House.  The exterior façade was preserved, as were a few of the interior's furnishings and decorations.  Still, the extensive transformation left many feeling that the White House was no longer the same.",Critical Analysis & Reasoning Skills,38,Humanities
35,Passage Title: White House Reconstruction,"The destruction of the White House was not the fault of its original builders.  When James Hoban designed a presidential palace to reflect George Washington's vision for a residence in 1792, he upheld the best architectural standards of his day.  By the time of the 1948–1952 renovation, however, there were plenty of people to blame for the White House's condition.  A combination of poor planning, carelessness, and neglect throughout the intervening years caused a catastrophic deterioration.
Robert Klara recounts a history of changes to the White House that gradually compromised its structural integrity, beginning as far back as 1833 when Andrew Jackson had pipes installed for running water.  Subsequent presidents added modern conveniences in pace with technological developments: gas lighting for James Polk, a telephone system for Rutherford Hayes, and air conditioning for Chester Arthur, among others.  None of these advancements could have been anticipated by Hoban, and they had the dual effect of increasing the load borne by the house while decreasing its ability to bear that burden.  The builders responsible for the many additions performed ""careless and foolish maneuvers that a first-year architecture student would never have made,"" such as installing doorways in load-bearing walls or destroying the keystones of a brick arch.  Deep cuts were carved into crucial support beams to make room for pipes, wiring, and air ducts, weakening the beams and concentrating weight that would otherwise have been more evenly distributed.
By the turn of the twentieth century, the structural failings of the White House had become difficult to ignore.  The building was likely beyond saving at that point, and President Roosevelt didn't help matters by allowing architect Charles McKim only four months to make repairs.  McKim reinforced the second floor as well as he could within the time allotted, but his efforts only delayed the inevitable.  In 1925, Calvin Coolidge scoffed at reports that the White House roof was unsafe until ""[a] chunk of it broke off and bonked President Coolidge in the head.""  Poetic justice notwithstanding, the repairs to which he only then begrudgingly agreed had a disastrous side effect: the weight of the roof was shifted away from the stronger outer walls to the weaker inner ones, causing enormous structural strain.
The need to reconstruct the mansion reached a breaking point during the Harry Truman administration.  Walls continued to sink and pull away from ceilings; heavy chandeliers swung dangerously from movement on the floor above them; the entire East Room required scaffolding to prevent the collapse of its ceiling.  More dramatic incidents involved daughter Margaret Truman's piano.  Not only did the floor of her sitting room begin to sway up and down while she was playing a duet with a friend, but no steps were taken to move the piano away from the weak floor afterward.  Predictably, White House residents and staff were later horrified when one of the piano's legs crashed through, puncturing the overtaxed wood and sending debris cascading into the room below.  As for Margaret, Klara relates how she called the Steinway company to ask for ""a man who could come over to rescue a piano.""
That event was the crescendo in a long string of architectural warnings.  Ultimately, there was little choice but to tear down and rebuild the White House.  The exterior façade was preserved, as were a few of the interior's furnishings and decorations.  Still, the extensive transformation left many feeling that the White House was no longer the same.",Critical Analysis & Reasoning Skills,38,Humanities
36,Passage Title: White House Reconstruction,"The destruction of the White House was not the fault of its original builders.  When James Hoban designed a presidential palace to reflect George Washington's vision for a residence in 1792, he upheld the best architectural standards of his day.  By the time of the 1948–1952 renovation, however, there were plenty of people to blame for the White House's condition.  A combination of poor planning, carelessness, and neglect throughout the intervening years caused a catastrophic deterioration.
Robert Klara recounts a history of changes to the White House that gradually compromised its structural integrity, beginning as far back as 1833 when Andrew Jackson had pipes installed for running water.  Subsequent presidents added modern conveniences in pace with technological developments: gas lighting for James Polk, a telephone system for Rutherford Hayes, and air conditioning for Chester Arthur, among others.  None of these advancements could have been anticipated by Hoban, and they had the dual effect of increasing the load borne by the house while decreasing its ability to bear that burden.  The builders responsible for the many additions performed ""careless and foolish maneuvers that a first-year architecture student would never have made,"" such as installing doorways in load-bearing walls or destroying the keystones of a brick arch.  Deep cuts were carved into crucial support beams to make room for pipes, wiring, and air ducts, weakening the beams and concentrating weight that would otherwise have been more evenly distributed.
By the turn of the twentieth century, the structural failings of the White House had become difficult to ignore.  The building was likely beyond saving at that point, and President Roosevelt didn't help matters by allowing architect Charles McKim only four months to make repairs.  McKim reinforced the second floor as well as he could within the time allotted, but his efforts only delayed the inevitable.  In 1925, Calvin Coolidge scoffed at reports that the White House roof was unsafe until ""[a] chunk of it broke off and bonked President Coolidge in the head.""  Poetic justice notwithstanding, the repairs to which he only then begrudgingly agreed had a disastrous side effect: the weight of the roof was shifted away from the stronger outer walls to the weaker inner ones, causing enormous structural strain.
The need to reconstruct the mansion reached a breaking point during the Harry Truman administration.  Walls continued to sink and pull away from ceilings; heavy chandeliers swung dangerously from movement on the floor above them; the entire East Room required scaffolding to prevent the collapse of its ceiling.  More dramatic incidents involved daughter Margaret Truman's piano.  Not only did the floor of her sitting room begin to sway up and down while she was playing a duet with a friend, but no steps were taken to move the piano away from the weak floor afterward.  Predictably, White House residents and staff were later horrified when one of the piano's legs crashed through, puncturing the overtaxed wood and sending debris cascading into the room below.  As for Margaret, Klara relates how she called the Steinway company to ask for ""a man who could come over to rescue a piano.""
That event was the crescendo in a long string of architectural warnings.  Ultimately, there was little choice but to tear down and rebuild the White House.  The exterior façade was preserved, as were a few of the interior's furnishings and decorations.  Still, the extensive transformation left many feeling that the White House was no longer the same.",Critical Analysis & Reasoning Skills,38,Humanities
37,Passage Title: White House Reconstruction,"The destruction of the White House was not the fault of its original builders.  When James Hoban designed a presidential palace to reflect George Washington's vision for a residence in 1792, he upheld the best architectural standards of his day.  By the time of the 1948–1952 renovation, however, there were plenty of people to blame for the White House's condition.  A combination of poor planning, carelessness, and neglect throughout the intervening years caused a catastrophic deterioration.
Robert Klara recounts a history of changes to the White House that gradually compromised its structural integrity, beginning as far back as 1833 when Andrew Jackson had pipes installed for running water.  Subsequent presidents added modern conveniences in pace with technological developments: gas lighting for James Polk, a telephone system for Rutherford Hayes, and air conditioning for Chester Arthur, among others.  None of these advancements could have been anticipated by Hoban, and they had the dual effect of increasing the load borne by the house while decreasing its ability to bear that burden.  The builders responsible for the many additions performed ""careless and foolish maneuvers that a first-year architecture student would never have made,"" such as installing doorways in load-bearing walls or destroying the keystones of a brick arch.  Deep cuts were carved into crucial support beams to make room for pipes, wiring, and air ducts, weakening the beams and concentrating weight that would otherwise have been more evenly distributed.
By the turn of the twentieth century, the structural failings of the White House had become difficult to ignore.  The building was likely beyond saving at that point, and President Roosevelt didn't help matters by allowing architect Charles McKim only four months to make repairs.  McKim reinforced the second floor as well as he could within the time allotted, but his efforts only delayed the inevitable.  In 1925, Calvin Coolidge scoffed at reports that the White House roof was unsafe until ""[a] chunk of it broke off and bonked President Coolidge in the head.""  Poetic justice notwithstanding, the repairs to which he only then begrudgingly agreed had a disastrous side effect: the weight of the roof was shifted away from the stronger outer walls to the weaker inner ones, causing enormous structural strain.
The need to reconstruct the mansion reached a breaking point during the Harry Truman administration.  Walls continued to sink and pull away from ceilings; heavy chandeliers swung dangerously from movement on the floor above them; the entire East Room required scaffolding to prevent the collapse of its ceiling.  More dramatic incidents involved daughter Margaret Truman's piano.  Not only did the floor of her sitting room begin to sway up and down while she was playing a duet with a friend, but no steps were taken to move the piano away from the weak floor afterward.  Predictably, White House residents and staff were later horrified when one of the piano's legs crashed through, puncturing the overtaxed wood and sending debris cascading into the room below.  As for Margaret, Klara relates how she called the Steinway company to ask for ""a man who could come over to rescue a piano.""
That event was the crescendo in a long string of architectural warnings.  Ultimately, there was little choice but to tear down and rebuild the White House.  The exterior façade was preserved, as were a few of the interior's furnishings and decorations.  Still, the extensive transformation left many feeling that the White House was no longer the same.",Critical Analysis & Reasoning Skills,38,Humanities
38,Passage Title: White House Reconstruction,"The destruction of the White House was not the fault of its original builders.  When James Hoban designed a presidential palace to reflect George Washington's vision for a residence in 1792, he upheld the best architectural standards of his day.  By the time of the 1948–1952 renovation, however, there were plenty of people to blame for the White House's condition.  A combination of poor planning, carelessness, and neglect throughout the intervening years caused a catastrophic deterioration.
Robert Klara recounts a history of changes to the White House that gradually compromised its structural integrity, beginning as far back as 1833 when Andrew Jackson had pipes installed for running water.  Subsequent presidents added modern conveniences in pace with technological developments: gas lighting for James Polk, a telephone system for Rutherford Hayes, and air conditioning for Chester Arthur, among others.  None of these advancements could have been anticipated by Hoban, and they had the dual effect of increasing the load borne by the house while decreasing its ability to bear that burden.  The builders responsible for the many additions performed ""careless and foolish maneuvers that a first-year architecture student would never have made,"" such as installing doorways in load-bearing walls or destroying the keystones of a brick arch.  Deep cuts were carved into crucial support beams to make room for pipes, wiring, and air ducts, weakening the beams and concentrating weight that would otherwise have been more evenly distributed.
By the turn of the twentieth century, the structural failings of the White House had become difficult to ignore.  The building was likely beyond saving at that point, and President Roosevelt didn't help matters by allowing architect Charles McKim only four months to make repairs.  McKim reinforced the second floor as well as he could within the time allotted, but his efforts only delayed the inevitable.  In 1925, Calvin Coolidge scoffed at reports that the White House roof was unsafe until ""[a] chunk of it broke off and bonked President Coolidge in the head.""  Poetic justice notwithstanding, the repairs to which he only then begrudgingly agreed had a disastrous side effect: the weight of the roof was shifted away from the stronger outer walls to the weaker inner ones, causing enormous structural strain.
The need to reconstruct the mansion reached a breaking point during the Harry Truman administration.  Walls continued to sink and pull away from ceilings; heavy chandeliers swung dangerously from movement on the floor above them; the entire East Room required scaffolding to prevent the collapse of its ceiling.  More dramatic incidents involved daughter Margaret Truman's piano.  Not only did the floor of her sitting room begin to sway up and down while she was playing a duet with a friend, but no steps were taken to move the piano away from the weak floor afterward.  Predictably, White House residents and staff were later horrified when one of the piano's legs crashed through, puncturing the overtaxed wood and sending debris cascading into the room below.  As for Margaret, Klara relates how she called the Steinway company to ask for ""a man who could come over to rescue a piano.""
That event was the crescendo in a long string of architectural warnings.  Ultimately, there was little choice but to tear down and rebuild the White House.  The exterior façade was preserved, as were a few of the interior's furnishings and decorations.  Still, the extensive transformation left many feeling that the White House was no longer the same.",Critical Analysis & Reasoning Skills,38,Humanities
39,Passage Title: Boethius and Foreknowledge,"The philosophical Dilemma of Freedom and Foreknowledge concerns the apparent incompatibility between human free will and God's knowledge of the future.  The dilemma proceeds as follows: Free will requires the ability to choose from among alternative possible actions.  But if God knows in advance what a person will do, then there are no possible alternatives, only a single guaranteed outcome conforming to God's prior knowledge.  Consequently, no act is performed freely but instead occurs of necessity.  Hence, the dilemma seems to require that one abandon either belief in free will or the belief that God possesses knowledge of the future.
Various attempts have been made to resolve this dilemma, the most famous being the solution offered by the Roman consul Boethius.  A gifted scholar, Boethius initially enjoyed the favor of the barbarian patrician Theodoric, who ruled the western half of the Roman Empire in the early sixth century CE.  However, Boethius fell under suspicion of sympathy to the Eastern empire and disloyalty to Theodoric, leading ultimately to his execution in 524 or 525.  It was during the imprisonment preceding this fate that he wrote The Consolation of Philosophy, so named because it personifies Philosophy as a comforting spirit who engages the reader in conversation.  Boethius receives her instruction, attaining peace amidst his suffering through the wisdom she imparts.
The spirit's solution to the dilemma rests partially on rejecting the notion that God truly has foreknowledge as humans understand it.  As described in Book V of Boethius' work, human existence ""still lacks the future, while already having lost the past.""  Hence, the concept of foreknowledge arises from humanity's experience of events as a temporal succession.  But for God, who exists eternally, events are not temporal.  Past, present, and future are to God's mind what the impressions of a single moment are to human senses; God grasps the entirety of time as an immediate perception.  Therefore, Philosophy concludes that when considering God's understanding of what we would call future events, ""it will be more correct to think of it not as a kind of foreknowledge of the future, but as the knowledge of a never ending presence.""
This conception of God's eternality places divine knowledge of the ""future"" on par with human knowledge of the present.  That equivalence is important because…it would seem to eliminate the grounds for thinking that God's knowledge of future actions precludes their being freely chosen.  For example, suppose that one looks out a window and observes another person walking by.  The pedestrian's act of walking was not necessitated by the fact of being observed; rather, her choice to walk is what makes the observation possible.  If God's knowledge of future events is akin to such an observation, then it too confers no necessity on human actions.  It is only our experience of time which gives rise to the illusion that such knowledge interferes with free will.  Purportedly, then, God's timelessness resolves the Freedom and Foreknowledge Dilemma.
Although there is much to admire in Boethius' reasoning, some philosophers note that a similar dilemma can be generated without referring to God.  On their view of the problem, God is essentially irrelevant because the threat to free will does not come from foreknowledge itself; it comes from the possibility that statements about future actions are true even before those actions occur.  Thus, the Freedom and Foreknowledge Dilemma would be subsumed under a broader Dilemma of Freedom and Future Truths.  Unfortunately, bounded in time as we are, that issue must remain beyond our present discussion….
Boethius' death was brutal and, from our perspective, probably undeserved.  But perhaps in his final moments he found consolation in the thought that his fate had not been his fate.",Critical Analysis & Reasoning Skills,43,Humanities
40,Passage Title: Boethius and Foreknowledge,"The philosophical Dilemma of Freedom and Foreknowledge concerns the apparent incompatibility between human free will and God's knowledge of the future.  The dilemma proceeds as follows: Free will requires the ability to choose from among alternative possible actions.  But if God knows in advance what a person will do, then there are no possible alternatives, only a single guaranteed outcome conforming to God's prior knowledge.  Consequently, no act is performed freely but instead occurs of necessity.  Hence, the dilemma seems to require that one abandon either belief in free will or the belief that God possesses knowledge of the future.
Various attempts have been made to resolve this dilemma, the most famous being the solution offered by the Roman consul Boethius.  A gifted scholar, Boethius initially enjoyed the favor of the barbarian patrician Theodoric, who ruled the western half of the Roman Empire in the early sixth century CE.  However, Boethius fell under suspicion of sympathy to the Eastern empire and disloyalty to Theodoric, leading ultimately to his execution in 524 or 525.  It was during the imprisonment preceding this fate that he wrote The Consolation of Philosophy, so named because it personifies Philosophy as a comforting spirit who engages the reader in conversation.  Boethius receives her instruction, attaining peace amidst his suffering through the wisdom she imparts.
The spirit's solution to the dilemma rests partially on rejecting the notion that God truly has foreknowledge as humans understand it.  As described in Book V of Boethius' work, human existence ""still lacks the future, while already having lost the past.""  Hence, the concept of foreknowledge arises from humanity's experience of events as a temporal succession.  But for God, who exists eternally, events are not temporal.  Past, present, and future are to God's mind what the impressions of a single moment are to human senses; God grasps the entirety of time as an immediate perception.  Therefore, Philosophy concludes that when considering God's understanding of what we would call future events, ""it will be more correct to think of it not as a kind of foreknowledge of the future, but as the knowledge of a never ending presence.""
This conception of God's eternality places divine knowledge of the ""future"" on par with human knowledge of the present.  That equivalence is important because…it would seem to eliminate the grounds for thinking that God's knowledge of future actions precludes their being freely chosen.  For example, suppose that one looks out a window and observes another person walking by.  The pedestrian's act of walking was not necessitated by the fact of being observed; rather, her choice to walk is what makes the observation possible.  If God's knowledge of future events is akin to such an observation, then it too confers no necessity on human actions.  It is only our experience of time which gives rise to the illusion that such knowledge interferes with free will.  Purportedly, then, God's timelessness resolves the Freedom and Foreknowledge Dilemma.
Although there is much to admire in Boethius' reasoning, some philosophers note that a similar dilemma can be generated without referring to God.  On their view of the problem, God is essentially irrelevant because the threat to free will does not come from foreknowledge itself; it comes from the possibility that statements about future actions are true even before those actions occur.  Thus, the Freedom and Foreknowledge Dilemma would be subsumed under a broader Dilemma of Freedom and Future Truths.  Unfortunately, bounded in time as we are, that issue must remain beyond our present discussion….
Boethius' death was brutal and, from our perspective, probably undeserved.  But perhaps in his final moments he found consolation in the thought that his fate had not been his fate.",Critical Analysis & Reasoning Skills,43,Humanities
41,Passage Title: Boethius and Foreknowledge,"The philosophical Dilemma of Freedom and Foreknowledge concerns the apparent incompatibility between human free will and God's knowledge of the future.  The dilemma proceeds as follows: Free will requires the ability to choose from among alternative possible actions.  But if God knows in advance what a person will do, then there are no possible alternatives, only a single guaranteed outcome conforming to God's prior knowledge.  Consequently, no act is performed freely but instead occurs of necessity.  Hence, the dilemma seems to require that one abandon either belief in free will or the belief that God possesses knowledge of the future.
Various attempts have been made to resolve this dilemma, the most famous being the solution offered by the Roman consul Boethius.  A gifted scholar, Boethius initially enjoyed the favor of the barbarian patrician Theodoric, who ruled the western half of the Roman Empire in the early sixth century CE.  However, Boethius fell under suspicion of sympathy to the Eastern empire and disloyalty to Theodoric, leading ultimately to his execution in 524 or 525.  It was during the imprisonment preceding this fate that he wrote The Consolation of Philosophy, so named because it personifies Philosophy as a comforting spirit who engages the reader in conversation.  Boethius receives her instruction, attaining peace amidst his suffering through the wisdom she imparts.
The spirit's solution to the dilemma rests partially on rejecting the notion that God truly has foreknowledge as humans understand it.  As described in Book V of Boethius' work, human existence ""still lacks the future, while already having lost the past.""  Hence, the concept of foreknowledge arises from humanity's experience of events as a temporal succession.  But for God, who exists eternally, events are not temporal.  Past, present, and future are to God's mind what the impressions of a single moment are to human senses; God grasps the entirety of time as an immediate perception.  Therefore, Philosophy concludes that when considering God's understanding of what we would call future events, ""it will be more correct to think of it not as a kind of foreknowledge of the future, but as the knowledge of a never ending presence.""
This conception of God's eternality places divine knowledge of the ""future"" on par with human knowledge of the present.  That equivalence is important because…it would seem to eliminate the grounds for thinking that God's knowledge of future actions precludes their being freely chosen.  For example, suppose that one looks out a window and observes another person walking by.  The pedestrian's act of walking was not necessitated by the fact of being observed; rather, her choice to walk is what makes the observation possible.  If God's knowledge of future events is akin to such an observation, then it too confers no necessity on human actions.  It is only our experience of time which gives rise to the illusion that such knowledge interferes with free will.  Purportedly, then, God's timelessness resolves the Freedom and Foreknowledge Dilemma.
Although there is much to admire in Boethius' reasoning, some philosophers note that a similar dilemma can be generated without referring to God.  On their view of the problem, God is essentially irrelevant because the threat to free will does not come from foreknowledge itself; it comes from the possibility that statements about future actions are true even before those actions occur.  Thus, the Freedom and Foreknowledge Dilemma would be subsumed under a broader Dilemma of Freedom and Future Truths.  Unfortunately, bounded in time as we are, that issue must remain beyond our present discussion….
Boethius' death was brutal and, from our perspective, probably undeserved.  But perhaps in his final moments he found consolation in the thought that his fate had not been his fate.",Critical Analysis & Reasoning Skills,43,Humanities
42,Passage Title: Boethius and Foreknowledge,"The philosophical Dilemma of Freedom and Foreknowledge concerns the apparent incompatibility between human free will and God's knowledge of the future.  The dilemma proceeds as follows: Free will requires the ability to choose from among alternative possible actions.  But if God knows in advance what a person will do, then there are no possible alternatives, only a single guaranteed outcome conforming to God's prior knowledge.  Consequently, no act is performed freely but instead occurs of necessity.  Hence, the dilemma seems to require that one abandon either belief in free will or the belief that God possesses knowledge of the future.
Various attempts have been made to resolve this dilemma, the most famous being the solution offered by the Roman consul Boethius.  A gifted scholar, Boethius initially enjoyed the favor of the barbarian patrician Theodoric, who ruled the western half of the Roman Empire in the early sixth century CE.  However, Boethius fell under suspicion of sympathy to the Eastern empire and disloyalty to Theodoric, leading ultimately to his execution in 524 or 525.  It was during the imprisonment preceding this fate that he wrote The Consolation of Philosophy, so named because it personifies Philosophy as a comforting spirit who engages the reader in conversation.  Boethius receives her instruction, attaining peace amidst his suffering through the wisdom she imparts.
The spirit's solution to the dilemma rests partially on rejecting the notion that God truly has foreknowledge as humans understand it.  As described in Book V of Boethius' work, human existence ""still lacks the future, while already having lost the past.""  Hence, the concept of foreknowledge arises from humanity's experience of events as a temporal succession.  But for God, who exists eternally, events are not temporal.  Past, present, and future are to God's mind what the impressions of a single moment are to human senses; God grasps the entirety of time as an immediate perception.  Therefore, Philosophy concludes that when considering God's understanding of what we would call future events, ""it will be more correct to think of it not as a kind of foreknowledge of the future, but as the knowledge of a never ending presence.""
This conception of God's eternality places divine knowledge of the ""future"" on par with human knowledge of the present.  That equivalence is important because…it would seem to eliminate the grounds for thinking that God's knowledge of future actions precludes their being freely chosen.  For example, suppose that one looks out a window and observes another person walking by.  The pedestrian's act of walking was not necessitated by the fact of being observed; rather, her choice to walk is what makes the observation possible.  If God's knowledge of future events is akin to such an observation, then it too confers no necessity on human actions.  It is only our experience of time which gives rise to the illusion that such knowledge interferes with free will.  Purportedly, then, God's timelessness resolves the Freedom and Foreknowledge Dilemma.
Although there is much to admire in Boethius' reasoning, some philosophers note that a similar dilemma can be generated without referring to God.  On their view of the problem, God is essentially irrelevant because the threat to free will does not come from foreknowledge itself; it comes from the possibility that statements about future actions are true even before those actions occur.  Thus, the Freedom and Foreknowledge Dilemma would be subsumed under a broader Dilemma of Freedom and Future Truths.  Unfortunately, bounded in time as we are, that issue must remain beyond our present discussion….
Boethius' death was brutal and, from our perspective, probably undeserved.  But perhaps in his final moments he found consolation in the thought that his fate had not been his fate.",Critical Analysis & Reasoning Skills,43,Humanities
43,Passage Title: Boethius and Foreknowledge,"The philosophical Dilemma of Freedom and Foreknowledge concerns the apparent incompatibility between human free will and God's knowledge of the future.  The dilemma proceeds as follows: Free will requires the ability to choose from among alternative possible actions.  But if God knows in advance what a person will do, then there are no possible alternatives, only a single guaranteed outcome conforming to God's prior knowledge.  Consequently, no act is performed freely but instead occurs of necessity.  Hence, the dilemma seems to require that one abandon either belief in free will or the belief that God possesses knowledge of the future.
Various attempts have been made to resolve this dilemma, the most famous being the solution offered by the Roman consul Boethius.  A gifted scholar, Boethius initially enjoyed the favor of the barbarian patrician Theodoric, who ruled the western half of the Roman Empire in the early sixth century CE.  However, Boethius fell under suspicion of sympathy to the Eastern empire and disloyalty to Theodoric, leading ultimately to his execution in 524 or 525.  It was during the imprisonment preceding this fate that he wrote The Consolation of Philosophy, so named because it personifies Philosophy as a comforting spirit who engages the reader in conversation.  Boethius receives her instruction, attaining peace amidst his suffering through the wisdom she imparts.
The spirit's solution to the dilemma rests partially on rejecting the notion that God truly has foreknowledge as humans understand it.  As described in Book V of Boethius' work, human existence ""still lacks the future, while already having lost the past.""  Hence, the concept of foreknowledge arises from humanity's experience of events as a temporal succession.  But for God, who exists eternally, events are not temporal.  Past, present, and future are to God's mind what the impressions of a single moment are to human senses; God grasps the entirety of time as an immediate perception.  Therefore, Philosophy concludes that when considering God's understanding of what we would call future events, ""it will be more correct to think of it not as a kind of foreknowledge of the future, but as the knowledge of a never ending presence.""
This conception of God's eternality places divine knowledge of the ""future"" on par with human knowledge of the present.  That equivalence is important because…it would seem to eliminate the grounds for thinking that God's knowledge of future actions precludes their being freely chosen.  For example, suppose that one looks out a window and observes another person walking by.  The pedestrian's act of walking was not necessitated by the fact of being observed; rather, her choice to walk is what makes the observation possible.  If God's knowledge of future events is akin to such an observation, then it too confers no necessity on human actions.  It is only our experience of time which gives rise to the illusion that such knowledge interferes with free will.  Purportedly, then, God's timelessness resolves the Freedom and Foreknowledge Dilemma.
Although there is much to admire in Boethius' reasoning, some philosophers note that a similar dilemma can be generated without referring to God.  On their view of the problem, God is essentially irrelevant because the threat to free will does not come from foreknowledge itself; it comes from the possibility that statements about future actions are true even before those actions occur.  Thus, the Freedom and Foreknowledge Dilemma would be subsumed under a broader Dilemma of Freedom and Future Truths.  Unfortunately, bounded in time as we are, that issue must remain beyond our present discussion….
Boethius' death was brutal and, from our perspective, probably undeserved.  But perhaps in his final moments he found consolation in the thought that his fate had not been his fate.",Critical Analysis & Reasoning Skills,43,Humanities
44,Passage Title: Sumptuary Laws,"In America's early democratic republic, the superficial signifiers of social status that had long distinguished the aristocracy in Europe were no longer supposed to apply.  However, despite political theory and an emphasis on rhetoric that implied democratic equality, many among the late eighteenth-century social elite flaunted their status and at the same time condemned any pretense by the lower classes to a higher social position.
For the American ""nobility,"" the immoderation that had marked public life since before the Revolution assumed epic proportions in the later decades of the century.  From the viewpoint of society's top tier, social pretense, revealed through presumptive choices of costume or coiffure, was becoming far too common.  Sumptuary legislation—largely a holdover from the colonial era—was accordingly reinstituted in some states as a means to regulate behavior and a remedy for inappropriate displays of extravagance.  Indeed, libertine attitudes so dominated the population that George Mason IV, a congressional representative from the Commonwealth of Virginia, proposed that the federal government impose sumptuary legislation on all strata of society, including the affluent who would have traditionally been excluded from such laws.  However, the motion did not pass, with such social trends proving too deeply rooted.  Nevertheless, Mason was hardly alone in his alarm at the specter of mounting excess.
The more established members of the republican gentility had long freely indulged in decadent consumption without attracting much attention.  Nevertheless, most of these older elites criticized their own younger generation, whose coming-of-age was characterized by more overt disregard for the Puritan austerity advocated by their forebears.  The self-gratifying taste for grande cuisine and propensities for haute couture of the upper-class youth were perceived as decidedly improper conduct by their elders.  Moreover, as this ostentatious comportment worsened, it influenced those from the inferior echelons of society to adopt similar displays.
The presumptuous behavior mimicked by the lower classes led to an uproar by those in positions of power, who swiftly moved to modify societal conduct.  In particular, dissonance swelled in Massachusetts, where, within Boston's most select circles, a palpable anxiety over the maintenance of social hierarchies and their rightful exterior signifiers arose.  For the elite few who made up America's haute monde, any posturing of manner or appropriation of dress disrupted the customary rigidity of the social hierarchy, which, to their dismay, had now become increasingly fluid.  There was a communal uneasiness, which grew into national dread, that artful impostors might slip through the boundaries of the social strata undetected.  Yet, vexingly, counterfeits were becoming more difficult to recognize.  Consequently, in order to restrict impersonation, those of questionable lineage faced the threat of ostracization for an act as simple as sitting for a portrait.
At the same time, a movement materialized that had Boston as its epicenter.  The new paradigm mandated that eighteenth-century gentlemen as well as ladies project positive, even edifying, public images, as their privileged position behooved.  Therefore, a distinctly American variety of noblesse oblige, though not without its detractors, spread throughout the new democratic republic.  In metropolitan centers across the new nation, the attributes of honor, modesty, and public virtue assumed increasing importance among the ""aristocracy.""  Keeping up appearances—that were oftentimes not all they seemed—became paramount in order to set examples of permissible public behavior for the benefit of those of humble birth or who were less affluent.  Meanwhile, the bulk of the American population, consisting of those with little wealth or status, did their best not to emulate their ""role models.""",Critical Analysis & Reasoning Skills,49,Social Sciences
45,Passage Title: Sumptuary Laws,"In America's early democratic republic, the superficial signifiers of social status that had long distinguished the aristocracy in Europe were no longer supposed to apply.  However, despite political theory and an emphasis on rhetoric that implied democratic equality, many among the late eighteenth-century social elite flaunted their status and at the same time condemned any pretense by the lower classes to a higher social position.
For the American ""nobility,"" the immoderation that had marked public life since before the Revolution assumed epic proportions in the later decades of the century.  From the viewpoint of society's top tier, social pretense, revealed through presumptive choices of costume or coiffure, was becoming far too common.  Sumptuary legislation—largely a holdover from the colonial era—was accordingly reinstituted in some states as a means to regulate behavior and a remedy for inappropriate displays of extravagance.  Indeed, libertine attitudes so dominated the population that George Mason IV, a congressional representative from the Commonwealth of Virginia, proposed that the federal government impose sumptuary legislation on all strata of society, including the affluent who would have traditionally been excluded from such laws.  However, the motion did not pass, with such social trends proving too deeply rooted.  Nevertheless, Mason was hardly alone in his alarm at the specter of mounting excess.
The more established members of the republican gentility had long freely indulged in decadent consumption without attracting much attention.  Nevertheless, most of these older elites criticized their own younger generation, whose coming-of-age was characterized by more overt disregard for the Puritan austerity advocated by their forebears.  The self-gratifying taste for grande cuisine and propensities for haute couture of the upper-class youth were perceived as decidedly improper conduct by their elders.  Moreover, as this ostentatious comportment worsened, it influenced those from the inferior echelons of society to adopt similar displays.
The presumptuous behavior mimicked by the lower classes led to an uproar by those in positions of power, who swiftly moved to modify societal conduct.  In particular, dissonance swelled in Massachusetts, where, within Boston's most select circles, a palpable anxiety over the maintenance of social hierarchies and their rightful exterior signifiers arose.  For the elite few who made up America's haute monde, any posturing of manner or appropriation of dress disrupted the customary rigidity of the social hierarchy, which, to their dismay, had now become increasingly fluid.  There was a communal uneasiness, which grew into national dread, that artful impostors might slip through the boundaries of the social strata undetected.  Yet, vexingly, counterfeits were becoming more difficult to recognize.  Consequently, in order to restrict impersonation, those of questionable lineage faced the threat of ostracization for an act as simple as sitting for a portrait.
At the same time, a movement materialized that had Boston as its epicenter.  The new paradigm mandated that eighteenth-century gentlemen as well as ladies project positive, even edifying, public images, as their privileged position behooved.  Therefore, a distinctly American variety of noblesse oblige, though not without its detractors, spread throughout the new democratic republic.  In metropolitan centers across the new nation, the attributes of honor, modesty, and public virtue assumed increasing importance among the ""aristocracy.""  Keeping up appearances—that were oftentimes not all they seemed—became paramount in order to set examples of permissible public behavior for the benefit of those of humble birth or who were less affluent.  Meanwhile, the bulk of the American population, consisting of those with little wealth or status, did their best not to emulate their ""role models.""",Critical Analysis & Reasoning Skills,49,Social Sciences
46,Passage Title: Sumptuary Laws,"In America's early democratic republic, the superficial signifiers of social status that had long distinguished the aristocracy in Europe were no longer supposed to apply.  However, despite political theory and an emphasis on rhetoric that implied democratic equality, many among the late eighteenth-century social elite flaunted their status and at the same time condemned any pretense by the lower classes to a higher social position.
For the American ""nobility,"" the immoderation that had marked public life since before the Revolution assumed epic proportions in the later decades of the century.  From the viewpoint of society's top tier, social pretense, revealed through presumptive choices of costume or coiffure, was becoming far too common.  Sumptuary legislation—largely a holdover from the colonial era—was accordingly reinstituted in some states as a means to regulate behavior and a remedy for inappropriate displays of extravagance.  Indeed, libertine attitudes so dominated the population that George Mason IV, a congressional representative from the Commonwealth of Virginia, proposed that the federal government impose sumptuary legislation on all strata of society, including the affluent who would have traditionally been excluded from such laws.  However, the motion did not pass, with such social trends proving too deeply rooted.  Nevertheless, Mason was hardly alone in his alarm at the specter of mounting excess.
The more established members of the republican gentility had long freely indulged in decadent consumption without attracting much attention.  Nevertheless, most of these older elites criticized their own younger generation, whose coming-of-age was characterized by more overt disregard for the Puritan austerity advocated by their forebears.  The self-gratifying taste for grande cuisine and propensities for haute couture of the upper-class youth were perceived as decidedly improper conduct by their elders.  Moreover, as this ostentatious comportment worsened, it influenced those from the inferior echelons of society to adopt similar displays.
The presumptuous behavior mimicked by the lower classes led to an uproar by those in positions of power, who swiftly moved to modify societal conduct.  In particular, dissonance swelled in Massachusetts, where, within Boston's most select circles, a palpable anxiety over the maintenance of social hierarchies and their rightful exterior signifiers arose.  For the elite few who made up America's haute monde, any posturing of manner or appropriation of dress disrupted the customary rigidity of the social hierarchy, which, to their dismay, had now become increasingly fluid.  There was a communal uneasiness, which grew into national dread, that artful impostors might slip through the boundaries of the social strata undetected.  Yet, vexingly, counterfeits were becoming more difficult to recognize.  Consequently, in order to restrict impersonation, those of questionable lineage faced the threat of ostracization for an act as simple as sitting for a portrait.
At the same time, a movement materialized that had Boston as its epicenter.  The new paradigm mandated that eighteenth-century gentlemen as well as ladies project positive, even edifying, public images, as their privileged position behooved.  Therefore, a distinctly American variety of noblesse oblige, though not without its detractors, spread throughout the new democratic republic.  In metropolitan centers across the new nation, the attributes of honor, modesty, and public virtue assumed increasing importance among the ""aristocracy.""  Keeping up appearances—that were oftentimes not all they seemed—became paramount in order to set examples of permissible public behavior for the benefit of those of humble birth or who were less affluent.  Meanwhile, the bulk of the American population, consisting of those with little wealth or status, did their best not to emulate their ""role models.""",Critical Analysis & Reasoning Skills,49,Social Sciences
47,Passage Title: Sumptuary Laws,"In America's early democratic republic, the superficial signifiers of social status that had long distinguished the aristocracy in Europe were no longer supposed to apply.  However, despite political theory and an emphasis on rhetoric that implied democratic equality, many among the late eighteenth-century social elite flaunted their status and at the same time condemned any pretense by the lower classes to a higher social position.
For the American ""nobility,"" the immoderation that had marked public life since before the Revolution assumed epic proportions in the later decades of the century.  From the viewpoint of society's top tier, social pretense, revealed through presumptive choices of costume or coiffure, was becoming far too common.  Sumptuary legislation—largely a holdover from the colonial era—was accordingly reinstituted in some states as a means to regulate behavior and a remedy for inappropriate displays of extravagance.  Indeed, libertine attitudes so dominated the population that George Mason IV, a congressional representative from the Commonwealth of Virginia, proposed that the federal government impose sumptuary legislation on all strata of society, including the affluent who would have traditionally been excluded from such laws.  However, the motion did not pass, with such social trends proving too deeply rooted.  Nevertheless, Mason was hardly alone in his alarm at the specter of mounting excess.
The more established members of the republican gentility had long freely indulged in decadent consumption without attracting much attention.  Nevertheless, most of these older elites criticized their own younger generation, whose coming-of-age was characterized by more overt disregard for the Puritan austerity advocated by their forebears.  The self-gratifying taste for grande cuisine and propensities for haute couture of the upper-class youth were perceived as decidedly improper conduct by their elders.  Moreover, as this ostentatious comportment worsened, it influenced those from the inferior echelons of society to adopt similar displays.
The presumptuous behavior mimicked by the lower classes led to an uproar by those in positions of power, who swiftly moved to modify societal conduct.  In particular, dissonance swelled in Massachusetts, where, within Boston's most select circles, a palpable anxiety over the maintenance of social hierarchies and their rightful exterior signifiers arose.  For the elite few who made up America's haute monde, any posturing of manner or appropriation of dress disrupted the customary rigidity of the social hierarchy, which, to their dismay, had now become increasingly fluid.  There was a communal uneasiness, which grew into national dread, that artful impostors might slip through the boundaries of the social strata undetected.  Yet, vexingly, counterfeits were becoming more difficult to recognize.  Consequently, in order to restrict impersonation, those of questionable lineage faced the threat of ostracization for an act as simple as sitting for a portrait.
At the same time, a movement materialized that had Boston as its epicenter.  The new paradigm mandated that eighteenth-century gentlemen as well as ladies project positive, even edifying, public images, as their privileged position behooved.  Therefore, a distinctly American variety of noblesse oblige, though not without its detractors, spread throughout the new democratic republic.  In metropolitan centers across the new nation, the attributes of honor, modesty, and public virtue assumed increasing importance among the ""aristocracy.""  Keeping up appearances—that were oftentimes not all they seemed—became paramount in order to set examples of permissible public behavior for the benefit of those of humble birth or who were less affluent.  Meanwhile, the bulk of the American population, consisting of those with little wealth or status, did their best not to emulate their ""role models.""",Critical Analysis & Reasoning Skills,49,Social Sciences
48,Passage Title: Sumptuary Laws,"In America's early democratic republic, the superficial signifiers of social status that had long distinguished the aristocracy in Europe were no longer supposed to apply.  However, despite political theory and an emphasis on rhetoric that implied democratic equality, many among the late eighteenth-century social elite flaunted their status and at the same time condemned any pretense by the lower classes to a higher social position.
For the American ""nobility,"" the immoderation that had marked public life since before the Revolution assumed epic proportions in the later decades of the century.  From the viewpoint of society's top tier, social pretense, revealed through presumptive choices of costume or coiffure, was becoming far too common.  Sumptuary legislation—largely a holdover from the colonial era—was accordingly reinstituted in some states as a means to regulate behavior and a remedy for inappropriate displays of extravagance.  Indeed, libertine attitudes so dominated the population that George Mason IV, a congressional representative from the Commonwealth of Virginia, proposed that the federal government impose sumptuary legislation on all strata of society, including the affluent who would have traditionally been excluded from such laws.  However, the motion did not pass, with such social trends proving too deeply rooted.  Nevertheless, Mason was hardly alone in his alarm at the specter of mounting excess.
The more established members of the republican gentility had long freely indulged in decadent consumption without attracting much attention.  Nevertheless, most of these older elites criticized their own younger generation, whose coming-of-age was characterized by more overt disregard for the Puritan austerity advocated by their forebears.  The self-gratifying taste for grande cuisine and propensities for haute couture of the upper-class youth were perceived as decidedly improper conduct by their elders.  Moreover, as this ostentatious comportment worsened, it influenced those from the inferior echelons of society to adopt similar displays.
The presumptuous behavior mimicked by the lower classes led to an uproar by those in positions of power, who swiftly moved to modify societal conduct.  In particular, dissonance swelled in Massachusetts, where, within Boston's most select circles, a palpable anxiety over the maintenance of social hierarchies and their rightful exterior signifiers arose.  For the elite few who made up America's haute monde, any posturing of manner or appropriation of dress disrupted the customary rigidity of the social hierarchy, which, to their dismay, had now become increasingly fluid.  There was a communal uneasiness, which grew into national dread, that artful impostors might slip through the boundaries of the social strata undetected.  Yet, vexingly, counterfeits were becoming more difficult to recognize.  Consequently, in order to restrict impersonation, those of questionable lineage faced the threat of ostracization for an act as simple as sitting for a portrait.
At the same time, a movement materialized that had Boston as its epicenter.  The new paradigm mandated that eighteenth-century gentlemen as well as ladies project positive, even edifying, public images, as their privileged position behooved.  Therefore, a distinctly American variety of noblesse oblige, though not without its detractors, spread throughout the new democratic republic.  In metropolitan centers across the new nation, the attributes of honor, modesty, and public virtue assumed increasing importance among the ""aristocracy.""  Keeping up appearances—that were oftentimes not all they seemed—became paramount in order to set examples of permissible public behavior for the benefit of those of humble birth or who were less affluent.  Meanwhile, the bulk of the American population, consisting of those with little wealth or status, did their best not to emulate their ""role models.""",Critical Analysis & Reasoning Skills,49,Social Sciences
49,Passage Title: Sumptuary Laws,"In America's early democratic republic, the superficial signifiers of social status that had long distinguished the aristocracy in Europe were no longer supposed to apply.  However, despite political theory and an emphasis on rhetoric that implied democratic equality, many among the late eighteenth-century social elite flaunted their status and at the same time condemned any pretense by the lower classes to a higher social position.
For the American ""nobility,"" the immoderation that had marked public life since before the Revolution assumed epic proportions in the later decades of the century.  From the viewpoint of society's top tier, social pretense, revealed through presumptive choices of costume or coiffure, was becoming far too common.  Sumptuary legislation—largely a holdover from the colonial era—was accordingly reinstituted in some states as a means to regulate behavior and a remedy for inappropriate displays of extravagance.  Indeed, libertine attitudes so dominated the population that George Mason IV, a congressional representative from the Commonwealth of Virginia, proposed that the federal government impose sumptuary legislation on all strata of society, including the affluent who would have traditionally been excluded from such laws.  However, the motion did not pass, with such social trends proving too deeply rooted.  Nevertheless, Mason was hardly alone in his alarm at the specter of mounting excess.
The more established members of the republican gentility had long freely indulged in decadent consumption without attracting much attention.  Nevertheless, most of these older elites criticized their own younger generation, whose coming-of-age was characterized by more overt disregard for the Puritan austerity advocated by their forebears.  The self-gratifying taste for grande cuisine and propensities for haute couture of the upper-class youth were perceived as decidedly improper conduct by their elders.  Moreover, as this ostentatious comportment worsened, it influenced those from the inferior echelons of society to adopt similar displays.
The presumptuous behavior mimicked by the lower classes led to an uproar by those in positions of power, who swiftly moved to modify societal conduct.  In particular, dissonance swelled in Massachusetts, where, within Boston's most select circles, a palpable anxiety over the maintenance of social hierarchies and their rightful exterior signifiers arose.  For the elite few who made up America's haute monde, any posturing of manner or appropriation of dress disrupted the customary rigidity of the social hierarchy, which, to their dismay, had now become increasingly fluid.  There was a communal uneasiness, which grew into national dread, that artful impostors might slip through the boundaries of the social strata undetected.  Yet, vexingly, counterfeits were becoming more difficult to recognize.  Consequently, in order to restrict impersonation, those of questionable lineage faced the threat of ostracization for an act as simple as sitting for a portrait.
At the same time, a movement materialized that had Boston as its epicenter.  The new paradigm mandated that eighteenth-century gentlemen as well as ladies project positive, even edifying, public images, as their privileged position behooved.  Therefore, a distinctly American variety of noblesse oblige, though not without its detractors, spread throughout the new democratic republic.  In metropolitan centers across the new nation, the attributes of honor, modesty, and public virtue assumed increasing importance among the ""aristocracy.""  Keeping up appearances—that were oftentimes not all they seemed—became paramount in order to set examples of permissible public behavior for the benefit of those of humble birth or who were less affluent.  Meanwhile, the bulk of the American population, consisting of those with little wealth or status, did their best not to emulate their ""role models.""",Critical Analysis & Reasoning Skills,49,Social Sciences
50,"Passage Title: Kant, Kantianism, and Traditional Political Theory","Although the primary canon in the history of political thought largely ignores the views of Immanuel Kant, he nevertheless was as important a political thinker as Rousseau, Locke, or even Aristotle.  Working in the context of the Prussian Enlightenment, Kant championed the idea that the only innate right of human beings is freedom.  Fittingly, the centerpiece of his political theory is that freedom can be the only basis for a just political state.
A political theory that advances freedom as its core tenet may appear inherently problematic.  The very existence of a political state would seem to require limitations on the freedoms of those within it.  However, this presents no serious problem for Kant's theory.  The Kantian notion of political freedom—which stands in stark contrast to the notion of transcendental freedom widely associated with Kant's nonpolitical writings—is restricted to the external relations between human persons.  In its political sense, freedom refers to the innate right of every person to independence from being constrained by another's choice.  In other words, every person has the innate right to make choices freely but only so long as these choices do not interfere with those of others.  Laws that do not allow for political freedom are forbidden in Kant's view because within a just civil condition, the promulgation of a law requires that an entire people agree to be subjected to it.
There are more substantive problems facing Kant's political theory.  Most notable among these concerns is Kant's promotion of welfare legislation.  As he says, ""The government is authorized to constrain the wealthy to provide the means of sustenance to those who are unable to provide for even their most necessary and basic needs.""  Given Kant's notion of political freedom, to enact such legislation would be catastrophic.  Welfare legislation violates Kant's own criterion regarding laws promulgated within a just civil condition: Governmental interference of this kind is a prime example of legislation on which an entire people will be divided.
This was an egregious error on Kant's part.  At the same time, proponents of a Kantian conception of the state need neither embrace this result nor dismiss the possibility of consistently incorporating welfare legislation into their preferred brand of political theory.  Although Kant did not explicitly advocate it himself, a Kantian may hold that public provision of welfare is necessary for the state to fulfill its primary function—namely, protecting the liberties of its citizens.  For instance, if failing to adopt welfare legislation would threaten the continued existence, security, or stability of the state, then Kantians may justify such legislation on the grounds that it serves an instrumentally necessary role in upholding the fundamental aims and principles of Kantianism itself.  It is interesting to note that followers of Kant will likely have independent reasons to accept something along these lines.  Kant himself held that rational entities—such as persons, corporations, and states—have a duty of self-preservation, and thus are obligated to take measures to ensure their continued existence, security, and stability.
There are still other issues surrounding a Kantian theory of the state.  These will need to be dealt with separately, but they should not go overlooked.  Breaking down the barriers that leave Kant outside the primary canon of political theory is a laudable task, which will take much effort to accomplish.",Critical Analysis & Reasoning Skills,54,Social Sciences
51,"Passage Title: Kant, Kantianism, and Traditional Political Theory","Although the primary canon in the history of political thought largely ignores the views of Immanuel Kant, he nevertheless was as important a political thinker as Rousseau, Locke, or even Aristotle.  Working in the context of the Prussian Enlightenment, Kant championed the idea that the only innate right of human beings is freedom.  Fittingly, the centerpiece of his political theory is that freedom can be the only basis for a just political state.
A political theory that advances freedom as its core tenet may appear inherently problematic.  The very existence of a political state would seem to require limitations on the freedoms of those within it.  However, this presents no serious problem for Kant's theory.  The Kantian notion of political freedom—which stands in stark contrast to the notion of transcendental freedom widely associated with Kant's nonpolitical writings—is restricted to the external relations between human persons.  In its political sense, freedom refers to the innate right of every person to independence from being constrained by another's choice.  In other words, every person has the innate right to make choices freely but only so long as these choices do not interfere with those of others.  Laws that do not allow for political freedom are forbidden in Kant's view because within a just civil condition, the promulgation of a law requires that an entire people agree to be subjected to it.
There are more substantive problems facing Kant's political theory.  Most notable among these concerns is Kant's promotion of welfare legislation.  As he says, ""The government is authorized to constrain the wealthy to provide the means of sustenance to those who are unable to provide for even their most necessary and basic needs.""  Given Kant's notion of political freedom, to enact such legislation would be catastrophic.  Welfare legislation violates Kant's own criterion regarding laws promulgated within a just civil condition: Governmental interference of this kind is a prime example of legislation on which an entire people will be divided.
This was an egregious error on Kant's part.  At the same time, proponents of a Kantian conception of the state need neither embrace this result nor dismiss the possibility of consistently incorporating welfare legislation into their preferred brand of political theory.  Although Kant did not explicitly advocate it himself, a Kantian may hold that public provision of welfare is necessary for the state to fulfill its primary function—namely, protecting the liberties of its citizens.  For instance, if failing to adopt welfare legislation would threaten the continued existence, security, or stability of the state, then Kantians may justify such legislation on the grounds that it serves an instrumentally necessary role in upholding the fundamental aims and principles of Kantianism itself.  It is interesting to note that followers of Kant will likely have independent reasons to accept something along these lines.  Kant himself held that rational entities—such as persons, corporations, and states—have a duty of self-preservation, and thus are obligated to take measures to ensure their continued existence, security, and stability.
There are still other issues surrounding a Kantian theory of the state.  These will need to be dealt with separately, but they should not go overlooked.  Breaking down the barriers that leave Kant outside the primary canon of political theory is a laudable task, which will take much effort to accomplish.",Critical Analysis & Reasoning Skills,54,Social Sciences
52,"Passage Title: Kant, Kantianism, and Traditional Political Theory","Although the primary canon in the history of political thought largely ignores the views of Immanuel Kant, he nevertheless was as important a political thinker as Rousseau, Locke, or even Aristotle.  Working in the context of the Prussian Enlightenment, Kant championed the idea that the only innate right of human beings is freedom.  Fittingly, the centerpiece of his political theory is that freedom can be the only basis for a just political state.
A political theory that advances freedom as its core tenet may appear inherently problematic.  The very existence of a political state would seem to require limitations on the freedoms of those within it.  However, this presents no serious problem for Kant's theory.  The Kantian notion of political freedom—which stands in stark contrast to the notion of transcendental freedom widely associated with Kant's nonpolitical writings—is restricted to the external relations between human persons.  In its political sense, freedom refers to the innate right of every person to independence from being constrained by another's choice.  In other words, every person has the innate right to make choices freely but only so long as these choices do not interfere with those of others.  Laws that do not allow for political freedom are forbidden in Kant's view because within a just civil condition, the promulgation of a law requires that an entire people agree to be subjected to it.
There are more substantive problems facing Kant's political theory.  Most notable among these concerns is Kant's promotion of welfare legislation.  As he says, ""The government is authorized to constrain the wealthy to provide the means of sustenance to those who are unable to provide for even their most necessary and basic needs.""  Given Kant's notion of political freedom, to enact such legislation would be catastrophic.  Welfare legislation violates Kant's own criterion regarding laws promulgated within a just civil condition: Governmental interference of this kind is a prime example of legislation on which an entire people will be divided.
This was an egregious error on Kant's part.  At the same time, proponents of a Kantian conception of the state need neither embrace this result nor dismiss the possibility of consistently incorporating welfare legislation into their preferred brand of political theory.  Although Kant did not explicitly advocate it himself, a Kantian may hold that public provision of welfare is necessary for the state to fulfill its primary function—namely, protecting the liberties of its citizens.  For instance, if failing to adopt welfare legislation would threaten the continued existence, security, or stability of the state, then Kantians may justify such legislation on the grounds that it serves an instrumentally necessary role in upholding the fundamental aims and principles of Kantianism itself.  It is interesting to note that followers of Kant will likely have independent reasons to accept something along these lines.  Kant himself held that rational entities—such as persons, corporations, and states—have a duty of self-preservation, and thus are obligated to take measures to ensure their continued existence, security, and stability.
There are still other issues surrounding a Kantian theory of the state.  These will need to be dealt with separately, but they should not go overlooked.  Breaking down the barriers that leave Kant outside the primary canon of political theory is a laudable task, which will take much effort to accomplish.",Critical Analysis & Reasoning Skills,54,Social Sciences
53,"Passage Title: Kant, Kantianism, and Traditional Political Theory","Although the primary canon in the history of political thought largely ignores the views of Immanuel Kant, he nevertheless was as important a political thinker as Rousseau, Locke, or even Aristotle.  Working in the context of the Prussian Enlightenment, Kant championed the idea that the only innate right of human beings is freedom.  Fittingly, the centerpiece of his political theory is that freedom can be the only basis for a just political state.
A political theory that advances freedom as its core tenet may appear inherently problematic.  The very existence of a political state would seem to require limitations on the freedoms of those within it.  However, this presents no serious problem for Kant's theory.  The Kantian notion of political freedom—which stands in stark contrast to the notion of transcendental freedom widely associated with Kant's nonpolitical writings—is restricted to the external relations between human persons.  In its political sense, freedom refers to the innate right of every person to independence from being constrained by another's choice.  In other words, every person has the innate right to make choices freely but only so long as these choices do not interfere with those of others.  Laws that do not allow for political freedom are forbidden in Kant's view because within a just civil condition, the promulgation of a law requires that an entire people agree to be subjected to it.
There are more substantive problems facing Kant's political theory.  Most notable among these concerns is Kant's promotion of welfare legislation.  As he says, ""The government is authorized to constrain the wealthy to provide the means of sustenance to those who are unable to provide for even their most necessary and basic needs.""  Given Kant's notion of political freedom, to enact such legislation would be catastrophic.  Welfare legislation violates Kant's own criterion regarding laws promulgated within a just civil condition: Governmental interference of this kind is a prime example of legislation on which an entire people will be divided.
This was an egregious error on Kant's part.  At the same time, proponents of a Kantian conception of the state need neither embrace this result nor dismiss the possibility of consistently incorporating welfare legislation into their preferred brand of political theory.  Although Kant did not explicitly advocate it himself, a Kantian may hold that public provision of welfare is necessary for the state to fulfill its primary function—namely, protecting the liberties of its citizens.  For instance, if failing to adopt welfare legislation would threaten the continued existence, security, or stability of the state, then Kantians may justify such legislation on the grounds that it serves an instrumentally necessary role in upholding the fundamental aims and principles of Kantianism itself.  It is interesting to note that followers of Kant will likely have independent reasons to accept something along these lines.  Kant himself held that rational entities—such as persons, corporations, and states—have a duty of self-preservation, and thus are obligated to take measures to ensure their continued existence, security, and stability.
There are still other issues surrounding a Kantian theory of the state.  These will need to be dealt with separately, but they should not go overlooked.  Breaking down the barriers that leave Kant outside the primary canon of political theory is a laudable task, which will take much effort to accomplish.",Critical Analysis & Reasoning Skills,54,Social Sciences
54,"Passage Title: Kant, Kantianism, and Traditional Political Theory","Although the primary canon in the history of political thought largely ignores the views of Immanuel Kant, he nevertheless was as important a political thinker as Rousseau, Locke, or even Aristotle.  Working in the context of the Prussian Enlightenment, Kant championed the idea that the only innate right of human beings is freedom.  Fittingly, the centerpiece of his political theory is that freedom can be the only basis for a just political state.
A political theory that advances freedom as its core tenet may appear inherently problematic.  The very existence of a political state would seem to require limitations on the freedoms of those within it.  However, this presents no serious problem for Kant's theory.  The Kantian notion of political freedom—which stands in stark contrast to the notion of transcendental freedom widely associated with Kant's nonpolitical writings—is restricted to the external relations between human persons.  In its political sense, freedom refers to the innate right of every person to independence from being constrained by another's choice.  In other words, every person has the innate right to make choices freely but only so long as these choices do not interfere with those of others.  Laws that do not allow for political freedom are forbidden in Kant's view because within a just civil condition, the promulgation of a law requires that an entire people agree to be subjected to it.
There are more substantive problems facing Kant's political theory.  Most notable among these concerns is Kant's promotion of welfare legislation.  As he says, ""The government is authorized to constrain the wealthy to provide the means of sustenance to those who are unable to provide for even their most necessary and basic needs.""  Given Kant's notion of political freedom, to enact such legislation would be catastrophic.  Welfare legislation violates Kant's own criterion regarding laws promulgated within a just civil condition: Governmental interference of this kind is a prime example of legislation on which an entire people will be divided.
This was an egregious error on Kant's part.  At the same time, proponents of a Kantian conception of the state need neither embrace this result nor dismiss the possibility of consistently incorporating welfare legislation into their preferred brand of political theory.  Although Kant did not explicitly advocate it himself, a Kantian may hold that public provision of welfare is necessary for the state to fulfill its primary function—namely, protecting the liberties of its citizens.  For instance, if failing to adopt welfare legislation would threaten the continued existence, security, or stability of the state, then Kantians may justify such legislation on the grounds that it serves an instrumentally necessary role in upholding the fundamental aims and principles of Kantianism itself.  It is interesting to note that followers of Kant will likely have independent reasons to accept something along these lines.  Kant himself held that rational entities—such as persons, corporations, and states—have a duty of self-preservation, and thus are obligated to take measures to ensure their continued existence, security, and stability.
There are still other issues surrounding a Kantian theory of the state.  These will need to be dealt with separately, but they should not go overlooked.  Breaking down the barriers that leave Kant outside the primary canon of political theory is a laudable task, which will take much effort to accomplish.",Critical Analysis & Reasoning Skills,54,Social Sciences
55,"Passage Title: Kant, Kantianism, and Traditional Political Theory","Although the primary canon in the history of political thought largely ignores the views of Immanuel Kant, he nevertheless was as important a political thinker as Rousseau, Locke, or even Aristotle.  Working in the context of the Prussian Enlightenment, Kant championed the idea that the only innate right of human beings is freedom.  Fittingly, the centerpiece of his political theory is that freedom can be the only basis for a just political state.
A political theory that advances freedom as its core tenet may appear inherently problematic.  The very existence of a political state would seem to require limitations on the freedoms of those within it.  However, this presents no serious problem for Kant's theory.  The Kantian notion of political freedom—which stands in stark contrast to the notion of transcendental freedom widely associated with Kant's nonpolitical writings—is restricted to the external relations between human persons.  In its political sense, freedom refers to the innate right of every person to independence from being constrained by another's choice.  In other words, every person has the innate right to make choices freely but only so long as these choices do not interfere with those of others.  Laws that do not allow for political freedom are forbidden in Kant's view because within a just civil condition, the promulgation of a law requires that an entire people agree to be subjected to it.
There are more substantive problems facing Kant's political theory.  Most notable among these concerns is Kant's promotion of welfare legislation.  As he says, ""The government is authorized to constrain the wealthy to provide the means of sustenance to those who are unable to provide for even their most necessary and basic needs.""  Given Kant's notion of political freedom, to enact such legislation would be catastrophic.  Welfare legislation violates Kant's own criterion regarding laws promulgated within a just civil condition: Governmental interference of this kind is a prime example of legislation on which an entire people will be divided.
This was an egregious error on Kant's part.  At the same time, proponents of a Kantian conception of the state need neither embrace this result nor dismiss the possibility of consistently incorporating welfare legislation into their preferred brand of political theory.  Although Kant did not explicitly advocate it himself, a Kantian may hold that public provision of welfare is necessary for the state to fulfill its primary function—namely, protecting the liberties of its citizens.  For instance, if failing to adopt welfare legislation would threaten the continued existence, security, or stability of the state, then Kantians may justify such legislation on the grounds that it serves an instrumentally necessary role in upholding the fundamental aims and principles of Kantianism itself.  It is interesting to note that followers of Kant will likely have independent reasons to accept something along these lines.  Kant himself held that rational entities—such as persons, corporations, and states—have a duty of self-preservation, and thus are obligated to take measures to ensure their continued existence, security, and stability.
There are still other issues surrounding a Kantian theory of the state.  These will need to be dealt with separately, but they should not go overlooked.  Breaking down the barriers that leave Kant outside the primary canon of political theory is a laudable task, which will take much effort to accomplish.",Critical Analysis & Reasoning Skills,54,Social Sciences
56,"Passage Title: Kant, Kantianism, and Traditional Political Theory","Although the primary canon in the history of political thought largely ignores the views of Immanuel Kant, he nevertheless was as important a political thinker as Rousseau, Locke, or even Aristotle.  Working in the context of the Prussian Enlightenment, Kant championed the idea that the only innate right of human beings is freedom.  Fittingly, the centerpiece of his political theory is that freedom can be the only basis for a just political state.
A political theory that advances freedom as its core tenet may appear inherently problematic.  The very existence of a political state would seem to require limitations on the freedoms of those within it.  However, this presents no serious problem for Kant's theory.  The Kantian notion of political freedom—which stands in stark contrast to the notion of transcendental freedom widely associated with Kant's nonpolitical writings—is restricted to the external relations between human persons.  In its political sense, freedom refers to the innate right of every person to independence from being constrained by another's choice.  In other words, every person has the innate right to make choices freely but only so long as these choices do not interfere with those of others.  Laws that do not allow for political freedom are forbidden in Kant's view because within a just civil condition, the promulgation of a law requires that an entire people agree to be subjected to it.
There are more substantive problems facing Kant's political theory.  Most notable among these concerns is Kant's promotion of welfare legislation.  As he says, ""The government is authorized to constrain the wealthy to provide the means of sustenance to those who are unable to provide for even their most necessary and basic needs.""  Given Kant's notion of political freedom, to enact such legislation would be catastrophic.  Welfare legislation violates Kant's own criterion regarding laws promulgated within a just civil condition: Governmental interference of this kind is a prime example of legislation on which an entire people will be divided.
This was an egregious error on Kant's part.  At the same time, proponents of a Kantian conception of the state need neither embrace this result nor dismiss the possibility of consistently incorporating welfare legislation into their preferred brand of political theory.  Although Kant did not explicitly advocate it himself, a Kantian may hold that public provision of welfare is necessary for the state to fulfill its primary function—namely, protecting the liberties of its citizens.  For instance, if failing to adopt welfare legislation would threaten the continued existence, security, or stability of the state, then Kantians may justify such legislation on the grounds that it serves an instrumentally necessary role in upholding the fundamental aims and principles of Kantianism itself.  It is interesting to note that followers of Kant will likely have independent reasons to accept something along these lines.  Kant himself held that rational entities—such as persons, corporations, and states—have a duty of self-preservation, and thus are obligated to take measures to ensure their continued existence, security, and stability.
There are still other issues surrounding a Kantian theory of the state.  These will need to be dealt with separately, but they should not go overlooked.  Breaking down the barriers that leave Kant outside the primary canon of political theory is a laudable task, which will take much effort to accomplish.",Critical Analysis & Reasoning Skills,54,Social Sciences
57,"Passage Title: Kant, Kantianism, and Traditional Political Theory","Although the primary canon in the history of political thought largely ignores the views of Immanuel Kant, he nevertheless was as important a political thinker as Rousseau, Locke, or even Aristotle.  Working in the context of the Prussian Enlightenment, Kant championed the idea that the only innate right of human beings is freedom.  Fittingly, the centerpiece of his political theory is that freedom can be the only basis for a just political state.
A political theory that advances freedom as its core tenet may appear inherently problematic.  The very existence of a political state would seem to require limitations on the freedoms of those within it.  However, this presents no serious problem for Kant's theory.  The Kantian notion of political freedom—which stands in stark contrast to the notion of transcendental freedom widely associated with Kant's nonpolitical writings—is restricted to the external relations between human persons.  In its political sense, freedom refers to the innate right of every person to independence from being constrained by another's choice.  In other words, every person has the innate right to make choices freely but only so long as these choices do not interfere with those of others.  Laws that do not allow for political freedom are forbidden in Kant's view because within a just civil condition, the promulgation of a law requires that an entire people agree to be subjected to it.
There are more substantive problems facing Kant's political theory.  Most notable among these concerns is Kant's promotion of welfare legislation.  As he says, ""The government is authorized to constrain the wealthy to provide the means of sustenance to those who are unable to provide for even their most necessary and basic needs.""  Given Kant's notion of political freedom, to enact such legislation would be catastrophic.  Welfare legislation violates Kant's own criterion regarding laws promulgated within a just civil condition: Governmental interference of this kind is a prime example of legislation on which an entire people will be divided.
This was an egregious error on Kant's part.  At the same time, proponents of a Kantian conception of the state need neither embrace this result nor dismiss the possibility of consistently incorporating welfare legislation into their preferred brand of political theory.  Although Kant did not explicitly advocate it himself, a Kantian may hold that public provision of welfare is necessary for the state to fulfill its primary function—namely, protecting the liberties of its citizens.  For instance, if failing to adopt welfare legislation would threaten the continued existence, security, or stability of the state, then Kantians may justify such legislation on the grounds that it serves an instrumentally necessary role in upholding the fundamental aims and principles of Kantianism itself.  It is interesting to note that followers of Kant will likely have independent reasons to accept something along these lines.  Kant himself held that rational entities—such as persons, corporations, and states—have a duty of self-preservation, and thus are obligated to take measures to ensure their continued existence, security, and stability.
There are still other issues surrounding a Kantian theory of the state.  These will need to be dealt with separately, but they should not go overlooked.  Breaking down the barriers that leave Kant outside the primary canon of political theory is a laudable task, which will take much effort to accomplish.",Critical Analysis & Reasoning Skills,54,Social Sciences
58,"Passage Title: Kant, Kantianism, and Traditional Political Theory","Although the primary canon in the history of political thought largely ignores the views of Immanuel Kant, he nevertheless was as important a political thinker as Rousseau, Locke, or even Aristotle.  Working in the context of the Prussian Enlightenment, Kant championed the idea that the only innate right of human beings is freedom.  Fittingly, the centerpiece of his political theory is that freedom can be the only basis for a just political state.
A political theory that advances freedom as its core tenet may appear inherently problematic.  The very existence of a political state would seem to require limitations on the freedoms of those within it.  However, this presents no serious problem for Kant's theory.  The Kantian notion of political freedom—which stands in stark contrast to the notion of transcendental freedom widely associated with Kant's nonpolitical writings—is restricted to the external relations between human persons.  In its political sense, freedom refers to the innate right of every person to independence from being constrained by another's choice.  In other words, every person has the innate right to make choices freely but only so long as these choices do not interfere with those of others.  Laws that do not allow for political freedom are forbidden in Kant's view because within a just civil condition, the promulgation of a law requires that an entire people agree to be subjected to it.
There are more substantive problems facing Kant's political theory.  Most notable among these concerns is Kant's promotion of welfare legislation.  As he says, ""The government is authorized to constrain the wealthy to provide the means of sustenance to those who are unable to provide for even their most necessary and basic needs.""  Given Kant's notion of political freedom, to enact such legislation would be catastrophic.  Welfare legislation violates Kant's own criterion regarding laws promulgated within a just civil condition: Governmental interference of this kind is a prime example of legislation on which an entire people will be divided.
This was an egregious error on Kant's part.  At the same time, proponents of a Kantian conception of the state need neither embrace this result nor dismiss the possibility of consistently incorporating welfare legislation into their preferred brand of political theory.  Although Kant did not explicitly advocate it himself, a Kantian may hold that public provision of welfare is necessary for the state to fulfill its primary function—namely, protecting the liberties of its citizens.  For instance, if failing to adopt welfare legislation would threaten the continued existence, security, or stability of the state, then Kantians may justify such legislation on the grounds that it serves an instrumentally necessary role in upholding the fundamental aims and principles of Kantianism itself.  It is interesting to note that followers of Kant will likely have independent reasons to accept something along these lines.  Kant himself held that rational entities—such as persons, corporations, and states—have a duty of self-preservation, and thus are obligated to take measures to ensure their continued existence, security, and stability.
There are still other issues surrounding a Kantian theory of the state.  These will need to be dealt with separately, but they should not go overlooked.  Breaking down the barriers that leave Kant outside the primary canon of political theory is a laudable task, which will take much effort to accomplish.",Critical Analysis & Reasoning Skills,54,Social Sciences
59,"Passage Title: Kant, Kantianism, and Traditional Political Theory","Although the primary canon in the history of political thought largely ignores the views of Immanuel Kant, he nevertheless was as important a political thinker as Rousseau, Locke, or even Aristotle.  Working in the context of the Prussian Enlightenment, Kant championed the idea that the only innate right of human beings is freedom.  Fittingly, the centerpiece of his political theory is that freedom can be the only basis for a just political state.
A political theory that advances freedom as its core tenet may appear inherently problematic.  The very existence of a political state would seem to require limitations on the freedoms of those within it.  However, this presents no serious problem for Kant's theory.  The Kantian notion of political freedom—which stands in stark contrast to the notion of transcendental freedom widely associated with Kant's nonpolitical writings—is restricted to the external relations between human persons.  In its political sense, freedom refers to the innate right of every person to independence from being constrained by another's choice.  In other words, every person has the innate right to make choices freely but only so long as these choices do not interfere with those of others.  Laws that do not allow for political freedom are forbidden in Kant's view because within a just civil condition, the promulgation of a law requires that an entire people agree to be subjected to it.
There are more substantive problems facing Kant's political theory.  Most notable among these concerns is Kant's promotion of welfare legislation.  As he says, ""The government is authorized to constrain the wealthy to provide the means of sustenance to those who are unable to provide for even their most necessary and basic needs.""  Given Kant's notion of political freedom, to enact such legislation would be catastrophic.  Welfare legislation violates Kant's own criterion regarding laws promulgated within a just civil condition: Governmental interference of this kind is a prime example of legislation on which an entire people will be divided.
This was an egregious error on Kant's part.  At the same time, proponents of a Kantian conception of the state need neither embrace this result nor dismiss the possibility of consistently incorporating welfare legislation into their preferred brand of political theory.  Although Kant did not explicitly advocate it himself, a Kantian may hold that public provision of welfare is necessary for the state to fulfill its primary function—namely, protecting the liberties of its citizens.  For instance, if failing to adopt welfare legislation would threaten the continued existence, security, or stability of the state, then Kantians may justify such legislation on the grounds that it serves an instrumentally necessary role in upholding the fundamental aims and principles of Kantianism itself.  It is interesting to note that followers of Kant will likely have independent reasons to accept something along these lines.  Kant himself held that rational entities—such as persons, corporations, and states—have a duty of self-preservation, and thus are obligated to take measures to ensure their continued existence, security, and stability.
There are still other issues surrounding a Kantian theory of the state.  These will need to be dealt with separately, but they should not go overlooked.  Breaking down the barriers that leave Kant outside the primary canon of political theory is a laudable task, which will take much effort to accomplish.",Critical Analysis & Reasoning Skills,54,Social Sciences
60,"Passage Title: Kant, Kantianism, and Traditional Political Theory","Although the primary canon in the history of political thought largely ignores the views of Immanuel Kant, he nevertheless was as important a political thinker as Rousseau, Locke, or even Aristotle.  Working in the context of the Prussian Enlightenment, Kant championed the idea that the only innate right of human beings is freedom.  Fittingly, the centerpiece of his political theory is that freedom can be the only basis for a just political state.
A political theory that advances freedom as its core tenet may appear inherently problematic.  The very existence of a political state would seem to require limitations on the freedoms of those within it.  However, this presents no serious problem for Kant's theory.  The Kantian notion of political freedom—which stands in stark contrast to the notion of transcendental freedom widely associated with Kant's nonpolitical writings—is restricted to the external relations between human persons.  In its political sense, freedom refers to the innate right of every person to independence from being constrained by another's choice.  In other words, every person has the innate right to make choices freely but only so long as these choices do not interfere with those of others.  Laws that do not allow for political freedom are forbidden in Kant's view because within a just civil condition, the promulgation of a law requires that an entire people agree to be subjected to it.
There are more substantive problems facing Kant's political theory.  Most notable among these concerns is Kant's promotion of welfare legislation.  As he says, ""The government is authorized to constrain the wealthy to provide the means of sustenance to those who are unable to provide for even their most necessary and basic needs.""  Given Kant's notion of political freedom, to enact such legislation would be catastrophic.  Welfare legislation violates Kant's own criterion regarding laws promulgated within a just civil condition: Governmental interference of this kind is a prime example of legislation on which an entire people will be divided.
This was an egregious error on Kant's part.  At the same time, proponents of a Kantian conception of the state need neither embrace this result nor dismiss the possibility of consistently incorporating welfare legislation into their preferred brand of political theory.  Although Kant did not explicitly advocate it himself, a Kantian may hold that public provision of welfare is necessary for the state to fulfill its primary function—namely, protecting the liberties of its citizens.  For instance, if failing to adopt welfare legislation would threaten the continued existence, security, or stability of the state, then Kantians may justify such legislation on the grounds that it serves an instrumentally necessary role in upholding the fundamental aims and principles of Kantianism itself.  It is interesting to note that followers of Kant will likely have independent reasons to accept something along these lines.  Kant himself held that rational entities—such as persons, corporations, and states—have a duty of self-preservation, and thus are obligated to take measures to ensure their continued existence, security, and stability.
There are still other issues surrounding a Kantian theory of the state.  These will need to be dealt with separately, but they should not go overlooked.  Breaking down the barriers that leave Kant outside the primary canon of political theory is a laudable task, which will take much effort to accomplish.",Critical Analysis & Reasoning Skills,54,Social Sciences
61,"Passage Title: Kant, Kantianism, and Traditional Political Theory","Although the primary canon in the history of political thought largely ignores the views of Immanuel Kant, he nevertheless was as important a political thinker as Rousseau, Locke, or even Aristotle.  Working in the context of the Prussian Enlightenment, Kant championed the idea that the only innate right of human beings is freedom.  Fittingly, the centerpiece of his political theory is that freedom can be the only basis for a just political state.
A political theory that advances freedom as its core tenet may appear inherently problematic.  The very existence of a political state would seem to require limitations on the freedoms of those within it.  However, this presents no serious problem for Kant's theory.  The Kantian notion of political freedom—which stands in stark contrast to the notion of transcendental freedom widely associated with Kant's nonpolitical writings—is restricted to the external relations between human persons.  In its political sense, freedom refers to the innate right of every person to independence from being constrained by another's choice.  In other words, every person has the innate right to make choices freely but only so long as these choices do not interfere with those of others.  Laws that do not allow for political freedom are forbidden in Kant's view because within a just civil condition, the promulgation of a law requires that an entire people agree to be subjected to it.
There are more substantive problems facing Kant's political theory.  Most notable among these concerns is Kant's promotion of welfare legislation.  As he says, ""The government is authorized to constrain the wealthy to provide the means of sustenance to those who are unable to provide for even their most necessary and basic needs.""  Given Kant's notion of political freedom, to enact such legislation would be catastrophic.  Welfare legislation violates Kant's own criterion regarding laws promulgated within a just civil condition: Governmental interference of this kind is a prime example of legislation on which an entire people will be divided.
This was an egregious error on Kant's part.  At the same time, proponents of a Kantian conception of the state need neither embrace this result nor dismiss the possibility of consistently incorporating welfare legislation into their preferred brand of political theory.  Although Kant did not explicitly advocate it himself, a Kantian may hold that public provision of welfare is necessary for the state to fulfill its primary function—namely, protecting the liberties of its citizens.  For instance, if failing to adopt welfare legislation would threaten the continued existence, security, or stability of the state, then Kantians may justify such legislation on the grounds that it serves an instrumentally necessary role in upholding the fundamental aims and principles of Kantianism itself.  It is interesting to note that followers of Kant will likely have independent reasons to accept something along these lines.  Kant himself held that rational entities—such as persons, corporations, and states—have a duty of self-preservation, and thus are obligated to take measures to ensure their continued existence, security, and stability.
There are still other issues surrounding a Kantian theory of the state.  These will need to be dealt with separately, but they should not go overlooked.  Breaking down the barriers that leave Kant outside the primary canon of political theory is a laudable task, which will take much effort to accomplish.",Critical Analysis & Reasoning Skills,54,Social Sciences
62,"Passage Title: Kant, Kantianism, and Traditional Political Theory","Although the primary canon in the history of political thought largely ignores the views of Immanuel Kant, he nevertheless was as important a political thinker as Rousseau, Locke, or even Aristotle.  Working in the context of the Prussian Enlightenment, Kant championed the idea that the only innate right of human beings is freedom.  Fittingly, the centerpiece of his political theory is that freedom can be the only basis for a just political state.
A political theory that advances freedom as its core tenet may appear inherently problematic.  The very existence of a political state would seem to require limitations on the freedoms of those within it.  However, this presents no serious problem for Kant's theory.  The Kantian notion of political freedom—which stands in stark contrast to the notion of transcendental freedom widely associated with Kant's nonpolitical writings—is restricted to the external relations between human persons.  In its political sense, freedom refers to the innate right of every person to independence from being constrained by another's choice.  In other words, every person has the innate right to make choices freely but only so long as these choices do not interfere with those of others.  Laws that do not allow for political freedom are forbidden in Kant's view because within a just civil condition, the promulgation of a law requires that an entire people agree to be subjected to it.
There are more substantive problems facing Kant's political theory.  Most notable among these concerns is Kant's promotion of welfare legislation.  As he says, ""The government is authorized to constrain the wealthy to provide the means of sustenance to those who are unable to provide for even their most necessary and basic needs.""  Given Kant's notion of political freedom, to enact such legislation would be catastrophic.  Welfare legislation violates Kant's own criterion regarding laws promulgated within a just civil condition: Governmental interference of this kind is a prime example of legislation on which an entire people will be divided.
This was an egregious error on Kant's part.  At the same time, proponents of a Kantian conception of the state need neither embrace this result nor dismiss the possibility of consistently incorporating welfare legislation into their preferred brand of political theory.  Although Kant did not explicitly advocate it himself, a Kantian may hold that public provision of welfare is necessary for the state to fulfill its primary function—namely, protecting the liberties of its citizens.  For instance, if failing to adopt welfare legislation would threaten the continued existence, security, or stability of the state, then Kantians may justify such legislation on the grounds that it serves an instrumentally necessary role in upholding the fundamental aims and principles of Kantianism itself.  It is interesting to note that followers of Kant will likely have independent reasons to accept something along these lines.  Kant himself held that rational entities—such as persons, corporations, and states—have a duty of self-preservation, and thus are obligated to take measures to ensure their continued existence, security, and stability.
There are still other issues surrounding a Kantian theory of the state.  These will need to be dealt with separately, but they should not go overlooked.  Breaking down the barriers that leave Kant outside the primary canon of political theory is a laudable task, which will take much effort to accomplish.",Critical Analysis & Reasoning Skills,54,Social Sciences
63,"Passage Title: Kant, Kantianism, and Traditional Political Theory","Although the primary canon in the history of political thought largely ignores the views of Immanuel Kant, he nevertheless was as important a political thinker as Rousseau, Locke, or even Aristotle.  Working in the context of the Prussian Enlightenment, Kant championed the idea that the only innate right of human beings is freedom.  Fittingly, the centerpiece of his political theory is that freedom can be the only basis for a just political state.
A political theory that advances freedom as its core tenet may appear inherently problematic.  The very existence of a political state would seem to require limitations on the freedoms of those within it.  However, this presents no serious problem for Kant's theory.  The Kantian notion of political freedom—which stands in stark contrast to the notion of transcendental freedom widely associated with Kant's nonpolitical writings—is restricted to the external relations between human persons.  In its political sense, freedom refers to the innate right of every person to independence from being constrained by another's choice.  In other words, every person has the innate right to make choices freely but only so long as these choices do not interfere with those of others.  Laws that do not allow for political freedom are forbidden in Kant's view because within a just civil condition, the promulgation of a law requires that an entire people agree to be subjected to it.
There are more substantive problems facing Kant's political theory.  Most notable among these concerns is Kant's promotion of welfare legislation.  As he says, ""The government is authorized to constrain the wealthy to provide the means of sustenance to those who are unable to provide for even their most necessary and basic needs.""  Given Kant's notion of political freedom, to enact such legislation would be catastrophic.  Welfare legislation violates Kant's own criterion regarding laws promulgated within a just civil condition: Governmental interference of this kind is a prime example of legislation on which an entire people will be divided.
This was an egregious error on Kant's part.  At the same time, proponents of a Kantian conception of the state need neither embrace this result nor dismiss the possibility of consistently incorporating welfare legislation into their preferred brand of political theory.  Although Kant did not explicitly advocate it himself, a Kantian may hold that public provision of welfare is necessary for the state to fulfill its primary function—namely, protecting the liberties of its citizens.  For instance, if failing to adopt welfare legislation would threaten the continued existence, security, or stability of the state, then Kantians may justify such legislation on the grounds that it serves an instrumentally necessary role in upholding the fundamental aims and principles of Kantianism itself.  It is interesting to note that followers of Kant will likely have independent reasons to accept something along these lines.  Kant himself held that rational entities—such as persons, corporations, and states—have a duty of self-preservation, and thus are obligated to take measures to ensure their continued existence, security, and stability.
There are still other issues surrounding a Kantian theory of the state.  These will need to be dealt with separately, but they should not go overlooked.  Breaking down the barriers that leave Kant outside the primary canon of political theory is a laudable task, which will take much effort to accomplish.",Critical Analysis & Reasoning Skills,54,Social Sciences
64,"Passage Title: Kant, Kantianism, and Traditional Political Theory","Although the primary canon in the history of political thought largely ignores the views of Immanuel Kant, he nevertheless was as important a political thinker as Rousseau, Locke, or even Aristotle.  Working in the context of the Prussian Enlightenment, Kant championed the idea that the only innate right of human beings is freedom.  Fittingly, the centerpiece of his political theory is that freedom can be the only basis for a just political state.
A political theory that advances freedom as its core tenet may appear inherently problematic.  The very existence of a political state would seem to require limitations on the freedoms of those within it.  However, this presents no serious problem for Kant's theory.  The Kantian notion of political freedom—which stands in stark contrast to the notion of transcendental freedom widely associated with Kant's nonpolitical writings—is restricted to the external relations between human persons.  In its political sense, freedom refers to the innate right of every person to independence from being constrained by another's choice.  In other words, every person has the innate right to make choices freely but only so long as these choices do not interfere with those of others.  Laws that do not allow for political freedom are forbidden in Kant's view because within a just civil condition, the promulgation of a law requires that an entire people agree to be subjected to it.
There are more substantive problems facing Kant's political theory.  Most notable among these concerns is Kant's promotion of welfare legislation.  As he says, ""The government is authorized to constrain the wealthy to provide the means of sustenance to those who are unable to provide for even their most necessary and basic needs.""  Given Kant's notion of political freedom, to enact such legislation would be catastrophic.  Welfare legislation violates Kant's own criterion regarding laws promulgated within a just civil condition: Governmental interference of this kind is a prime example of legislation on which an entire people will be divided.
This was an egregious error on Kant's part.  At the same time, proponents of a Kantian conception of the state need neither embrace this result nor dismiss the possibility of consistently incorporating welfare legislation into their preferred brand of political theory.  Although Kant did not explicitly advocate it himself, a Kantian may hold that public provision of welfare is necessary for the state to fulfill its primary function—namely, protecting the liberties of its citizens.  For instance, if failing to adopt welfare legislation would threaten the continued existence, security, or stability of the state, then Kantians may justify such legislation on the grounds that it serves an instrumentally necessary role in upholding the fundamental aims and principles of Kantianism itself.  It is interesting to note that followers of Kant will likely have independent reasons to accept something along these lines.  Kant himself held that rational entities—such as persons, corporations, and states—have a duty of self-preservation, and thus are obligated to take measures to ensure their continued existence, security, and stability.
There are still other issues surrounding a Kantian theory of the state.  These will need to be dealt with separately, but they should not go overlooked.  Breaking down the barriers that leave Kant outside the primary canon of political theory is a laudable task, which will take much effort to accomplish.",Critical Analysis & Reasoning Skills,54,Social Sciences
65,"Passage Title: Kant, Kantianism, and Traditional Political Theory","Although the primary canon in the history of political thought largely ignores the views of Immanuel Kant, he nevertheless was as important a political thinker as Rousseau, Locke, or even Aristotle.  Working in the context of the Prussian Enlightenment, Kant championed the idea that the only innate right of human beings is freedom.  Fittingly, the centerpiece of his political theory is that freedom can be the only basis for a just political state.
A political theory that advances freedom as its core tenet may appear inherently problematic.  The very existence of a political state would seem to require limitations on the freedoms of those within it.  However, this presents no serious problem for Kant's theory.  The Kantian notion of political freedom—which stands in stark contrast to the notion of transcendental freedom widely associated with Kant's nonpolitical writings—is restricted to the external relations between human persons.  In its political sense, freedom refers to the innate right of every person to independence from being constrained by another's choice.  In other words, every person has the innate right to make choices freely but only so long as these choices do not interfere with those of others.  Laws that do not allow for political freedom are forbidden in Kant's view because within a just civil condition, the promulgation of a law requires that an entire people agree to be subjected to it.
There are more substantive problems facing Kant's political theory.  Most notable among these concerns is Kant's promotion of welfare legislation.  As he says, ""The government is authorized to constrain the wealthy to provide the means of sustenance to those who are unable to provide for even their most necessary and basic needs.""  Given Kant's notion of political freedom, to enact such legislation would be catastrophic.  Welfare legislation violates Kant's own criterion regarding laws promulgated within a just civil condition: Governmental interference of this kind is a prime example of legislation on which an entire people will be divided.
This was an egregious error on Kant's part.  At the same time, proponents of a Kantian conception of the state need neither embrace this result nor dismiss the possibility of consistently incorporating welfare legislation into their preferred brand of political theory.  Although Kant did not explicitly advocate it himself, a Kantian may hold that public provision of welfare is necessary for the state to fulfill its primary function—namely, protecting the liberties of its citizens.  For instance, if failing to adopt welfare legislation would threaten the continued existence, security, or stability of the state, then Kantians may justify such legislation on the grounds that it serves an instrumentally necessary role in upholding the fundamental aims and principles of Kantianism itself.  It is interesting to note that followers of Kant will likely have independent reasons to accept something along these lines.  Kant himself held that rational entities—such as persons, corporations, and states—have a duty of self-preservation, and thus are obligated to take measures to ensure their continued existence, security, and stability.
There are still other issues surrounding a Kantian theory of the state.  These will need to be dealt with separately, but they should not go overlooked.  Breaking down the barriers that leave Kant outside the primary canon of political theory is a laudable task, which will take much effort to accomplish.",Critical Analysis & Reasoning Skills,54,Social Sciences
66,,,,,
